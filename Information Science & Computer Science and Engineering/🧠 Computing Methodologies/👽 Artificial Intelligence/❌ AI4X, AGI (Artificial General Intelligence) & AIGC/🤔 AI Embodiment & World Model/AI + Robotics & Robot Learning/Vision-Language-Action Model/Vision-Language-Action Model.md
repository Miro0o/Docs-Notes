# Vision-Language-Action Model

[TOC]



## Res
### Related Topics
â†— [Multimodal AI & MLLM](../../../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/ðŸ%20Multimodal%20AI%20&%20MLLM/Multimodal%20AI%20&%20MLLM.md)
- â†— [Image, Video, Vision, & VLM (Vision Language Model)](../../../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/ðŸ%20Multimodal%20AI%20&%20MLLM/Image,%20Video,%20Vision,%20&%20VLM%20(Vision%20Language%20Model)/Image,%20Video,%20Vision,%20&%20VLM%20(Vision%20Language%20Model).md)


### Other Resources



## Intro
> ðŸ”— https://en.wikipedia.org/wiki/Vision-language-action_model

InÂ [robot learning](https://en.wikipedia.org/wiki/Robot_learning "Robot learning"), aÂ **vision-language-action model**Â (**VLA**) is a class ofÂ [multimodal](https://en.wikipedia.org/wiki/Multimodal_learning "Multimodal learning")Â [foundation models](https://en.wikipedia.org/wiki/Foundation_model "Foundation model")Â that integratesÂ [vision](https://en.wikipedia.org/wiki/Computer_vision "Computer vision"),Â [language](https://en.wikipedia.org/wiki/Natural_language "Natural language")Â and actions. Given an input image (or video) of the robot's surroundings and a text instruction, a VLA directly outputs low-level robot actions that can be executed to accomplish the requested task.

VLAs are generally constructed byÂ [fine-tuning](https://en.wikipedia.org/wiki/Fine-tuning_\(deep_learning\) "Fine-tuning (deep learning)")Â aÂ [vision-language model](https://en.wikipedia.org/wiki/Vision-language_model "Vision-language model")Â (VLM), i.e. aÂ [large language model](https://en.wikipedia.org/wiki/Large_language_model "Large language model")Â extended withÂ [vision](https://en.wikipedia.org/wiki/Computer_vision "Computer vision")Â capabilities) on a large-scale dataset that pairs visual observation and language instructions with robot trajectories. These models combine a vision-language encoder ([vision transformer](https://en.wikipedia.org/wiki/Vision_transformer "Vision transformer")), which translates an image observation and aÂ [natural language](https://en.wikipedia.org/wiki/Natural_language "Natural language")Â description into a distribution within aÂ [latent space](https://en.wikipedia.org/wiki/Latent_space "Latent space"), with an action decoder that transforms this representation into continuous output actions, directly executable on the robot.[[3]](https://en.wikipedia.org/wiki/Vision-language-action_model#cite_note-3)

The concept was pioneered in July 2023 byÂ [Google DeepMind](https://en.wikipedia.org/wiki/Google_DeepMind "Google DeepMind")Â with RT-2, a VLM adapted for end-to-end manipulation tasks, capable of unifyingÂ [perception](https://en.wikipedia.org/wiki/Machine_perception "Machine perception"),Â [reasoning](https://en.wikipedia.org/wiki/Reasoning_language_model "Reasoning language model")Â andÂ [control](https://en.wikipedia.org/wiki/Robot_control "Robot control").



## Ref
