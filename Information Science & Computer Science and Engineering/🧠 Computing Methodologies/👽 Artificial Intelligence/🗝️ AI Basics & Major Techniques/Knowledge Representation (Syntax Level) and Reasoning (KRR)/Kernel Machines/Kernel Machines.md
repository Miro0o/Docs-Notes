# Kernel Machines

[TOC]



## Res
### Related Topics


### Other Resources



## Intro
> ðŸ”— https://en.wikipedia.org/wiki/Kernel_method

InÂ [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning"),Â **kernel machines**Â are a class of algorithms forÂ [pattern analysis](https://en.wikipedia.org/wiki/Pattern_analysis "Pattern analysis"), whose best known member is theÂ [support-vector machine](https://en.wikipedia.org/wiki/Support-vector_machine "Support-vector machine")Â (SVM). These methods involve using linear classifiers to solve nonlinear problems.Â The general task ofÂ [pattern analysis](https://en.wikipedia.org/wiki/Pattern_analysis "Pattern analysis")Â is to find and study general types of relations (for exampleÂ [clusters](https://en.wikipedia.org/wiki/Cluster_analysis "Cluster analysis"),Â [rankings](https://en.wikipedia.org/wiki/Ranking "Ranking"),Â [principal components](https://en.wikipedia.org/wiki/Principal_components "Principal components"),Â [correlations](https://en.wikipedia.org/wiki/Correlation "Correlation"),Â [classifications](https://en.wikipedia.org/wiki/Statistical_classification "Statistical classification")) in datasets. For many algorithms that solve these tasks, the data in raw representation have to be explicitly transformed intoÂ [feature vector](https://en.wikipedia.org/wiki/Feature_vector "Feature vector")Â representations via a user-specifiedÂ _feature map_: in contrast, kernel methods require only a user-specifiedÂ _kernel_, i.e., aÂ [similarity function](https://en.wikipedia.org/wiki/Similarity_function "Similarity function")Â over all pairs of data points computed usingÂ [inner products](https://en.wikipedia.org/wiki/Inner_products "Inner products"). The feature map in kernel machines is infinite dimensional but only requires a finite dimensional matrix from user-input according to theÂ [representer theorem](https://en.wikipedia.org/wiki/Representer_theorem "Representer theorem"). Kernel machines are slow to compute for datasets larger than a couple of thousand examples without parallel processing.

Kernel methods owe their name to the use ofÂ [kernel functions](https://en.wikipedia.org/wiki/Positive-definite_kernel "Positive-definite kernel"), which enable them to operate in a high-dimensional,Â _implicit_Â [feature space](https://en.wikipedia.org/wiki/Feature_space "Feature space")Â without ever computing the coordinates of the data in that space, but rather by simply computing theÂ [inner products](https://en.wikipedia.org/wiki/Inner_product "Inner product")Â between theÂ [images](https://en.wikipedia.org/wiki/Image_\(mathematics\) "Image (mathematics)")Â of all pairs of data in the feature space. This operation is often computationally cheaper than the explicit computation of the coordinates. This approach is called the "**kernel trick**".Â Kernel functions have been introduced for sequence data,Â [graphs](https://en.wikipedia.org/wiki/Graph_kernel "Graph kernel"), text, images, as well as vectors.

Algorithms capable of operating with kernels include theÂ [kernel perceptron](https://en.wikipedia.org/wiki/Kernel_perceptron "Kernel perceptron"), support-vector machines (SVM),Â [Gaussian processes](https://en.wikipedia.org/wiki/Gaussian_process "Gaussian process"),Â [principal components analysis](https://en.wikipedia.org/wiki/Principal_components_analysis "Principal components analysis")Â (PCA),Â [canonical correlation analysis](https://en.wikipedia.org/wiki/Canonical_correlation_analysis "Canonical correlation analysis"),Â [ridge regression](https://en.wikipedia.org/wiki/Ridge_regression "Ridge regression"),Â [spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering "Spectral clustering"),Â [linear adaptive filters](https://en.wikipedia.org/wiki/Adaptive_filter "Adaptive filter")Â and many others.

Most kernel algorithms are based onÂ [convex optimization](https://en.wikipedia.org/wiki/Convex_optimization "Convex optimization")Â orÂ [eigenproblems](https://en.wikipedia.org/wiki/Eigenvalue,_eigenvector_and_eigenspace "Eigenvalue, eigenvector and eigenspace")Â and are statistically well-founded. Typically, their statistical properties are analyzed usingÂ [statistical learning theory](https://en.wikipedia.org/wiki/Statistical_learning_theory "Statistical learning theory")Â (for example, usingÂ [Rademacher complexity](https://en.wikipedia.org/wiki/Rademacher_complexity "Rademacher complexity")).



## Ref
