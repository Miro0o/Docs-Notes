# Video Generation & VLM (Vision Language Model)

[TOC]



## Res
### Related Topics
â†— [Vision-Language-Action Model](../../../âŒ%20AI4X,%20AGI%20(Artificial%20General%20Intelligence)%20&%20AIGC/ğŸ¤”%20AI%20Embodiment%20&%20World%20Model/AI%20+%20Robotics%20&%20Robot%20Learning/Vision-Language-Action%20Model/Vision-Language-Action%20Model.md)


### Other Resources



## Intro
> ğŸ”— https://www.nvidia.com/en-us/glossary/vision-language-models/

A vision language model is an AI system built by combining aÂ [large language model](https://www.nvidia.com/en-us/glossary/large-language-models/)Â (LLM) with a vision encoder, giving the LLM the ability to â€œsee.â€

With this ability, VLMs can process and provide advanced understanding of video, image, and text inputs supplied in the prompt to generate text responses.

Unlike traditionalÂ [computer vision](https://www.nvidia.com/en-us/glossary/computer-vision/)Â (CV) models, VLMs arenâ€™t bound by a fixed set of classes or a specific task, like classification or detection. Retrained on a vast corpus of text and image/video-caption pairs, VLMs can be instructed in natural language and used to handle many classic vision tasks, as well as new generative AI-powered tasks such as summarization and visual Q&A.

![vlm-architecture-diagram](../../../../../../Assets/Pics/vlm-architecture-diagram.svg)
<small>Figure 2: A common three-part architecture for vision language models<br><a>https://www.nvidia.com/en-us/glossary/vision-language-models/</a></small>



## Ref
