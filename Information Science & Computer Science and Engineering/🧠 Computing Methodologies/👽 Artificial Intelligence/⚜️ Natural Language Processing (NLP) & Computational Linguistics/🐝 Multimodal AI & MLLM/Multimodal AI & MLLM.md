# Multi-Modal AI & MLLM

[TOC]



## Res
### Related Topics
‚Üó [AI Embodiment & World Model](../../‚ùå%20AI4X,%20AGI%20(Artificial%20General%20Intelligence)%20&%20AIGC/ü§î%20AI%20Embodiment%20&%20World%20Model/AI%20Embodiment%20&%20World%20Model.md)
‚Üó [AI(LLM) x SE](../../../../Software%20Engineering/ü§ñ%20AI(LLM)%20x%20SE/AI(LLM)%20x%20SE.md)


### List of Multimodal Models
https://deepmind.google/models/

https://ai.meta.com/research/#projects


### Other Resources
Google DeepMind:¬†["Video models are zero-shot learners and reasoners"](https://video-zero-shot.github.io/)

Anthropic :¬†["Visual features across modalities: SVG and ASCII art reveal cross-modal understanding"](https://transformer-circuits.pub/2025/october-update/index.html)
- ![](../../../../../Assets/Pics/Pasted%20image%2020251203202107.png)
- In summary, our findings demonstrate that many features activating for plain text descriptions of concepts also activate for, and can generate, text-based visual depictions of those concepts. We showed this through: (1) using features to recognize entities within text-based visual formats like ASCII and SVG, and (2) steering features to transform visual depictions, e.g., turning smiles into frowns and faces into unicorns.

https://learn.deeplearning.ai/courses/building-ai-browser-agents/lesson/ee5yw/introduction
Building AI Browser Agents
- Building a Simple Web Agent
- Building an Autonomous Web Agent
- Agent Q
- Deep Dive into AgentQ and MCTS
- Future of AI Agents

Anthropic :¬†["The enterprise AI transformation guide for healthcare & life sciences"](https://drive.google.com/file/d/1a72THSYRR9HDhi46cFssUcT81ajywL7a/view?usp=share_link)
- Anthropic : The enterprise AI transformation guide for healthcare & life sciences :¬†["Step 2. Launch a pilot"](https://drive.google.com/file/d/14QJ5RhTT1jMzJho8RgPfSE70pZjJ7gai/view)
- Anthropic : The enterprise AI transformation guide for healthcare & life sciences :¬†["Chapter 4. Building an AI-first enterprise: real-world examples from Pfizer and Novo Nordisk"](https://drive.google.com/file/d/1a72THSYRR9HDhi46cFssUcT81ajywL7a/view?usp=share_link)
Google DeepMind :¬†["SIMA 2 - an agent that plays, reasons and learns with you in virtual 3D worlds"](https://deepmind.google/blog/sima-2-an-agent-that-plays-reasons-and-learns-with-you-in-virtual-3d-worlds/)

Anthropic :¬†["Novo Nordisk accelerates clinical documentation and drug development with Claude"](https://www.claude.com/customers/novo-nordisk)

Google Research :¬†["Accelerating scientific breakthroughs with an AI co-scientist"](https://research.google/blog/accelerating-scientific-breakthroughs-with-an-ai-co-scientist/)

Anthropic :¬†["Estimating AI productivity gains from Claude conversations"](https://www.anthropic.com/research/estimating-productivity-gains)
Anthropic :¬†["Introducing advanced tool use on the Claude developer platform"](https://www.anthropic.com/engineering/advanced-tool-use)

https://drive.google.com/file/d/10DdtjK4ztVvYiAHeQ5Wp-wZUpQgJX5vO/view
InterPLM: discovering interpretable features in protein language models via sparse autoencoders



## Intro
![AI-Layer.excalidraw | 800](../../../../../Assets/Illustrations/AI%20&%20LLM/AI-Layer.excalidraw.md)



## Ref
