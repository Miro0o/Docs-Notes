# Pre-Training

[TOC]



## Res
### Related Topics
â†— [Transformers](../../../../ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning%20(ML)/Knowledge%20Representation%20and%20Reasoning%20(Syntax%20Level)/ğŸŒŠ%20Neural%20Networks%20&%20Deep%20Learning%20Methods/2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/Transformers/Transformers.md)


### Other Resources
https://stanford-cs324.github.io/winter2022/lectures/training/
CS324 - Large Language Model | Stanford

https://stanford-cs324.github.io/winter2022/lectures/parallelism/
CS324 - Large Language Model | Stanford
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf).Â _M. Shoeybi, M. Patwary, Raul Puri, P. LeGresley, J. Casper, Bryan Catanzaro_. 2019.
- [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/pdf/1811.06965.pdf).Â _Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Z. Chen_. NeurIPS 2018.
- [Efficient large-scale language model training on GPU clusters using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf).Â _D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, J. Bernauer, Bryan Catanzaro, Amar Phanishayee, M. Zaharia_. SC 2021.
- [TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models](https://arxiv.org/pdf/2102.07988.pdf).Â _Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, D. Song, I. Stoica_. ICML 2021.



## Intro



## Ref
