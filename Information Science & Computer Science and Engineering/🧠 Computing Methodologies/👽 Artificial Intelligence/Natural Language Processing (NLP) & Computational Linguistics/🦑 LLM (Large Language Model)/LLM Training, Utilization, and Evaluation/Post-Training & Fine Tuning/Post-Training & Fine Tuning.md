# Post-Training & Fine Tuning

[TOC]



## Res
### Related Topics



### Commonly Used Datasets for Fine-tuning


### Papers
[Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/pdf/2110.08207.pdf).Â _Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang A. Sutawika, Zaid Alyafeai, Antoine Chaffin, Arnaud Stiegler, Teven Le Scao, Arun Raja, Manan Dey, M SAIFUL BARI, Canwen Xu, Urmish Thakker, Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong, Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea Santilli, Thibault FÃ©vry, Jason Alan Fries, Ryan Teehan, Stella Rose Biderman, Leo Gao, T. Bers, Thomas Wolf, Alexander M. Rush_. 2021. IntroducesÂ **T0**Â from BigScience.
[Finetuned Language Models Are Zero-Shot Learners](https://arxiv.org/pdf/2109.01652.pdf).Â _Jason Wei, Maarten Bosma, Vincent Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M. Dai, Quoc V. Le_. 2021. IntroducesÂ **FLAN**Â from Google.
    
[Prefix-Tuning: Optimizing Continuous Prompts for Generation](https://arxiv.org/pdf/2101.00190.pdf).Â _Xiang Lisa Li, Percy Liang_. ACL/IJCNLP 2021.
[Training language models to follow instructions with human feedback](https://cdn.openai.com/papers/Training_language_models_to_follow_instructions_with_human_feedback.pdf).Long Ouyang, Jeff Wu Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela Mishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askellâ€ , Peter Welinder, Paul Christiano, Jan Leike, Ryan Lowe. InstructGPT paper.

[The Power of Scale for Parameter-Efficient Prompt Tuning](https://arxiv.org/pdf/2104.08691.pdf), Brian Lester, Rami Al-Rfou, Noah Constant. EMNLP 2021. Introduces prompt tuning.

[Towards a Unified View of Parameter-Efficient Transfer Learning](https://arxiv.org/pdf/2110.04366.pdf), Junxian He, Chunting Zhou, Xuezhe Ma, Taylor Berg-Kirkpatrick, Graham Neubig. ICLR 2022.

[P-Tuning v2: Prompt Tuning Can Be Comparable to Fine-tuning Universally Across Scales and Tasks](https://arxiv.org/pdf/2110.07602.pdf), Xiao Liu, Kaixuan Ji, Yicheng Fu, Zhengxiao Du, Zhilin Yang, Jie Tang. arXiv 2021.


### Other Resources
https://stanford-cs324.github.io/winter2022/lectures/adaptation/
CS324 - Large Language Model | Stanford



## Intro
> ğŸ”— Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., â€¦ Wen, J.-R. (2025). _A Survey of Large Language Models_ (No. arXiv:2303.18223). arXiv. [https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)

After pre-training, LLMs can acquire the general abilities for solving various tasks. However, an increasing number of studies have shown that **LLMâ€™s abilities can be further adapted according to specific goals.** In this section, we introduce two major approaches to adapting pre-trained LLMs, namely instruction tuning and alignment tuning. The former approach mainly aims to enhance (or unlock) the abilities of LLMs, while the latter approach aims to align the behaviors of LLMs with human values or preferences. Further, we will also discuss efficient tuning and quantization for model adaptation in resource-limited settings. In what follows, we will introduce the four parts in detail.

> ğŸ¤– GPT-5
> Difference between instruction tuning and alignment tuning

**instruction tuning teaches the model _what to do_**, while **alignment tuning teaches it _how it should behave_**. They overlap in practice, but they target different problems.

Think of an LLM as:
- **Pretraining** â†’ learns language  
- **Instruction tuning** â†’ learns _obedience_  
- **Alignment tuning** â†’ learns _judgment_

| Dimension              | Instruction Tuning         | Alignment Tuning                      |
| ---------------------- | -------------------------- | ------------------------------------- |
| Main question          | â€œWhat should I do?â€        | â€œWhat should I _not_ do or _prefer_?â€ |
| Training signal        | Correct responses          | Preference comparisons / rewards      |
| Focus                  | Capability & usability     | Behavior & values                     |
| Output space           | Task-correct answers       | Acceptable / preferred answers        |
| Failure mode addressed | Not following instructions | Being harmful, rude, or misleading    |

In real pipelines:
- Instruction datasets already contain implicit alignment (polite tone, refusals)
- Alignment tuning often improves instruction following (via preference learning)
- RLHF models usually start from instruction-tuned checkpoints
So the distinction is **conceptual**, not strictly procedural.


### Instruction Tuning
â†— [Instruction Tuning](Fine%20Tuning%20Directions/Instruction%20Tuning.md)

> ğŸ¤– GPT-5

What it is
Instruction tuning trains the model on **(instruction â†’ response)** pairs so it learns to _follow user commands_.

Example data:
```
Instruction: Summarize the following text in one paragraph. 
Response: ...
```

Goal
Make the model:
- Understand what the user wants
- Produce task-appropriate outputs
- Generalize to _unseen instructions_

What it improves
- Task following
- Zero-shot / few-shot performance
- Helpfulness and usability

Typical datasets
- Human-written instructions (e.g. early InstructGPT)
- Synthetic instruction data (self-instruct, Alpaca-style)
- Task mixtures (QA, summarization, reasoning, coding)

Key property
After instruction tuning: The model **responds to prompts as commands, not just text continuation**


### Alignment Tuning
â†— [LLM Adaptation & Alignment Tuning](Fine%20Tuning%20Directions/LLM%20Adaptation%20&%20Alignment%20Tuning.md)

> ğŸ¤– GPT-5

What it is
Alignment tuning trains the model to:
- Follow **human preferences**
- Respect **safety norms**
- Avoid **harmful, misleading, or unwanted behavior**
This is less about _task execution_ and more about _behavioral constraints_.

Goal
Make the model:
- Helpful _and_ safe
- Honest about uncertainty
- Polite, non-toxic, non-manipulative
- Resistant to misuse

What it improves
- Safety (refusals, moderation)
- Tone and social behavior
- Preference alignment (â€œthis answer is better than that oneâ€)

Typical methods
- **RLHF** (Reinforcement Learning from Human Feedback)
- **RLAIF** (AI feedback instead of humans)
- **Preference optimization** (DPO, IPO, KTO, etc.)
- Constitutional AI

Key property
After alignment tuning: The model chooses **acceptable answers among many valid ones**



## Ref
[Make Post Train Solid Again - ybqçš„æ–‡ç«  - çŸ¥ä¹]: https://zhuanlan.zhihu.com/p/1995265459285694156 (2026.01)
LLM è®ºæ–‡åƒåƒä¸‡ï¼Œæœ‰ç”¨çš„å·¥ä½œå´æ²¡å‡ ç¯‡ã€‚è¿™ç¯‡æ–‡ç« ï¼Œæˆ‘æƒ³ç®€å•è®¨è®ºä¸‹åˆ°åº•è¯¥å¦‚ä½•æŠŠåè®­ç»ƒå·¥ä½œåšçš„ solidã€‚æ–‡ç« å¹¶æ²¡ä»€ä¹ˆæŠ€æœ¯ç»†èŠ‚ï¼Œå¤§å®¶éšä¾¿çœ‹çœ‹ã€‚
- æ•²å®šæ­£ç¡®çš„ Baseline
- å°‘ç”¨ sense æŒ‘æˆ˜ math
- å¤§å°æ¨¡å‹çš„ç»“è®ºè°¨æ…è¿ç§»
- simple yet effective
	- è¿‡å»ä¸€å¹´åœ¨çº¯è¯­è¨€æ¨¡å‹é¢†åŸŸï¼Œå‡ ä¹åªæœ‰ä¸¤ä¸ªå·¥ä½œæ˜¯å¾—åˆ°äº†ä¸šç•Œæ‰€æœ‰åŒè¡Œçš„è®¤å¯ï¼šä¸ŠåŠå¹´çš„åˆ©ç”¨ ORM æå‡æ¨¡å‹æ¨ç†èƒ½åŠ›ï¼Œä¸‹åŠå¹´çš„åˆ©ç”¨ TIS / IcePop ä¿è¯è®­æ¨ä¸€è‡´æ€§ï¼Œéƒ½æ˜¯ simple yet effective çš„å®Œç¾ä»£è¨€ã€‚
	- è¿™é‡Œï¼Œæˆ‘ä»¬é‡ç‚¹å›é¡¾ä¸€ä¸‹è®­æ¨ä¸ä¸€è‡´ï¼š2024 å¹´æ‰€æœ‰åŒè¡Œå°±éƒ½çŸ¥é“ vllmã€model.generateã€megatron å‰å‘ç®—å­ï¼Œè¿™ä¹‹é—´çš„ç»“æœæœ‰è¾ƒå¤§å·®å¼‚ï¼›2025 å¹´ä» TIS æå‡ºåˆ° ICEpop çš„è¿™æ®µæ—¶é—´å†…ï¼Œå‡ ä¹æ‰€æœ‰åŒè¡Œè€…éƒ½èƒ½æƒ³åˆ° IcePop çš„æ–¹æ¡ˆã€‚å¤§å®¶éƒ½æ›¾æœ‰æœºä¼šæå‡ºè¿™ä¸¤ä¸ªç®—æ³•ï¼Œä½†æŠŠæ¡ä½æœºä¼šçš„å°±æ˜¯é‚£ä¸¤ç¯‡ Notion åˆ†äº«ï¼Œè¡ŒåŠ¨åŠ›å¼ºã€å®éªŒä¸¥è°¨ã€ç†è®ºæ‰å®ï¼Œä¸¤ä¸ªå›¢é˜Ÿé…å¾—ä¸Šå¤§å®¶çš„èµæ‰¬ã€‚è¯è¯´å›æ¥ï¼Œè¿ TIS è¿™ç§ simple çš„ idea éƒ½åŸ‹æ²¡äº†ä¸€å¹´æ‰è¢«å¹¿è€Œå‘Šä¹‹ï¼Œå›´ç»•ç€ LLM çš„ policy gradient ç®—æ³•å¿…æœ‰å®è—ç­‰ç€å¤§å®¶å»æŒ–æ˜ã€‚
	- ä»ç»éªŒä¸Šæ¥è¯´ï¼Œå¦‚æœæŸä¸ªå·¥ä½œçš„æ ¸å¿ƒæ­¥éª¤ä¸æ˜¯ä¸¤å¥è¯èƒ½æ¦‚æ‹¬å‡ºæ¥çš„ï¼Œé‚£è¿™ä¸ªå·¥ä½œä¼¼ä¹ç¦»é›•èŠ±æ ‡ç­¾ä¹Ÿä¸è¿œäº†ã€‚ç›®å‰çš„ LLMï¼Œæ‰¾ä¸åˆ°ä»€ä¹ˆ solid çš„å·¥ä½œæ˜¯ä¸ simple çš„ã€‚
	