# LLM Infrastructure & Deployment

[TOC]



## Res
### Related Topics
↗ [AI (Data) Infrastructure & Techniques Stack](../../../🏗️%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack.md)
- ↗ [Foundation Models & Development & SDKs](../../../🏗️%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/🛫%20Foundation%20Models%20&%20Development%20&%20SDKs/Foundation%20Models%20&%20Development%20&%20SDKs.md)
- ↗ [Model Monitoring & Observability](../../../🏗️%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/Model%20Monitoring%20&%20Observability/Model%20Monitoring%20&%20Observability.md)
- ↗ [Model Web Demo & Web Deployment](../../../🏗️%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/Model%20Web%20Demo%20&%20Web%20Deployment/Model%20Web%20Demo%20&%20Web%20Deployment.md)

↗ [AI(LLM) x SE](../../../../../Software%20Engineering/🤖%20AI(LLM)%20x%20SE/AI(LLM)%20x%20SE.md)
- ↗ [Agentic AI Workflow Dev](../../../../../Software%20Engineering/🤖%20AI(LLM)%20x%20SE/🦾%20AI%20Powered%20Dev%20&%20Vibe%20Coding/Agentic%20AI%20Workflow%20Dev/Agentic%20AI%20Workflow%20Dev.md)

↗ [AI on Cloud](../../../🏗️%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/AI%20on%20Cloud/AI%20on%20Cloud.md)


### Other Resources
https://github.com/kkkunny/free-llm-collect
搜集免费的LLM（大语言模型）
- 之前一直用的公司的GPT API key做机器人，结果后来被收回了，又不想掏钱，只能上网寻觅下免费的LLM API，这里做个记录
- 注：这里记录的都是可以用来部署的项目或者官方API


https://github.com/cheahjs/free-llm-api-resources
This lists various services that provide free access or credits towards API-based LLM usage.



## Intro
### Deploy LLM on Different Levels - Desktop and Production
To explain these two deployments, take the comparison of ollama and vLLM for example (generated by Gemini 2.5 Flash):
#vLLM #ollama #LLM #software_deployment

While both Ollama and vLLM are tools for LLM inference (running a model), their **design goals** and **primary use cases** are fundamentally different:

| **Aspect**             | **Ollama**                                                                                                    | **vLLM (Very Large Language Model)**                                                                         |
| ---------------------- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **Primary Goal**       | **Simplicity and Accessibility.** To make it easy to run LLMs locally for prototyping and personal use.       | **High-Throughput and Efficiency.** To maximize LLM serving performance in production.                       |
| **Target Environment** | Local machines, developer laptops, single-user setups.                                                        | Production servers, cloud deployments, multi-GPU clusters.                                                   |
| **Performance**        | Good for single-user, low-concurrency requests. Prioritizes a simple user experience over raw speed at scale. | **Significantly Higher Throughput and Lower Latency** under heavy, concurrent load.                          |
| **Key Optimization**   | **Quantization** and easy packaging for resource-constrained hardware.                                        | **PagedAttention** (efficient memory management) and **Continuous Batching** (efficient request scheduling). |
| **Hardware Focus**     | Consumer-grade hardware (CPU and GPU) and Apple Silicon.                                                      | High-end, dedicated GPUs (like NVIDIA A100s/H100s).                                                          |
| **User Experience**    | Simple CLI and API, minimal setup. **Beginner-friendly.**                                                     | More complex setup, focused on advanced configuration for production needs. **Engineer-focused.**            |
| **Model Scope**        | Curated model library that are pre-packaged.                                                                  | Works with a wide range of models from the Hugging Face ecosystem.                                           |



## LLM Desktop Deployment



## LLM Deployment & Inference Services Providers
### LLM High-Performance Inference /Serving Engines
> Reference: [llm-inference-solutions](https://github.com/mani-kantap/llm-inference-solutions)

- [SGLang](https://github.com/sgl-project/sglang) - SGLang is a fast serving framework for large language models and vision language models.
- [vLLM](https://github.com/vllm-project/vllm) - A high-throughput and memory-efficient inference and serving engine for LLMs.
- [TGI](https://huggingface.co/docs/text-generation-inference/en/index) - a toolkit for deploying and serving Large Language Models (LLMs).
- [exllama](https://github.com/turboderp/exllama) - A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - LLM inference in C/C++.
- [ollama](https://github.com/ollama/ollama) - Get up and running with Llama 3, Mistral, Gemma, and other large language models.
- [Langfuse](https://github.com/langfuse/langfuse) - Open Source LLM Engineering Platform Tracing, Evaluations, Prompt Management, Evaluations and Playground.
- [FastChat](https://github.com/lm-sys/FastChat) - A distributed multi-model LLM serving system with web UI and OpenAI-compatible RESTful APIs.
- [mistral.rs](https://github.com/EricLBuehler/mistral.rs) - Blazingly fast LLM inference.
- [MindSQL](https://github.com/Mindinventory/MindSQL) - A python package for Txt-to-SQL with self hosting functionalities and RESTful APIs compatible with proprietary as well as open source LLM.
- [SkyPilot](https://github.com/skypilot-org/skypilot) - Run LLMs and batch jobs on any cloud. Get maximum cost savings, highest GPU availability, and managed execution -- all with a simple interface.
- [Haystack](https://haystack.deepset.ai/) - an open-source NLP framework that allows you to use LLMs and transformer-based models from Hugging Face, OpenAI and Cohere to interact with your own data.
- [Sidekick](https://github.com/ai-sidekick/sidekick) - Data integration platform for LLMs.
- [QA-Pilot](https://github.com/reid41/QA-Pilot) - An interactive chat project that leverages Ollama/OpenAI/MistralAI LLMs for rapid understanding and navigation of GitHub code repository or compressed file resources.
- [Shell-Pilot](https://github.com/reid41/shell-pilot) - Interact with LLM using Ollama models(or openAI, mistralAI)via pure shell scripts on your Linux(or MacOS) system, enhancing intelligent system management without any dependencies.
- [LangChain](https://github.com/hwchase17/langchain) - Building applications with LLMs through composability
- [Floom](https://github.com/FloomAI/Floom) AI gateway and marketplace for developers, enables streamlined integration of AI features into products
- [Swiss Army Llama](https://github.com/Dicklesworthstone/swiss_army_llama) - Comprehensive set of tools for working with local LLMs for various tasks.
- [LiteChain](https://github.com/rogeriochaves/litechain) - Lightweight alternative to LangChain for composing LLMs
- [magentic](https://github.com/jackmpcollins/magentic) - Seamlessly integrate LLMs as Python functions
- [wechat-chatgpt](https://github.com/fuergaosi233/wechat-chatgpt) - Use ChatGPT On Wechat via wechaty
- [promptfoo](https://github.com/typpo/promptfoo) - Test your prompts. Evaluate and compare LLM outputs, catch regressions, and improve prompt quality.
- [Agenta](https://github.com/agenta-ai/agenta) - Easily build, version, evaluate and deploy your LLM-powered apps.
- [Serge](https://github.com/serge-chat/serge) - a chat interface crafted with llama.cpp for running Alpaca models. No API keys, entirely self-hosted!
- [Langroid](https://github.com/langroid/langroid) - Harness LLMs with Multi-Agent Programming
- [Embedchain](https://github.com/embedchain/embedchain) - Framework to create ChatGPT like bots over your dataset.
- [Opik](https://github.com/comet-ml/opik) - Confidently evaluate, test, and ship LLM applications with a suite of observability tools to calibrate language model outputs across your dev and production lifecycle.
- [IntelliServer](https://github.com/intelligentnode/IntelliServer) - simplifies the evaluation of LLMs by providing a unified microservice to access and test multiple AI models.
- [OpenLLM](https://github.com/bentoml/OpenLLM) - Fine-tune, serve, deploy, and monitor any open-source LLMs in production. Used in production at [BentoML](https://bentoml.com/) for LLMs-based applications.
- [DeepSpeed-Mii](https://github.com/microsoft/DeepSpeed-MII) - MII makes low-latency and high-throughput inference, similar to vLLM powered by DeepSpeed.
- [Text-Embeddings-Inference](https://github.com/huggingface/text-embeddings-inference) - Inference for text-embeddings in Rust, HFOIL Licence.
- [Infinity](https://github.com/michaelfeil/infinity) - Inference for text-embeddings in Python
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) - Nvidia Framework for LLM Inference
- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer) - NVIDIA Framework for LLM Inference(Transitioned to TensorRT-LLM)
- [Flash-Attention](https://github.com/Dao-AILab/flash-attention) - A method designed to enhance the efficiency of Transformer models
- [Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat) - Formerly langchain-ChatGLM, local knowledge based LLM (like ChatGLM) QA app with langchain.
- [Search with Lepton](https://github.com/leptonai/search_with_lepton) - Build your own conversational search engine using less than 500 lines of code by [LeptonAI](https://github.com/leptonai).
- [Robocorp](https://github.com/robocorp/robocorp) - Create, deploy and operate Actions using Python anywhere to enhance your AI agents and assistants. Batteries included with an extensive set of libraries, helpers and logging.
- [LMDeploy](https://github.com/InternLM/lmdeploy) - A high-throughput and low-latency inference and serving framework for LLMs and VLs
- [Tune Studio](https://studio.tune.app/) - Playground for devs to finetune & deploy LLMs
- [LLocalSearch](https://github.com/nilsherzig/LLocalSearch) - Locally running websearch using LLM chains
- [AI Gateway](https://github.com/Portkey-AI/gateway) — Gateway streamlines requests to 100+ open & closed source models with a unified API. It is also production-ready with support for caching, fallbacks, retries, timeouts, loadbalancing, and can be edge-deployed for minimum latency.
- [talkd.ai dialog](https://github.com/talkdai/dialog) - Simple API for deploying any RAG or LLM that you want adding plugins.
- [Wllama](https://github.com/ngxson/wllama) - WebAssembly binding for llama.cpp - Enabling in-browser LLM inference
- [GPUStack](https://github.com/gpustack/gpustack) - An open-source GPU cluster manager for running LLMs
- [MNN-LLM](https://github.com/alibaba/MNN) -- A Device-Inference framework, including LLM Inference on device(Mobile Phone/PC/IOT)
- [CAMEL](https://www.camel-ai.org/) - First LLM Multi-agent framework.


### LLM Inference Services Providers & API
> 🔗 https://github.com/cheahjs/free-llm-api-resources

This lists various services that provide free access or credits towards API-based LLM usage.

> [!Note]
> Please don't abuse these services, else we might lose them.

> [!warning]
> This list explicitly excludes any services that are not legitimate (eg reverse engineers an existing chatbot)

- [Free Providers](https://github.com/cheahjs/free-llm-api-resources#free-providers)
    - [OpenRouter](https://github.com/cheahjs/free-llm-api-resources#openrouter)
    - [Google AI Studio](https://github.com/cheahjs/free-llm-api-resources#google-ai-studio)
    - [NVIDIA NIM](https://github.com/cheahjs/free-llm-api-resources#nvidia-nim)
    - [Mistral (La Plateforme)](https://github.com/cheahjs/free-llm-api-resources#mistral-la-plateforme)
    - [Mistral (Codestral)](https://github.com/cheahjs/free-llm-api-resources#mistral-codestral)
    - [HuggingFace Inference Providers](https://github.com/cheahjs/free-llm-api-resources#huggingface-inference-providers)
    - [Vercel AI Gateway](https://github.com/cheahjs/free-llm-api-resources#vercel-ai-gateway)
    - [Cerebras](https://github.com/cheahjs/free-llm-api-resources#cerebras)
    - [Groq](https://github.com/cheahjs/free-llm-api-resources#groq)
    - [Cohere](https://github.com/cheahjs/free-llm-api-resources#cohere)
    - [GitHub Models](https://github.com/cheahjs/free-llm-api-resources#github-models)
    - [Cloudflare Workers AI](https://github.com/cheahjs/free-llm-api-resources#cloudflare-workers-ai)
    - [Google Cloud Vertex AI](https://github.com/cheahjs/free-llm-api-resources#google-cloud-vertex-ai)
- [Providers with trial credits](https://github.com/cheahjs/free-llm-api-resources#providers-with-trial-credits)
    - [Fireworks](https://github.com/cheahjs/free-llm-api-resources#fireworks)
    - [Baseten](https://github.com/cheahjs/free-llm-api-resources#baseten)
    - [Nebius](https://github.com/cheahjs/free-llm-api-resources#nebius)
    - [Novita](https://github.com/cheahjs/free-llm-api-resources#novita)
    - [AI21](https://github.com/cheahjs/free-llm-api-resources#ai21)
    - [Upstage](https://github.com/cheahjs/free-llm-api-resources#upstage)
    - [NLP Cloud](https://github.com/cheahjs/free-llm-api-resources#nlp-cloud)
    - [Alibaba Cloud (International) Model Studio](https://github.com/cheahjs/free-llm-api-resources#alibaba-cloud-international-model-studio)
    - [Modal](https://github.com/cheahjs/free-llm-api-resources#modal)
    - [Inference.net](https://github.com/cheahjs/free-llm-api-resources#inferencenet)
    - [Hyperbolic](https://github.com/cheahjs/free-llm-api-resources#hyperbolic)
    - [SambaNova Cloud](https://github.com/cheahjs/free-llm-api-resources#sambanova-cloud)
    - [Scaleway Generative APIs](https://github.com/cheahjs/free-llm-api-resources#scaleway-generative-apis)


> 🔗 https://github.com/kkkunny/free-llm-collect

- 综合
    - [OpenRouter](https://openrouter.ai/deepseek/deepseek-r1:free)
        - 使用体验：响应较快
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：视模型而异
        - 是否需要外网环境部署：未知
    - [Poixe AI](https://poixe.com/)
        - 使用体验：响应较快
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：未知
- DeepSeek
    - [硅基流动提供的免费蒸馏R1](https://cloud.siliconflow.cn/models)
        - 使用体验：响应很慢
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：是
        - 是否需要外网环境部署：否
    - [无问芯穹提供的免费R1](https://cloud.infini-ai.com/promotion)
        - 使用体验：响应较快
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
- GPT
    - [官网无需登录ChatGPT逆向](https://github.com/missuo/FreeGPT35)
        - 使用体验：响应慢，偶尔抽风，不稳定
        - 是否需要登录或cookie：无需
        - 是否能够函数调用：否
        - 是否需要外网环境部署：是
    - [字节Coze海外平台提供的免费GPT](https://github.com/deanxv/coze-discord-proxy)
        - 使用体验：响应慢，有次数限制，十分稳定
        - 是否需要登录或cookie：因为使用了discord，需要传discord的cookie，cookie生命周期很长
        - 是否能够函数调用：否
        - 是否需要外网环境部署：是
    - [DuckDuckGo提供的免费GPT3.5](https://github.com/missuo/FreeDuckDuckGo)
        - 使用体验：响应较快，较稳定
        - 是否需要登录或cookie：无需
        - 是否能够函数调用：否
        - 是否需要外网环境部署：是
    - [ChandlerAi提供的免费GPT3.5](https://github.com/kkkunny/ChandlerAiAPI)
        - 使用体验：响应较快，较稳定
        - 是否需要登录或cookie：需要自己抓取token
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
- Claude
    - [DuckDuckGo提供的免费Claude3Haiku](https://github.com/missuo/FreeDuckDuckGo)
        - 使用体验：响应较快，较稳定
        - 是否需要登录或cookie：无需
        - 其他：这个项目没有增加Claude模型，可以自行寻找其他项目，或者自己改
        - 是否能够函数调用：否
        - 是否需要外网环境部署：是
- Bing Copilot
    - [BingAI逆向](https://github.com/Harry-zklcdc/go-proxy-bingai)
        - 使用体验：响应很慢，不稳定
        - 是否需要登录或cookie：需要在UI界面中登录或填cookie
        - 是否能够函数调用：否
        - 是否需要外网环境部署：是
- Gemini
    - [Gemini免费套餐](https://ai.google.dev/models/gemini?hl=zh-cn)
        - 使用体验：响应很快，十分稳定，免费版有调用量限制，对于个人而言完全够用了
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：是
        - 是否需要外网环境访问：是
- LLama
    - ~~[nvidia免费套餐](https://build.nvidia.com/)~~
        - ~~使用体验：响应很快，十分稳定，尚不清楚调用量上限~~
        - ~~是否需要登录或cookie：使用nvidia提供的API key~~
        - ~~是否需要外网环境访问：否~~
        - 有免费额度，用完即没
    - [HuggingFace提供的免费Chat逆向](https://github.com/kkkunny/HuggingChatAPI)
        - 这里有个我自己写的，已有的一个python的项目我没调用成功
        - 使用体验：响应较快，较稳定，尚不清楚调用量上限
        - 是否需要登录或cookie：需要cookie
        - 是否能够函数调用：否
        - 是否需要外网环境部署：是
    - [CloudFlare免费套餐](https://playground.ai.cloudflare.com/)
        - 使用体验：响应很快，十分稳定，尚不清楚调用量上限
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境访问：否
    - [Groq免费套餐](https://groq.com/)
        - 使用体验：响应较快，不稳定，有封号风险，挂的日本的梯子也没聊啥就被封了
        - 是否需要登录或cookie：需要API key
        - 是否需要外网环境访问：是
    - [ChandlerAi提供的免费LLama3](https://github.com/kkkunny/ChandlerAiAPI)
        - 使用体验：响应较快，较稳定
        - 是否需要登录或cookie：需要抓token
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
- 国产模型
    - [智谱清言免费套餐](https://open.bigmodel.cn/console/overview)
        - 使用体验：响应较快，很稳定，仅限glm-4-flash免费
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：是
        - 是否需要外网环境部署：否
    - [阿里 通义千问逆向](https://github.com/LLM-Red-Team/qwen-free-api)
        - 使用体验：响应较快，很稳定
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
    - [月之暗面 Kimi逆向](https://github.com/LLM-Red-Team/kimi-free-api)
        - 使用体验：响应较快，很稳定，据说有封号风险
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
    - [阶跃星辰 跃问逆向](https://github.com/LLM-Red-Team/step-free-api)
        - 使用体验：响应较快，很稳定
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
    - [智谱清言 GLM逆向](https://github.com/LLM-Red-Team/glm-free-api)
        - 使用体验：响应较快，很稳定
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
    - [讯飞 星火逆向](https://github.com/LLM-Red-Team/spark-free-api)
        - 使用体验：响应较快，很稳定
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
    - [秘塔AI逆向](https://github.com/LLM-Red-Team/metaso-free-api)
        - 使用体验：未使用过
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
    - [海螺AI逆向](https://github.com/LLM-Red-Team/hailuo-free-api)
        - 使用体验：未使用过
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
    - [聆心智能逆向](https://github.com/LLM-Red-Team/emohaa-free-api)
        - 使用体验：未使用过
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否
- 其他模型
    - [siliconflow](https://siliconflow.cn/zh-cn/)
        - 使用体验：响应较快，但免费的都是一些国内的非旗舰模型，另有FLUX.1、sd等文生图模型**限时免费！**
        - 是否需要外网环境部署：否
    - [mistral免费套餐](https://console.mistral.ai/)
        - 使用体验：响应较快，不太稳定
        - 是否需要登录或cookie：需要API key
        - 是否需要外网环境部署：否
        - 是否能够函数调用：是
    - [glhf免费API](https://glhf.chat/landing/home)
        - 使用体验：响应较慢。包含一些开源的旗舰模型，还可以自己部署模型
        - 是否需要登录或cookie：需要API key
        - 是否能够函数调用：否
        - 是否需要外网环境部署：否

## Ref
