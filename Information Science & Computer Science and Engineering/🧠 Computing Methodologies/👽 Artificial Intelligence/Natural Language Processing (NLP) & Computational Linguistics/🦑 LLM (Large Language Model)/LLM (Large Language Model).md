# LLM (Large Language Model)

[TOC]



## Res
### Related Topics
‚Üó [Linguistics](../../../../../Other%20Networks%20of%20Knowledge/Arts%20&%20Humanities/üìÉ%20Language%20&%20Literature/Linguistics/Linguistics.md)
‚Üó [Ordinary Language Philosophy](../../../../../Other%20Networks%20of%20Knowledge/‚ôÇ%20Philosophy%20&%20Its%20History/Modern%20Philosophy/Analytic%20Philosophy/Ordinary%20Language%20Philosophy/Ordinary%20Language%20Philosophy.md)
- ‚Üó [Ludwig Wittgenstein](../../../../../Other%20Networks%20of%20Knowledge/‚ôÇ%20Philosophy%20&%20Its%20History/Modern%20Philosophy/Analytic%20Philosophy/Ordinary%20Language%20Philosophy/Ludwig%20Wittgenstein.md)
‚Üó [Philosophy of Language](../../../../../Other%20Networks%20of%20Knowledge/‚ôÇ%20Philosophy%20&%20Its%20History/Contemporary%20Philosophy/üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë®%20Philosophy%20of%20Language%20&%20Mathematics/Philosophy%20of%20Language.md)

‚Üó [Algebraic Structure & Abstract Algebra & Modern Algebra](../../../../üßÆ%20Mathematics/üßä%20Algebra/üéÉ%20Algebraic%20Structure%20&%20Abstract%20Algebra%20&%20Modern%20Algebra/Algebraic%20Structure%20&%20Abstract%20Algebra%20&%20Modern%20Algebra.md)

‚Üó [Deep Learning (Neural Networks)](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/Deep%20Learning%20(Neural%20Networks).md)
- ‚Üó [Transformers](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/2Ô∏è‚É£%20Neural%20Network%20Models%20üóø/Transformers/Transformers.md)

LLM & Academics üßë‚Äçüéì
- ‚Üó [LLM & Federated Learning](../../../../Academics%20üéì%20(In%20CS)/üóíÔ∏è%20My%20Academic%20Projects%20Workspace/LLM%20&%20Federated%20Learning/LLM%20&%20Federated%20Learning.md)
- ‚Üó [LLM & Fuzzing](../../../../Academics%20üéì%20(In%20CS)/üóíÔ∏è%20My%20Academic%20Projects%20Workspace/LLM%20&%20Software%20Security%20and%20Analysis/LLM%20&%20Fuzzing.md)
- ‚Üó [LLM & Software Security and Analysis](../../../../Academics%20üéì%20(In%20CS)/üóíÔ∏è%20My%20Academic%20Projects%20Workspace/LLM%20&%20Software%20Security%20and%20Analysis/LLM%20&%20Software%20Security%20and%20Analysis.md)
‚Üó [LLM & Security](../../../../CyberSecurity/ü§ñ%20AI%20x%20Security/LLM%20&%20Security/LLM%20&%20Security.md)

‚Üó [AI(LLM) x SE](../../../../Software%20Engineering/ü§ñ%20AI(LLM)%20x%20SE/AI(LLM)%20x%20SE.md)
- ‚Üó [Agentic AI Workflow Dev](../../../../Software%20Engineering/ü§ñ%20AI(LLM)%20x%20SE/Agentic%20AI%20Workflow%20Dev/Agentic%20AI%20Workflow%20Dev.md)
- ‚Üó [LangChain & LangGraph](../../../../Software%20Engineering/ü§ñ%20AI(LLM)%20x%20SE/Agentic%20AI%20Workflow%20Dev/LLM%20Workflow%20&%20Agents%20Dev%20Frameworks/LangChain%20&%20LangGraph.md)

‚Üó [Artificial Intelligence Related Conferences & Journals](../../../../Academics%20üéì%20(In%20CS)/üéª%20Academic%20Venues%20in%20Computer%20Science/Application/Artificial%20Intelligence%20Related%20Conferences%20&%20Journals.md)
‚Üó [Research Topics in LLM](../../../../Academics%20üéì%20(In%20CS)/Academic%20Research%20Directions%20in%20CS/Research%20Topics%20in%20LLM.md)
‚Üó [XAI (AI Explainable & Interpretable)](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/üåÅ%20XAI%20(AI%20Explainable%20&%20Interpretable)/XAI%20(AI%20Explainable%20&%20Interpretable).md)


### Learning Resource
#### Texts & Docs
üìñ Â§ßËßÑÊ®°ËØ≠Ë®ÄÊ®°ÂûãÔºö‰ªéÁêÜËÆ∫Âà∞ÂÆûË∑µ
https://intro-llm.github.io
Â§ßËØ≠Ë®ÄÊ®°ÂûãÔºàLarge Language ModelsÔºåLLMÔºâÊòØ‰∏ÄÁßçÁî±ÂåÖÂê´Êï∞Áôæ‰∫ø‰ª•‰∏äÊùÉÈáçÁöÑÊ∑±Â∫¶Á•ûÁªèÁΩëÁªúÊûÑÂª∫ÁöÑËØ≠Ë®ÄÊ®°ÂûãÔºå‰ΩøÁî®Ëá™ÁõëÁù£Â≠¶‰π†ÊñπÊ≥ïÈÄöËøáÂ§ßÈáèÊó†Ê†áËÆ∞ÊñáÊú¨ËøõË°åËÆ≠ÁªÉ„ÄÇËá™2018Âπ¥‰ª•Êù•ÔºåÂåÖÂê´Google„ÄÅOpenAI„ÄÅMeta„ÄÅÁôæÂ∫¶„ÄÅÂçé‰∏∫Á≠âÂÖ¨Âè∏ÂíåÁ†îÁ©∂Êú∫ÊûÑÈÉΩÁ∫∑Á∫∑ÂèëÂ∏É‰∫ÜÂåÖÊã¨BERTÔºå GPTÁ≠âÂú®ÂÜÖÂ§öÁßçÊ®°ÂûãÔºåÂπ∂Âú®Âá†‰πéÊâÄÊúâËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜ‰ªªÂä°‰∏≠ÈÉΩË°®Áé∞Âá∫Ëâ≤„ÄÇ2021Âπ¥ÂºÄÂßãÂ§ßÊ®°ÂûãÂëàÁé∞ÁàÜÂèëÂºèÁöÑÂ¢ûÈïøÔºåÁâπÂà´ÊòØ2022Âπ¥11ÊúàChatGPTÂèëÂ∏ÉÂêéÔºåÊõ¥ÊòØÂºïËµ∑‰∫ÜÂÖ®‰∏ñÁïåÁöÑÂπøÊ≥õÂÖ≥Ê≥®„ÄÇÁî®Êà∑ÂèØ‰ª•‰ΩøÁî®Ëá™ÁÑ∂ËØ≠Ë®Ä‰∏éÁ≥ªÁªü‰∫§‰∫íÔºå‰ªéËÄåÂÆûÁé∞ÂåÖÊã¨ÈóÆÁ≠î„ÄÅÂàÜÁ±ª„ÄÅÊëòË¶Å„ÄÅÁøªËØë„ÄÅËÅäÂ§©Á≠â‰ªéÁêÜËß£Âà∞ÁîüÊàêÁöÑÂêÑÁßç‰ªªÂä°„ÄÇÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ±ïÁé∞Âá∫‰∫ÜÂº∫Â§ßÁöÑÂØπ‰∏ñÁïåÁü•ËØÜÊéåÊè°ÂíåÂØπËØ≠Ë®ÄÁöÑÁêÜËß£„ÄÇÊú¨‰π¶Â∞Ü‰ªãÁªçÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÂü∫Á°ÄÁêÜËÆ∫ÂåÖÊã¨ËØ≠Ë®ÄÊ®°Âûã„ÄÅÂàÜÂ∏ÉÂºèÊ®°ÂûãËÆ≠ÁªÉ‰ª•ÂèäÂº∫ÂåñÂ≠¶‰π†ÔºåÂπ∂‰ª•Deepspeed-ChatÊ°ÜÊû∂‰∏∫‰æã‰ªãÁªçÂÆûÁé∞Â§ßËØ≠Ë®ÄÊ®°ÂûãÂíåÁ±ªChatGPTÁ≥ªÁªüÁöÑÂÆûË∑µ„ÄÇ

---
üî• üëç üìÑ https://github.com/RUCAIBox/LLMSurvey ÔºàÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁªºËø∞ | ‰∏≠ÂõΩ‰∫∫Ê∞ëÂ§ßÂ≠¶È´òÁì¥‰∫∫Â∑•Êô∫ËÉΩÂ≠¶Èô¢Ôºâ
A collection of papers and resources related to Large Language Models.
The organization of papers refers to our survey¬†[**"A Survey of Large Language Models"**](https://arxiv.org/abs/2303.18223).
To facilitate the reading of our (English-verison) survey, we also translate a¬†[**Chinese version**](https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf)¬†for this survey. We will continue to update the Chinese version.

---
üî• üìÑ https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE
Papers must know to understand the world of deep learning & AIGC

---
üî• ü™ú https://github.com/Hannibal046/Awesome-LLM/tree/main
Large Language Models(LLM) have taken the¬†~~NLP community~~¬†~~AI community~~¬†**the Whole World**¬†by storm. Here is a curated list of papers about large language models, especially relating to ChatGPT. It also contains frameworks for LLM training, tools to deploy LLM, courses and tutorials about LLM and all publicly available LLM checkpoints and APIs.

![|500](../../../../../Assets/Pics/Pasted%20image%2020240512212009.png)

- [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM/tree/main#awesome-llm-)
    - [Milestone Papers](https://github.com/Hannibal046/Awesome-LLM/tree/main#milestone-papers)
    - [Other Papers](https://github.com/Hannibal046/Awesome-LLM/tree/main#other-papers)
    - [LLM Leaderboard](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-leaderboard)
    - [Open LLM](https://github.com/Hannibal046/Awesome-LLM/tree/main#open-llm)
    - [LLM Data](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-data)
    - [LLM Evaluation](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-evaluation)
    - [LLM Training Framework](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-training-frameworks)
    - [LLM Deployment](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-deployment)
    - [LLM Applications](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-applications)
    - [LLM Tutorials and Courses](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-tutorials-and-courses)
    - [LLM Books](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-books)
    - [Great thoughts about LLM](https://github.com/Hannibal046/Awesome-LLM/tree/main#great-thoughts-about-llm)
    - [Miscellaneous](https://github.com/Hannibal046/Awesome-LLM/tree/main#miscellaneous)

**Great thoughts about LLM**
> üîó https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#great-thoughts-about-llm

- [Why did all of the public reproduction of GPT-3 fail?](https://jingfengyang.github.io/gpt)
- [A Stage Review of Instruction Tuning](https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2)
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)
- [Why you should work on AI AGENTS!](https://www.youtube.com/watch?v=fqVLjtvWgq8)
- [Google "We Have No Moat, And Neither Does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
- [AI competition statement](https://petergabriel.com/news/ai-competition-statement/)
- [Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
- [Noam Chomsky: The False Promise of ChatGPT](https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html)
- [Is ChatGPT 175 Billion Parameters? Technical Analysis](https://orenleung.super.site/is-chatgpt-175-billion-parameters-technical-analysis)
- [The Next Generation Of Large Language Models](https://www.notion.so/Awesome-LLM-40c8aa3f2b444ecc82b79ae8bbd2696b)
- [Large Language Model Training in 2023](https://research.aimultiple.com/large-language-model-training/)
- [How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)
- [Open Pretrained Transformers](https://www.youtube.com/watch?v=p9IxoSkvZ-M&t=4s)
- [Scaling, emergence, and reasoning in large language models](https://docs.google.com/presentation/d/1EUV7W7X_w0BDrscDhPg7lMGzJCkeaPkGCJ3bN8dluXc/edit?pli=1&resourcekey=0-7Nz5A7y8JozyVrnDtcEKJA#slide=id.g16197112905_0_0)

**Miscellaneous**
> üîó https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#miscellaneous

- [Arize-Phoenix](https://phoenix.arize.com/)¬†- Open-source tool for ML observability that runs in your notebook environment. Monitor and fine tune LLM, CV and Tabular Models.
- [Emergent Mind](https://www.emergentmind.com/)¬†- The latest AI news, curated & explained by GPT-4.
- [ShareGPT](https://sharegpt.com/)¬†- Share your wildest ChatGPT conversations with one click.
- [Major LLMs + Data Availability](https://docs.google.com/spreadsheets/d/1bmpDdLZxvTCleLGVPgzoMTQ0iDP2-7v7QziPrzPdHyM/edit#gid=0)
- [500+ Best AI Tools](https://vaulted-polonium-23c.notion.site/500-Best-AI-Tools-e954b36bf688404ababf74a13f98d126)
- [Cohere Summarize Beta](https://txt.cohere.ai/summarize-beta/)¬†- Introducing Cohere Summarize Beta: A New Endpoint for Text Summarization
- [chatgpt-wrapper](https://github.com/mmabrouk/chatgpt-wrapper)¬†- ChatGPT Wrapper is an open-source unofficial Python API and CLI that lets you interact with ChatGPT.
- [Open-evals](https://github.com/open-evals/evals)¬†- A framework extend openai's¬†[Evals](https://github.com/openai/evals)¬†for different language model.
- [Cursor](https://www.cursor.so/)¬†- Write, edit, and chat about your code with a powerful AI.
- [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)¬†- an experimental open-source application showcasing the capabilities of the GPT-4 language model.
- [OpenAGI](https://github.com/agiresearch/OpenAGI)¬†- When LLM Meets Domain Experts.
- [EasyEdit](https://github.com/zjunlp/EasyEdit)¬†- An easy-to-use framework to edit large language models.
- [chatgpt-shroud](https://github.com/guyShilo/chatgpt-shroud)¬†- A Chrome extension for OpenAI's ChatGPT, enhancing user privacy by enabling easy hiding and unhiding of chat history. Ideal for privacy during screen shares.

---
https://github.com/Shubhamsaboo/awesome-llm-apps
A curated collection of¬†**Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.**¬†This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

---
ü§î https://transformer-circuits.pub/2025/attribution-graphs/biology.html
**On the Biology of a Large Language Model | Anthropic**
We investigate the internal mechanisms used by Claude 3.5 Haiku ‚Äî Anthropic's lightweight production model ‚Äî in a variety of contexts, using our circuit tracing methodology.
#### Tutorials & Books
https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-tutorials-and-courses
**LLM Tutorials and Courses**
- [Andrej Karpathy Series](https://www.youtube.com/@AndrejKarpathy)¬†- My favorite!
- [Umar Jamil Series](https://www.youtube.com/@umarjamilai)¬†- high quality and educational videos you don't want to miss.
- [Alexander Rush Series](https://rush-nlp.com/projects/)¬†- high quality and educational materials you don't want to miss.
- [llm-course](https://github.com/mlabonne/llm-course)¬†- Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.
- [UWaterloo CS 886](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/)¬†- Recent Advances on Foundation Models.
- [CS25-Transformers United](https://web.stanford.edu/class/cs25/)
- [ChatGPT Prompt Engineering](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
- [Princeton: Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)
- [CS324 - Large Language Models](https://stanford-cs324.github.io/winter2022/)
	- üè´ [CS 324 Large Language Model](../../../../üó∫%20CS%20Overview/üíã%20Intro%20to%20Computer%20Science/üë©üèº‚Äçüè´%20Courses%20of%20Universities/Stanford/CS%20324%20Large%20Language%20Model/CS%20324%20Large%20Language%20Model.md)
- [State of GPT](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)
- [A Visual Guide to Mamba and State Space Models](https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state?utm_source=multiple-personal-recommendations-email&utm_medium=email&open=false)
- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [minbpe](https://www.youtube.com/watch?v=zduSFxRajkE&t=1157s)¬†- Minimal, clean code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.
- [femtoGPT](https://github.com/keyvank/femtoGPT)¬†- Pure Rust implementation of a minimal Generative Pretrained Transformer.
- [Neurips2022-Foundational Robustness of Foundation Models](https://nips.cc/virtual/2022/tutorial/55796)
- [ICML2022-Welcome to the "Big Model" Era: Techniques and Systems to Train and Serve Bigger Models](https://icml.cc/virtual/2022/tutorial/18440)
- [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
- [LLM‚ÄëRL‚ÄëVisualized (EN)](https://github.com/changyeyu/LLM-RL-Visualized/blob/master/src/README_EN.md)¬†|¬†[LLM‚ÄëRL‚ÄëVisualized (‰∏≠Êñá)](https://github.com/changyeyu/LLM-RL-Visualized)¬†- 100+ LLM / RL Algorithm Mapsüìö.

https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-books
**LLM Books**
- [Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs](https://amzn.to/3GUlRng)¬†- it comes with a¬†[GitHub repository](https://github.com/benman1/generative_ai_with_langchain)¬†that showcases a lot of the functionality
- [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)¬†- A guide to building your own working LLM.
- [BUILD GPT: HOW AI WORKS](https://www.amazon.com/dp/9152799727?ref_=cm_sw_r_cp_ud_dp_W3ZHCD6QWM3DPPC0ARTT_1)¬†- explains how to code a Generative Pre-trained Transformer, or GPT, from scratch.
- [Hands-On Large Language Models: Language Understanding and Generation](https://www.llm-book.com/)¬†- Explore the world of Large Language Models with over 275 custom made figures in this illustrated guide!
- [The Chinese Book for Large Language Models](http://aibox.ruc.edu.cn/zws/index.htm)¬†- An Introductory LLM Textbook Based on¬†[_A Survey of Large Language Models_](https://arxiv.org/abs/2303.18223).

https://diffusion.csail.mit.edu/
Introduction to Flow Matching and Diffusion Models
MIT Computer Science Class 6.S184: Generative AI with Stochastic Differential Equations
- Diffusion and flow-based models have become the state of the art for generative AI across a wide range of data modalities, including images, videos, shapes, molecules, music, and more! This course aims to build up the mathematical framework underlying these models from first principles. At the end of the class, students will have built a toy image diffusion model from scratch, and along the way, will have gained hands-on experience with the mathematical toolbox of stochastic differential equations that is useful in many other fields. This course is ideal for students who want to develop a principled understanding of the theory and practice of generative AI.
#### Videos
https://youtu.be/1il-s4mgNdI?si=DxlD_98ITLZsnCIw
What does it mean for computers to understand language? | LM1
vcubingx

https://youtu.be/kCc8FmEb1nY?si=Dhj1moY2pHkyiCiT
Let's build GPT: from scratch, in code, spelled out.
Andrej Karpathy

https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=AUDMGwyz7-yL33Xd
Neural networks | 3Blue1Brown
- [But what is a neural network? | Deep learning chapter 1](https://youtu.be/aircAruvnKk?si=RiyEviyfGbC8YwS0)
- [Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://youtu.be/IHZwWFHWa-w?si=DqZgN_65JZfHX-81)
- [Backpropagation, intuitively | Deep Learning Chapter 3](https://youtu.be/Ilg3gGewQ5U?si=yYl6Vi6Sb-NxWbh5)
- [Backpropagation calculus | Deep Learning Chapter 4](https://youtu.be/tIeHLnjs5U8?si=w84SrOkyDnMwKSk7)
- [Large Language Models explained briefly](https://youtu.be/LPZh9BOjkQs?si=7CRyWTVnx3BIGQGy)
- [Transformers, the tech behind LLMs | Deep Learning Chapter 5](https://youtu.be/wjZofJX0v4M?si=cLC36CWJiJPKQJgT)
	- „Äê„ÄêÂÆòÊñπÂèåËØ≠„ÄëGPTÊòØ‰ªÄ‰πàÔºüÁõ¥ËßÇËß£ÈáäTransformer | Ê∑±Â∫¶Â≠¶‰π†Á¨¨5Á´†-ÂìîÂì©ÂìîÂì©„Äë https://b23.tv/rcO76mO
- [Attention in transformers, step-by-step | Deep Learning Chapter 6](https://youtu.be/eMlx5fFNoYc?si=UqpVj1vDxOtWAnlc)
	- „Äê„ÄêÂÆòÊñπÂèåËØ≠„ÄëÁõ¥ËßÇËß£ÈáäÊ≥®ÊÑèÂäõÊú∫Âà∂ÔºåTransformerÁöÑÊ†∏ÂøÉ | „ÄêÊ∑±Â∫¶Â≠¶‰π†Á¨¨6Á´†„Äë-ÂìîÂì©ÂìîÂì©„Äë https://b23.tv/f0udg4P
- [How might LLMs store facts | Deep Learning Chapter 7](https://youtu.be/9-Jl0dxWQs8?si=jJPuNPfLV6AtWNJa)

Lex Fridman

Machine Learning Street Talk

StatQuest with Josh Starmer

Jeremy Howard

Serrano.Academy

Hamel Husain

Jason Liu

Dave Ebbelaar
#### Blogs & Communities
https://www.alignmentforum.org/


### Papers & Researches
#### LLM Survey Papers
Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., & Gao, J. (2025). _Large Language Models: A Survey_ (arXiv:2402.06196). arXiv. [https://doi.org/10.48550/arXiv.2402.06196](https://doi.org/10.48550/arXiv.2402.06196)

üöß üëç https://github.com/RUCAIBox/LLMSurvey
A collection of papers and resources related to Large Language Models.
The organization of papers refers to our survey¬†[**"A Survey of Large Language Models"**](https://arxiv.org/abs/2303.18223).¬†
- Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., ‚Ä¶ Wen, J.-R. (2025). _A Survey of Large Language Models_ (arXiv:2303.18223). arXiv. [https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)
#### Other Papers
> üîó https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#other-papers (2025.01)

If you're interested in the field of LLM, you may find the above list of milestone papers helpful to explore its history and state-of-the-art. However, each direction of LLM offers a unique set of insights and contributions, which are essential to understanding the field as a whole. For a detailed list of papers in various subfields, please refer to the following link:
- [Awesome-LLM-hallucination](https://github.com/LuckyyySTA/Awesome-LLM-hallucination)¬†- LLM hallucination paper list.
- [awesome-hallucination-detection](https://github.com/EdinburghNLP/awesome-hallucination-detection)¬†- List of papers on hallucination detection in LLMs.
- [LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide)¬†- A curated list of practical guide resources of LLMs
- [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)¬†- A collection of prompt examples to be used with the ChatGPT model.
- [awesome-chatgpt-prompts-zh](https://github.com/PlexPt/awesome-chatgpt-prompts-zh)¬†- A Chinese collection of prompt examples to be used with the ChatGPT model.
- [Awesome ChatGPT](https://github.com/humanloop/awesome-chatgpt)¬†- Curated list of resources for ChatGPT and GPT-3 from OpenAI.
- [Chain-of-Thoughts Papers](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers)¬†- A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models.
- [Awesome Deliberative Prompting](https://github.com/logikon-ai/awesome-deliberative-prompting)¬†- How to ask LLMs to produce reliable reasoning and make reason-responsive decisions.
- [Instruction-Tuning-Papers](https://github.com/SinclairCoder/Instruction-Tuning-Papers)¬†- A trend starts from¬†`Natrural-Instruction`¬†(ACL 2022),¬†`FLAN`¬†(ICLR 2022) and¬†`T0`¬†(ICLR 2022).
- [LLM Reading List](https://github.com/crazyofapple/Reading_groups/)¬†- A paper & resource list of large language models.
- [Reasoning using Language Models](https://github.com/atfortes/LM-Reasoning-Papers)¬†- Collection of papers and resources on Reasoning using Language Models.
- [Chain-of-Thought Hub](https://github.com/FranxYao/chain-of-thought-hub)¬†- Measuring LLMs' Reasoning Performance
- [Awesome GPT](https://github.com/formulahendry/awesome-gpt)¬†- A curated list of awesome projects and resources related to GPT, ChatGPT, OpenAI, LLM, and more.
- [Awesome GPT-3](https://github.com/elyase/awesome-gpt3)¬†- a collection of demos and articles about the¬†[OpenAI GPT-3 API](https://openai.com/blog/openai-api/).
- [Awesome LLM Human Preference Datasets](https://github.com/PolisAI/awesome-llm-human-preference-datasets)¬†- a collection of human preference datasets for LLM instruction tuning, RLHF and evaluation.
- [RWKV-howto](https://github.com/Hannibal046/RWKV-howto)¬†- possibly useful materials and tutorial for learning RWKV.
- [ModelEditingPapers](https://github.com/zjunlp/ModelEditingPapers)¬†- A paper & resource list on model editing for large language models.
- [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security)¬†- A curation of awesome tools, documents and projects about LLM Security.
- [Awesome-Align-LLM-Human](https://github.com/GaryYufei/AlignLLMHumanSurvey)¬†- A collection of papers and resources about aligning large language models (LLMs) with human.
- [Awesome-Code-LLM](https://github.com/huybery/Awesome-Code-LLM)¬†- An awesome and curated list of best code-LLM for research.
- [Awesome-LLM-Compression](https://github.com/HuangOwen/Awesome-LLM-Compression)¬†- Awesome LLM compression research papers and tools.
- [Awesome-LLM-Systems](https://github.com/AmberLJC/LLMSys-PaperList)¬†- Awesome LLM systems research papers.
- [awesome-llm-webapps](https://github.com/snowfort-ai/awesome-llm-webapps)¬†- A collection of open source, actively maintained web apps for LLM applications.
- [awesome-japanese-llm](https://github.com/llm-jp/awesome-japanese-llm)¬†- Êó•Êú¨Ë™ûLLM„Åæ„Å®„ÇÅ - Overview of Japanese LLMs.
- [Awesome-LLM-Healthcare](https://github.com/mingze-yuan/Awesome-LLM-Healthcare)¬†- The paper list of the review on LLMs in medicine.
- [Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference)¬†- A curated list of Awesome LLM Inference Paper with codes.
- [Awesome-LLM-3D](https://github.com/ActiveVisionLab/Awesome-LLM-3D)¬†- A curated list of Multi-modal Large Language Model in 3D world, including 3D understanding, reasoning, generation, and embodied agents.
- [LLMDatahub](https://github.com/Zjh-819/LLMDataHub)¬†- a curated collection of datasets specifically designed for chatbot training, including links, size, language, usage, and a brief description of each dataset
- [Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)¬†- Êï¥ÁêÜÂºÄÊ∫êÁöÑ‰∏≠ÊñáÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºå‰ª•ËßÑÊ®°ËæÉÂ∞è„ÄÅÂèØÁßÅÊúâÂåñÈÉ®ÁΩ≤„ÄÅËÆ≠ÁªÉÊàêÊú¨ËæÉ‰ΩéÁöÑÊ®°Âûã‰∏∫‰∏ªÔºåÂåÖÊã¨Â∫ïÂ∫ßÊ®°ÂûãÔºåÂûÇÁõ¥È¢ÜÂüüÂæÆË∞ÉÂèäÂ∫îÁî®ÔºåÊï∞ÊçÆÈõÜ‰∏éÊïôÁ®ãÁ≠â„ÄÇ
- [LLM4Opt](https://github.com/FeiLiu36/LLM4Opt)¬†- Applying Large language models (LLMs) for diverse optimization tasks (Opt) is an emerging research area. This is a collection of references and papers of LLM4Opt.
- [awesome-language-model-analysis](https://github.com/Furyton/awesome-language-model-analysis)¬†- This paper list focuses on the theoretical or empirical analysis of language models, e.g., the learning dynamics, expressive capacity, interpretability, generalization, and other interesting topics.


### Other Resources
üé¨ https://youtu.be/OFS90-FX6pg?si=hlsJj4DUWzGrZ_V-
The Origin of ChatGPT | Art of the Problem
I follow the 35 year journey that led to the explosion of Large Language Models. From Jordan's pioneering work in 1986 to today's GPT-4, this documentary traces how AI learned to talk. Featuring insights from AI pioneers including Chomsky, Hofstadter, Hinton, and LeCun, exploring the revolutionary concepts that made ChatGPT possible: transformer architecture, attention mechanism, next-token prediction, and emergent capabilities. Next video following open ai's o1 model My script, references & visualizations here: https://docs.google.com/document/d/1s7FNPoKPW9y3EhvzNgexJaEG2pP4Fx_rmI4askoKZPA/edit?usp=sharing

üé¨ (1hr Talk) Intro to Large Language Models | Andrej Karpathy
https://youtu.be/zjkBMFhNj_g?si=G546Rtz9r9hc233z

üëç https://huggingface.co/spaces/Eliahu/Model-Atlas

https://www.anthropic.com/research/estimating-productivity-gains
Estimating AI productivity gains from Claude conversations



## Intro: LLM Principles & Utilization
[Large Language Models explained briefly | 3Blue1Brown](https://youtu.be/LPZh9BOjkQs?si=7CRyWTVnx3BIGQGy)

üìé https://cameronrwolfe.substack.com/p/understanding-and-using-supervised
- [Transformer Architecture](https://cameronrwolfe.substack.com/i/136366740/the-transformer-from-top-to-bottom): Nearly all modern language models‚Äî_and many other deep learning models_‚Äîare based upon this architecture.
- [Decoder-only Transformers](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)¬†: This is the specific variant of the transformer architecture that is used by most generative LLMs.
- [Brief History of LLMs](https://twitter.com/cwolferesearch/status/1639378997627826176?s=20): LLMs have gone through several phases from the creation of¬†[GPT](https://cameronrwolfe.substack.com/i/85568430/improving-language-understanding-by-generative-pre-training-gpt)¬† to the release of ChatGPT.¬†
- [Next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction): this¬†[self-supervised](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)¬†training objective underlies nearly all LLM functionality and is used by SFT!
- [Language Model Pretraining](https://cameronrwolfe.substack.com/i/136638774/language-model-pretraining): language models are pretrained over a massive, unlabeled textual corpus.¬†
- [Language Model Inference](https://cameronrwolfe.substack.com/i/136638774/autoregressive-inference-process): language models can be used to generate coherent sequences of text via autoregressive next token prediction.

‚Üó [Natural Language Processing (NLP) /Intro](../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics.md#Intro)

![AI-Layer.excalidraw | 800](../../../../../Assets/Illustrations/AI%20&%20LLM/AI-Layer.excalidraw)


### LLM Backgrounds
#### üìú The Development History of AI, NLP, and LLM
‚Üó [The Development History of AI](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/The%20Development%20History%20of%20AI.md)
‚Üó [Deep Learning (Neural Networks)](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/Deep%20Learning%20(Neural%20Networks).md)
‚Üó [Natural Language Processing (NLP) & Computational Linguistics](../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics.md)

![](../../../../../Assets/Pics/Screenshot%202025-09-01%20at%2010.56.49.png)
<small>
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., ‚Ä¶ Wen, J.-R. (2025). A Survey of Large Language Models (arXiv:2303.18223). arXiv. <br><a>https://doi.org/10.48550/arXiv.2303.18223</a></small>
#### Scaling Laws
https://stanford-cs324.github.io/winter2022/lectures/scaling-laws/
#### Emergent Abilities
##### How Emergent Abilities Relate to Scaling Laws


### LLM Modeling ‚≠ê
‚Üó [LLM Foundation Models List & Evaluation & Leaderboard](ü™ú%20LLM%20Foundation%20Models%20List%20&%20Evaluation%20&%20Leaderboard/LLM%20Foundation%20Models%20List%20&%20Evaluation%20&%20Leaderboard.md)

![](../../../../../Assets/Pics/Screenshot%202025-09-04%20at%2020.14.39.png)
<small><a>https://poloclub.github.io/transformer-explainer/</a></small>
#### Tokenization & Embedding
> üîó https://stanford-cs324.github.io/winter2022/lectures/modeling/#model-architecture

#### LLM Model Architectures
‚Üó [Transformers](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/2Ô∏è‚É£%20Neural%20Network%20Models%20üóø/Transformers/Transformers.md)
- Tokenization
- Attention
- Probability

‚Üó [RWKV (Receptance Weighted Key Value)](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/2Ô∏è‚É£%20Neural%20Network%20Models%20üóø/RNN%20(Recurrent%20Neural%20Network)/RWKV%20(Receptance%20Weighted%20Key%20Value).md)
‚Üó [Mamba](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/2Ô∏è‚É£%20Neural%20Network%20Models%20üóø/SSM%20(State-Space%20Model)/Mamba.md)


### LLM Training, Utilization, and Evaluation
‚Üó [LLM Training, Utilization, and Evaluation](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Training,%20Utilization,%20and%20Evaluation.md)
- ‚Üó [Pre-Training](LLM%20Training,%20Utilization,%20and%20Evaluation/Pre-Training/Pre-Training.md) (In-Weight Learning)
- ‚Üó [LLM Adaptation & Alignment Tuning](LLM%20Training,%20Utilization,%20and%20Evaluation/Post-Training%20&%20Fine%20Tuning/LLM%20Adaptation%20&%20Alignment%20Tuning/LLM%20Adaptation%20&%20Alignment%20Tuning.md)
- ‚Üó [LLM Utilization & Prompt Engineering](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/LLM%20Utilization%20&%20Prompt%20Engineering.md) (In-Context Learning)
	- ‚Üó [CoT (Chain-of-Thought)](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/CoT%20(Chain-of-Thought).md)
	- ‚Üó [RAG (Retrieval Augmented Generation)](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/RAG%20(Retrieval%20Augmented%20Generation).md)
	- ‚Üó [Context Engineering & ICL (In-Context Learning)](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/Context%20Engineering%20&%20ICL%20(In-Context%20Learning).md)
#### LLM Reasoning & Large Reasoning Models (LRM) ü§î
‚Üó [Reinforcement Learning (RL) & Sequential Decision Making](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üìä%20Statistical%20Learning%20Theory%20&%20ML%20Types/Reinforcement%20Learning%20(RL)%20&%20Sequential%20Decision%20Making/Reinforcement%20Learning%20(RL)%20&%20Sequential%20Decision%20Making.md)
- ‚Üó [LLM and RL](../../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üìä%20Statistical%20Learning%20Theory%20&%20ML%20Types/Reinforcement%20Learning%20(RL)%20&%20Sequential%20Decision%20Making/LLM%20and%20RL.md)
‚Üó [RLFT (Reinforcement Learning Fine Tuning)](LLM%20Training,%20Utilization,%20and%20Evaluation/Post-Training%20&%20Fine%20Tuning/LLM%20Adaptation%20&%20Alignment%20Tuning/RLFT%20(Reinforcement%20Learning%20Fine%20Tuning)/RLFT%20(Reinforcement%20Learning%20Fine%20Tuning).md)

> üîó https://en.wikipedia.org/wiki/Reasoning_model

A¬†**reasoning model**, also known as¬†**reasoning language models**¬†(**RLMs**) or¬†**large reasoning models**¬†(**LRMs**), is a type of¬†[large language model](https://en.wikipedia.org/wiki/Large_language_model "Large language model")¬†(LLM) that has been specifically trained to solve complex tasks requiring multiple steps of logical¬†[reasoning](https://en.wikipedia.org/wiki/Reasoning "Reasoning").¬†These models demonstrate superior performance on logic, mathematics, and programming tasks compared to standard LLMs. They possess the ability to¬†[revisit and revise](https://en.wikipedia.org/wiki/Backtracking "Backtracking")¬†earlier reasoning steps and utilize additional computation during inference as a method to¬†[scale performance](https://en.wikipedia.org/wiki/Neural_scaling_law "Neural scaling law"), complementing traditional scaling approaches based on training data size, model parameters, and training compute.

Unlike traditional language models that generate responses immediately, reasoning models allocate additional compute, or thinking, time before producing an answer to solve multi-step problems.¬†[OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI")¬†introduced this terminology in September 2024 when it released the¬†[o1 series](https://en.wikipedia.org/wiki/OpenAI_o1 "OpenAI o1"), describing the models as designed to "spend more time thinking" before responding. The company framed o1 as a reset in model naming that targets complex tasks in science, coding, and mathematics, and it contrasted o1's performance with¬†[GPT-4o](https://en.wikipedia.org/wiki/GPT-4o "GPT-4o")¬†on benchmarks such as¬†[AIME](https://en.wikipedia.org/wiki/American_Invitational_Mathematics_Examination "American Invitational Mathematics Examination")¬†and¬†[Codeforces](https://en.wikipedia.org/wiki/Codeforces "Codeforces"). Independent reporting the same week summarized the launch and highlighted OpenAI's claim that o1 automates¬†[chain-of-thought](https://en.wikipedia.org/wiki/Chain-of-thought_prompting "Chain-of-thought prompting")¬†style reasoning to achieve large gains on difficult exams.¬†

In operation, reasoning models generate internal chains of intermediate steps, then select and refine a final answer.¬†[OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI")¬†reported that o1's accuracy improves as the model is given more¬†[reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning "Reinforcement learning")¬†during training and more test-time compute at inference. The company initially chose to hide raw chains and instead return a model-written summary, stating that it "decided not to show" the underlying thoughts so researchers could monitor them without exposing unaligned content to end users. Commercial deployments document separate "reasoning tokens" that meter hidden thinking and a control for "reasoning effort" that tunes how much compute the model uses. These features make the models slower than ordinary chat systems while enabling stronger performance on difficult problems.


### LLM Infrastructure & Deployment
‚Üó [LLM Infrastructure & Deployment](LLM%20Infrastructure%20&%20Deployment/LLM%20Infrastructure%20&%20Deployment.md)
‚Üó [AI (Data) Infrastructure & Techniques Stack](../../üèóÔ∏è%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack.md)


### LLM Applications & LLM-Driven Automation
‚Üó [LLM Applications & LLM-Driven Automation](üöÆ%20LLM%20Applications%20&%20LLM-Driven%20Automation/LLM%20Applications%20&%20LLM-Driven%20Automation.md)
#### Agentic LLM and LLM OS
‚Üó [LLM Agents, AI Workflow, & Agentic MLLM](üöÆ%20LLM%20Applications%20&%20LLM-Driven%20Automation/ü´£%20LLM%20Agents,%20AI%20Workflow,%20&%20Agentic%20MLLM/LLM%20Agents,%20AI%20Workflow,%20&%20Agentic%20MLLM.md)
‚Üó [LLM OS](üöÆ%20LLM%20Applications%20&%20LLM-Driven%20Automation/LLM%20OS.md)
#### Artificial General Intelligence?
‚Üó [AI4X, AGI (Artificial General Intelligence) & AIGC](../../‚ùå%20AI4X,%20AGI%20(Artificial%20General%20Intelligence)%20&%20AIGC/AI4X,%20AGI%20(Artificial%20General%20Intelligence)%20&%20AIGC.md)



## üìú The Technical Evolution of LLM & Future Directions
> ‚Üó [LLM Foundation Models List & Evaluation & Leaderboard](ü™ú%20LLM%20Foundation%20Models%20List%20&%20Evaluation%20&%20Leaderboard/LLM%20Foundation%20Models%20List%20&%20Evaluation%20&%20Leaderboard.md)

![](../../../../../Assets/Pics/Screenshot%202025-09-15%20at%2010.55.30.png)
<small>Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., ‚Ä¶ Wen, J.-R. (2025). _A Survey of Large Language Models_ (arXiv:2303.18223). arXiv. <br> <a>https://doi.org/10.48550/arXiv.2303.18223</a></small>


### LLM Milestone Papers ‚≠ê
> https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#milestone-papers (2025.01)

|  Date   |       keywords       |      Institute       | Paper                                                                                                                                                                                                              |
| :-----: | :------------------: | :------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2017-06 |     Transformers     |        Google        | [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)                                                                                                                                                  |
| 2018-06 |       GPT 1.0        |        OpenAI        | [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)                                                                             |
| 2018-10 |         BERT         |        Google        | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423.pdf)                                                                                          |
| 2019-02 |       GPT 2.0        |        OpenAI        | [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)                                          |
| 2019-09 |     Megatron-LM      |        NVIDIA        | [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)                                                                                      |
| 2019-10 |          T5          |        Google        | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/v21/20-074.html)                                                                                       |
| 2019-10 |         ZeRO         |      Microsoft       | [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf)                                                                                                       |
| 2020-01 |     Scaling Law      |        OpenAI        | [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)                                                                                                                                    |
| 2020-05 |       GPT 3.0        |        OpenAI        | [Language models are few-shot learners](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)                                                                                         |
| 2021-01 | Switch Transformers  |        Google        | [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)                                                                               |
| 2021-08 |        Codex         |        OpenAI        | [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)                                                                                                                           |
| 2021-08 |  Foundation Models   |       Stanford       | [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)                                                                                                                        |
| 2021-09 |         FLAN         |        Google        | [Finetuned Language Models are Zero-Shot Learners](https://openreview.net/forum?id=gEZrGCozdqR)                                                                                                                    |
| 2021-10 |          T0          |  HuggingFace et al.  | [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)                                                                                                              |
| 2021-12 |         GLaM         |        Google        | [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/pdf/2112.06905.pdf)                                                                                                         |
| 2021-12 |        WebGPT        |        OpenAI        | [WebGPT: Browser-assisted question-answering with human feedback](https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22) |
| 2021-12 |        Retro         |       DeepMind       | [Improving language models by retrieving from trillions of tokens](https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens)                                         |
| 2021-12 |        Gopher        |       DeepMind       | [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446.pdf)                                                                                                 |
| 2022-01 |         COT          |        Google        | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)                                                                                                      |
| 2022-01 |        LaMDA         |        Google        | [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf)                                                                                                                             |
| 2022-01 |       Minerva        |        Google        | [Solving Quantitative Reasoning Problems with Language Models](https://arxiv.org/abs/2206.14858)                                                                                                                   |
| 2022-01 | Megatron-Turing NLG  |   Microsoft&NVIDIA   | [Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990.pdf)                                                                         |
| 2022-03 |     InstructGPT      |        OpenAI        | [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)                                                                                                        |
| 2022-04 |         PaLM         |        Google        | [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)                                                                                                                              |
| 2022-04 |      Chinchilla      |       DeepMind       | [An empirical analysis of compute-optimal large language model training](https://arxiv.org/abs/2408.00724)                                                                                                         |
| 2022-05 |         OPT          |         Meta         | [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)                                                                                                                          |
| 2022-05 |         UL2          |        Google        | [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1)                                                                                                                                         |
| 2022-06 |  Emergent Abilities  |        Google        | [Emergent Abilities of Large Language Models](https://openreview.net/pdf?id=yzkSU5zdwD)                                                                                                                            |
| 2022-06 |      BIG-bench       |        Google        | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://github.com/google/BIG-bench)                                                                                |
| 2022-06 |        METALM        |      Microsoft       | [Language Models are General-Purpose Interfaces](https://arxiv.org/pdf/2206.06336.pdf)                                                                                                                             |
| 2022-09 |       Sparrow        |       DeepMind       | [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/pdf/2209.14375.pdf)                                                                                                       |
| 2022-10 |     Flan-T5/PaLM     |        Google        | [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)                                                                                                                              |
| 2022-10 |       GLM-130B       |       Tsinghua       | [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/pdf/2210.02414.pdf)                                                                                                                              |
| 2022-11 |         HELM         |       Stanford       | [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf)                                                                                                                                     |
| 2022-11 |        BLOOM         |      BigScience      | [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/pdf/2211.05100.pdf)                                                                                                            |
| 2022-11 |      Galactica       |         Meta         | [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085.pdf)                                                                                                                              |
| 2022-12 |       OPT-IML        |         Meta         | [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/pdf/2212.12017)                                                                                   |
| 2023-01 | Flan 2022 Collection |        Google        | [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/pdf/2301.13688.pdf)                                                                                           |
| 2023-02 |        LLaMA         |         Meta         | [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)                                                            |
| 2023-02 |       Kosmos-1       |      Microsoft       | [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045)                                                                                                         |
| 2023-03 |         LRU          |       DeepMind       | [Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/abs/2303.06349)                                                                                                                      |
| 2023-03 |        PaLM-E        |        Google        | [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io/)                                                                                                                                         |
| 2023-03 |        GPT 4         |        OpenAI        | [GPT-4 Technical Report](https://openai.com/research/gpt-4)                                                                                                                                                        |
| 2023-04 |        LLaVA         | UW‚ÄìMadison&Microsoft | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)                                                                                                                                                      |
| 2023-04 |        Pythia        |  EleutherAI et al.   | [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)                                                                                                |
| 2023-05 |      Dromedary       |      CMU et al.      | [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/abs/2305.03047)                                                                                 |
| 2023-05 |        PaLM 2        |        Google        | [PaLM 2 Technical Report](https://ai.google/static/documents/palm2techreport.pdf)                                                                                                                                  |
| 2023-05 |         RWKV         |       Bo Peng        | [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)                                                                                                                                 |
| 2023-05 |         DPO          |       Stanford       | [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)                                                                                             |
| 2023-05 |         ToT          |   Google&Princeton   | [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601.pdf)                                                                                                    |
| 2023-07 |        LLaMA2        |         Meta         | [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)                                                                                                                        |
| 2023-10 |      Mistral 7B      |       Mistral        | [Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf)                                                                                                                                                                 |
| 2023-12 |        Mamba         |    CMU&Princeton     | [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752)                                                                                                               |
| 2024-01 |     DeepSeek-v2      |       DeepSeek       | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)                                                                                             |
| 2024-05 |        Mamba2        |    CMU&Princeton     | [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060)                                                                      |
| 2024-05 |        Llama3        |         Meta         | [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)                                                                                                                                                     |
| 2024-12 |       Qwen2.5        |       Alibaba        | [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115)                                                                                                                                                       |
| 2024-12 |     DeepSeek-V3      |       DeepSeek       | [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437v1)                                                                                                                                                 |
| 2025-01 |     DeepSeek-R1      |       DeepSeek       | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)                                                                                             |
|         |                      |                      |                                                                                                                                                                                                                    |


### Technical Evolution of Specific LLM Model Series
#### GPT-series Model
‚Üó [OpenAI ChatGPT](ü™ú%20LLM%20Foundation%20Models%20List%20&%20Evaluation%20&%20Leaderboard/OpenAI%20ChatGPT.md)

> Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., ‚Ä¶ Wen, J.-R. (2025). _A Survey of Large Language Models_ (arXiv:2303.18223). arXiv. [https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)
#### Gemini-series Model
‚Üó [Google Gemini](ü™ú%20LLM%20Foundation%20Models%20List%20&%20Evaluation%20&%20Leaderboard/Google%20Gemini.md)
#### LLaMA-series Model
‚Üó [Meta LLama](ü™ú%20LLM%20Foundation%20Models%20List%20&%20Evaluation%20&%20Leaderboard/Meta%20LLama.md)
#### Qwen-series (ÈÄö‰πâÂçÉÈóÆ) Model
‚Üó [Alibaba Qwen](ü™ú%20LLM%20Foundation%20Models%20List%20&%20Evaluation%20&%20Leaderboard/Alibaba%20Qwen.md)
#### DeepSeek-series Model
‚Üó [DeepSeek](ü™ú%20LLM%20Foundation%20Models%20List%20&%20Evaluation%20&%20Leaderboard/DeepSeek.md)



## Ref
[‰ªÄ‰πàÊòØLLMÂ§ßËØ≠Ë®ÄÊ®°ÂûãÔºüLarge Language ModelÔºå‰ªéÈáèÂèòÂà∞Ë¥®Âèò - ËâæÂá°AFanÁöÑÊñáÁ´† - Áü•‰πé]: https://zhuanlan.zhihu.com/p/622518771

[üëç Â§ßËØ≠Ë®ÄÊ®°ÂûãË∞ÉÁ†îÊ±áÊÄª - guolipaÁöÑÊñáÁ´† - Áü•‰πé]: https://zhuanlan.zhihu.com/p/614766286

[üëç Â§ßËØ≠Ë®ÄÊ®°ÂûãÁªºËø∞ | ‰∏≠ÂõΩ‰∫∫Ê∞ëÂ§ßÂ≠¶È´òÁì¥‰∫∫Â∑•Êô∫ËÉΩÂ≠¶Èô¢]: http://ai.ruc.edu.cn/research/science/20230605100.html

[Âü∫‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑÊºèÊ¥ûÊ£ÄÊµãÊñπÊ≥ïÁªºËø∞]: https://m.fx361.com/news/2022/1215/16831048.html

[Á¨¨‰∫åÁØá Âü∫‰∫éËá™ÁÑ∂ËØ≠Ë®ÄÂ§ÑÁêÜÁöÑÊºèÊ¥ûÊ£ÄÊµãÊñπÊ≥ïÁªºËø∞]: https://blog.csdn.net/qq_55202378/article/details/127583425
[76È°µÁªºËø∞+300‰ΩôÁØáÂèÇËÄÉÊñáÁåÆÔºåÂ§©Â§ßÂõ¢ÈòüÂÖ®Èù¢‰ªãÁªçÂ§ßËØ≠Ë®ÄÊ®°ÂûãÂØπÈΩêÊäÄÊúØ]: https://cloud.tencent.com/developer/article/2336345

[‰ªéPromptÊ≥®ÂÖ•Âà∞ÂëΩ‰ª§ÊâßË°åÔºöÊé¢Á©∂LLMÂ§ßÂûãËØ≠Ë®ÄÊ®°Âûã‰∏≠ OpenAIÁöÑÈ£éÈô©ÁÇπ]: https://www.secpulse.com/archives/199158.html

Prompt Injection ÊòØ‰∏ÄÁßçÊîªÂáªÊäÄÊúØÔºåÈªëÂÆ¢ÊàñÊÅ∂ÊÑèÊîªÂáªËÄÖÊìçÁ∫µ AI Ê®°ÂûãÁöÑËæìÂÖ•ÂÄºÔºå‰ª•ËØ±ÂØºÊ®°ÂûãËøîÂõûÈùûÈ¢ÑÊúüÁöÑÁªìÊûú„ÄÇËøôÈáåÊèêÂà∞ÁöÑÂ±û‰∫éÊòØSSTIÊúçÂä°Á´ØÊ®°ÊùøÊ≥®ÂÖ•„ÄÇ

ËøôÂÖÅËÆ∏ÊîªÂáªËÄÖÂà©Áî®Ê®°ÂûãÁöÑÂÆâÂÖ®ÊÄßÊù•Ê≥ÑÈú≤Áî®Êà∑Êï∞ÊçÆÊàñÊâ≠Êõ≤Ê®°ÂûãÁöÑËÆ≠ÁªÉÁªìÊûú„ÄÇÂú®Êüê‰∫õÊ®°Âûã‰∏≠ÔºåÂæàÂ§öÊÉÖÂÜµ‰∏ãËæìÂÖ•ÊèêÁ§∫ÁöÑÊï∞ÊçÆ‰ºöÁõ¥Êé•Êö¥Èú≤ÊàñÂØπËæìÂá∫ÊúâÂæàÂ§ßÂΩ±Âìç„ÄÇ

[Ê∑±ÂÖ•ÂâñÊûêÂ§ßÊ®°ÂûãÂÆâÂÖ®ÈóÆÈ¢òÔºöLangchainÊ°ÜÊû∂ÁöÑÈöêËóèÈ£éÈô© | ËÖæËÆØÊäÄÊúØÂ∑•Á®ã]: https://www.secrss.com/articles/59635

[üëç AAAI2024 | ÂàÜ‰∫´10ÁØá‰ºòÁßÄËÆ∫ÊñáÔºåÊ∂âÂèäÂõæÁ•ûÁªèÁΩëÁªú„ÄÅÂ§ßÊ®°Âûã‰ºòÂåñ„ÄÅË°®Ê†ºÂàÜÊûêÁ≠âÁÉ≠Èó®ËØùÈ¢ò]: https://mp.weixin.qq.com/s/F7X8N_wUyZQNhDtIfHm17Q

[ü§î ‰ªéLLM‰∏≠ÂÆåÂÖ®Ê∂àÈô§Áü©Èòµ‰πòÊ≥ïÔºåÊïàÊûúÂá∫Â•áÂæóÂ•ΩÔºå10‰∫øÂèÇÊï∞Ë∑ëÂú®FPGA‰∏äÊé•ËøëÂ§ßËÑëÂäüËÄó]: https://mp.weixin.qq.com/s/3YSega29u8hnc8CcCrNwqQ

[80 „ÄêÁúãÂ§ß‰Ω¨YouTubeËÉúËØªÂõõÂπ¥Êú¨Áßë - cheezit03 | Â∞èÁ∫¢‰π¶ - ‰Ω†ÁöÑÁîüÊ¥ªÂÖ¥Ë∂£Á§æÂå∫„Äë üòÜ eMXAZs7NTrAMe8a üòÜ ]: https://www.xiaohongshu.com/discovery/item/67cda8e7000000002903d0f9?source=webshare&xhsshare=pc_web&xsec_token=AB-gNabAhHDfNbngCiSe41bKwI6pTghIBsMcRPTSSb1Qo=&xsec_source=pc_share

[Áü•ËØÜÂíåÂ§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÈõÜÊàêË∂ãÂäøÔºöÁªºËø∞ - ÈªÑÊµ¥ÁöÑÊñáÁ´† - Áü•‰πé]: https://zhuanlan.zhihu.com/p/668825246

[Tracing the thoughts of a large language model | Anthropic]: https://youtu.be/Bj9BD2D3DzA?si=fcpeY9wYY5IQDd2q

QuantumBlack AI by McKinsey:¬†["The next innovation revolution - powered by AI"](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-next-innovation-revolution-powered-by-ai)
Gruber & Tal:¬†[The Market Opportunity Navigator](https://wheretoplay.co/the-navigator/)¬†,¬†[PDF worksheet](https://drive.google.com/file/d/1eBUlii5EfLJM0Fc9JRk3Iy7DR6K0eykS/view?usp=share_link)

[(1hr Talk) Intro to Large Language Models | Andrej Karpathy]: https://youtu.be/zjkBMFhNj_g?si=G546Rtz9r9hc233z

[ü§î Â§ßËØ≠Ë®ÄÊ®°ÂûãÁöÑÊàêÂäüÂê¶ÂÆö‰∫Ü‰πîÂßÜÊñØÂü∫ÁöÑÊôÆÈÅçËØ≠Ê≥ï]: http://xhslink.com/o/5hvBkMxyUli
