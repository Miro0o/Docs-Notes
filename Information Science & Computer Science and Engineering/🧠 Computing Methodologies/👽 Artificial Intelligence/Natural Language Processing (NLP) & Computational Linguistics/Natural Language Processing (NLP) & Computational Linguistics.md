# Natural Language Processing (NLP) & Computational Linguistics

[TOC]



## Res
### Related Topics
‚Üó [Linguistics](../../../../Other%20Networks%20of%20Knowledge/Arts%20&%20Humanities/üìÉ%20Language%20&%20Literature/Linguistics/Linguistics.md)
‚Üó [Ordinary Language Philosophy](../../../../Other%20Networks%20of%20Knowledge/‚ôÇ%20Philosophy%20&%20Its%20History/Modern%20Philosophy/Analytic%20Philosophy/Ordinary%20Language%20Philosophy/Ordinary%20Language%20Philosophy.md)
‚Üó [Philosophy of Language](../../../../Other%20Networks%20of%20Knowledge/‚ôÇ%20Philosophy%20&%20Its%20History/Contemporary%20Philosophy/üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë®%20Philosophy%20of%20Language%20&%20Mathematics/Philosophy%20of%20Language.md)

‚Üó [Mathematical Logic Basics (Formal Logic & Its Semantics)](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/üìç%20Mathematical%20Logic%20Basics%20(Formal%20Logic)/Mathematical%20Logic%20Basics%20(Formal%20Logic%20&%20Its%20Semantics).md)
‚Üó [Automata Theory and (Formal) Language Theory](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/üò∂‚Äçüå´Ô∏è%20Theory%20of%20Computation/üçè%20Automata%20Theory%20and%20(Formal)%20Language%20Theory/Automata%20Theory%20and%20(Formal)%20Language%20Theory.md)
‚Üó [Formal Syntax & Metasyntax (and Metalanguage)](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/üìç%20Mathematical%20Logic%20Basics%20(Formal%20Logic)/üìå%20Formal%20Syntax%20&%20Metasyntax%20(and%20Metalanguage)/Formal%20Syntax%20&%20Metasyntax%20(and%20Metalanguage).md)

‚Üó [Algebraic Structure & Abstract Algebra & Modern Algebra](../../../üßÆ%20Mathematics/üßä%20Algebra/üéÉ%20Algebraic%20Structure%20&%20Abstract%20Algebra%20&%20Modern%20Algebra/Algebraic%20Structure%20&%20Abstract%20Algebra%20&%20Modern%20Algebra.md)

‚Üó [Computer Languages & Programming Methodology](../../../üîë%20CS%20Core/üë©‚Äçüíª%20Computer%20Languages%20&%20Programming%20Methodology/Computer%20Languages%20&%20Programming%20Methodology.md)
‚Üó [Programming Language Processing & Program Execution](../../../üîë%20CS%20Core/üõ£Ô∏è%20Programming%20Language%20Processing%20&%20Program%20Execution/Programming%20Language%20Processing%20&%20Program%20Execution.md)

‚Üó [LLM (Large Language Model)](ü¶ë%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md)
‚Üó [Knowledge Graph (KG)](../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/Knowledge%20Representation%20and%20Reasoning%20(KRR)/Knowledge%20Graph%20(KG)/Knowledge%20Graph%20(KG).md)


### Learning Resources
Daniel Jurafsky and James H. Martin. 2025. Speech and Language Processing: An Introduction to Natural Language Processing, Computational Linguistics, and Speech Recognition with Language Models, 3rd edition. Online manuscript released August 24, 2025. https://web.stanford.edu/~jurafsky/slp3.
- Second Edition: https://home.cs.colorado.edu/~martin/slp.html

| **Volume I: Large Language Models**            |                                                                                                                         |                                                                                                                                                                                                                                                                                                                                |
| :--------------------------------------------- | :---------------------------------------------------------------------------------------------------------------------- | :----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
|                                                | **Chapter**                                                                                                             | **Slides**                                                                                                                                                                                                                                                                                                                     |
|                                                | 1: Introduction                                                                                                         |                                                                                                                                                                                                                                                                                                                                |
|                                                | 2: [Words and Tokens](https://web.stanford.edu/~jurafsky/slp3/2.pdf)                                                    | 2: Words and Tokens [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/tokens_aug25.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/tokens_aug25.pdf)] 2: Edit Distance [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/med24.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/med24.pdf)] |
|                                                | 3: [N-gram Language Models](https://web.stanford.edu/~jurafsky/slp3/3.pdf)                                              | 3: [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/lm_jan25.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/lm_jan25.pdf)]                                                                                                                                                                                 |
|                                                | 4: [Logistic Regression and Text Classification](https://web.stanford.edu/~jurafsky/slp3/4.pdf)                         | 4: [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/logreg25aug.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/logreg25aug.pdf)]                                                                                                                                                                           |
|                                                | 5: [Embeddings](https://web.stanford.edu/~jurafsky/slp3/5.pdf)                                                          | 5: [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/vector25aug.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/vector25aug.pdf)]                                                                                                                                                                           |
|                                                | 6: [Neural Networks](https://web.stanford.edu/~jurafsky/slp3/6.pdf)                                                     | 6: [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/nn25aug.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/nn25aug.pdf)]                                                                                                                                                                                   |
|                                                | 7: [Large Language Models](https://web.stanford.edu/~jurafsky/slp3/7.pdf)                                               | 7: [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/llm25aug.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/llm25aug.pdf)]                                                                                                                                                                                 |
|                                                | 8: [Transformers](https://web.stanford.edu/~jurafsky/slp3/8.pdf)                                                        | 8: [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/transformer25aug.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/transformer25aug.pdf)]                                                                                                                                                                 |
|                                                | 9: [Post-training: Instruction Tuning, Alignment, and Test-Time Compute](https://web.stanford.edu/~jurafsky/slp3/9.pdf) |                                                                                                                                                                                                                                                                                                                                |
|                                                | 10: [Masked Language Models](https://web.stanford.edu/~jurafsky/slp3/10.pdf)                                            | 10: [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/mlmjan25.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/mlmjan25.pdf)]                                                                                                                                                                                |
|                                                | 11: [Information Retrieval and Retrieval-Augmented Generation](https://web.stanford.edu/~jurafsky/slp3/11.pdf)          |                                                                                                                                                                                                                                                                                                                                |
|                                                | 12: [Machine Translation](https://web.stanford.edu/~jurafsky/slp3/12.pdf)                                               |                                                                                                                                                                                                                                                                                                                                |
|                                                | 13: [RNNs and LSTMs](https://web.stanford.edu/~jurafsky/slp3/13.pdf)                                                    | 13: [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/rnnjan25.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/rnnjan25.pdf)]                                                                                                                                                                                |
|                                                | 14: [Phonetics and Speech Feature Extraction](https://web.stanford.edu/~jurafsky/slp3/14.pdf)                           |                                                                                                                                                                                                                                                                                                                                |
|                                                | 15: [Automatic Speech Recognition](https://web.stanford.edu/~jurafsky/slp3/15.pdf)                                      |                                                                                                                                                                                                                                                                                                                                |
|                                                | 16: [Text-to-Speech](https://web.stanford.edu/~jurafsky/slp3/16.pdf)                                                    |                                                                                                                                                                                                                                                                                                                                |
|                                                |                                                                                                                         |                                                                                                                                                                                                                                                                                                                                |
| **Volume II: Annotating Linguistic Structure** |                                                                                                                         |                                                                                                                                                                                                                                                                                                                                |
|                                                | **Chapter**                                                                                                             | **Slides**                                                                                                                                                                                                                                                                                                                     |
|                                                | 17: [Sequence Labeling for Parts of Speech and Named Entities](https://web.stanford.edu/~jurafsky/slp3/17.pdf)          | 17: (Intro only) [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/8_POSNER_intro_May_6_2021.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/8_POSNER_intro_May_6_2021.pdf)]                                                                                                                                 |
|                                                | 18: [Context-Free Grammars and Constituency Parsing](https://web.stanford.edu/~jurafsky/slp3/18.pdf)                    |                                                                                                                                                                                                                                                                                                                                |
|                                                | 19: [Dependency Parsing](https://web.stanford.edu/~jurafsky/slp3/19.pdf)                                                |                                                                                                                                                                                                                                                                                                                                |
|                                                | 20: [Information Extraction: Relations, Events, and Time](https://web.stanford.edu/~jurafsky/slp3/20.pdf)               |                                                                                                                                                                                                                                                                                                                                |
|                                                | 21: [Semantic Role Labeling and Argument Structure](https://web.stanford.edu/~jurafsky/slp3/21.pdf)                     |                                                                                                                                                                                                                                                                                                                                |
|                                                | 22: [Lexicons for Sentiment, Affect, and Connotation](https://web.stanford.edu/~jurafsky/slp3/22.pdf)                   |                                                                                                                                                                                                                                                                                                                                |
|                                                | 23: [Coreference Resolution and Entity Linking](https://web.stanford.edu/~jurafsky/slp3/23.pdf)                         |                                                                                                                                                                                                                                                                                                                                |
|                                                | 24: [Discourse Coherence](https://web.stanford.edu/~jurafsky/slp3/24.pdf)                                               |                                                                                                                                                                                                                                                                                                                                |
|                                                | 25: [Conversation and its Structure](https://web.stanford.edu/~jurafsky/slp3/25.pdf)                                    |                                                                                                                                                                                                                                                                                                                                |
|                                                |                                                                                                                         |                                                                                                                                                                                                                                                                                                                                |
| **Appendix (will be just on the web)**         |                                                                                                                         |                                                                                                                                                                                                                                                                                                                                |
|                                                | A: [Hidden Markov Models](https://web.stanford.edu/~jurafsky/slp3/A.pdf)                                                |                                                                                                                                                                                                                                                                                                                                |
|                                                | B: [Naive Bayes Classification](https://web.stanford.edu/~jurafsky/slp3/B.pdf)                                          | B: [[pptx](https://web.stanford.edu/~jurafsky/slp3/slides/nb24aug.pptx)] [[pdf](https://web.stanford.edu/~jurafsky/slp3/slides/nb24aug.pdf)]<br>                                                                                                                                                                               |
|                                                | C: [Kneser-Ney Smoothing](https://web.stanford.edu/~jurafsky/slp3/C.pdf)                                                |                                                                                                                                                                                                                                                                                                                                |
|                                                | D: [Spelling Correction and the Noisy Channel](https://web.stanford.edu/~jurafsky/slp3/D.pdf)                           |                                                                                                                                                                                                                                                                                                                                |
|                                                | E: [Statistical Constituency Parsing](https://web.stanford.edu/~jurafsky/slp3/E.pdf)                                    |                                                                                                                                                                                                                                                                                                                                |
|                                                | F: [Context-Free Grammars](https://web.stanford.edu/~jurafsky/slp3/F.pdf)                                               |                                                                                                                                                                                                                                                                                                                                |
|                                                | G: [Combinatory Categorial Grammar](https://web.stanford.edu/~jurafsky/slp3/G.pdf)                                      |                                                                                                                                                                                                                                                                                                                                |
|                                                | H: [Logical Representations of Sentence Meaning](https://web.stanford.edu/~jurafsky/slp3/H.pdf)                         |                                                                                                                                                                                                                                                                                                                                |
|                                                | I: [Word Senses and WordNet](https://web.stanford.edu/~jurafsky/slp3/I.pdf)                                             |                                                                                                                                                                                                                                                                                                                                |
|                                                | J: [PPMI](https://web.stanford.edu/~jurafsky/slp3/J.pdf)                                                                |                                                                                                                                                                                                                                                                                                                                |
|                                                | K: [Frame-based Dialogue Systems](https://web.stanford.edu/~jurafsky/slp3/K.pdf)                                        |                                                                                                                                                                                                                                                                                                                                |

üè´ [CS224N Natural Language Processing with Deep Learning](../../../üó∫%20CS%20Overview/üíã%20Intro%20to%20Computer%20Science/üë©üèº‚Äçüè´%20Courses%20of%20Universities/Stanford/CS224N%20Natural%20Language%20Processing%20with%20Deep%20Learning/CS224N%20Natural%20Language%20Processing%20with%20Deep%20Learning.md)
üîó https://web.stanford.edu/class/cs224n/
The following texts are useful, but none are required. All of them can be read free online.
- Dan Jurafsky and James H. Martin.¬†[Speech and Language Processing (2024 pre-release)](https://web.stanford.edu/~jurafsky/slp3/)
- Jacob Eisenstein.¬†[Natural Language Processing](https://github.com/jacobeisenstein/gt-nlp-class/blob/master/notes/eisenstein-nlp-notes.pdf)
- Yoav Goldberg.¬†[A Primer on Neural Network Models for Natural Language Processing](http://u.cs.biu.ac.il/~yogo/nnlp.pdf)
- Ian Goodfellow, Yoshua Bengio, and Aaron Courville.¬†[Deep Learning](http://www.deeplearningbook.org/)
- Delip Rao and Brian McMahan.¬†[Natural Language Processing with PyTorch](http://library.stanford.edu/sfx?genre=book&atitle=&title=Natural%20language%20processing%20with%20PyTorch%20:%20build%20intelligent%20language%20applications%20using%20deep%20learning%20/&isbn=9781491978207&volume=&issue=&date=20190101&aulast=Rao,%20Delip,,%20author.&spage=&pages=&sid=EBSCO:VLeBooks:edsvle.AH35866319)¬†(requires Stanford login).
- Lewis Tunstall, Leandro von Werra, and Thomas Wolf.¬†[Natural Language Processing with Transformers](https://transformersbook.com/)

If you have no background in neural networks but would like to take the course anyway, you might well find one of these books helpful to give you more background:
- Michael A. Nielsen.¬†[Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/)
- Eugene Charniak.¬†[Introduction to Deep Learning](https://mitpress.mit.edu/books/introduction-deep-learning)


### Other Resources



## Intro
### Language and Language Processing
‚Üó [Philosophy of Language](../../../../Other%20Networks%20of%20Knowledge/‚ôÇ%20Philosophy%20&%20Its%20History/Contemporary%20Philosophy/üë©‚Äç‚ù§Ô∏è‚Äçüíã‚Äçüë®%20Philosophy%20of%20Language%20&%20Mathematics/Philosophy%20of%20Language.md)
- ‚Üó [Ludwig Wittgenstein](../../../../Other%20Networks%20of%20Knowledge/‚ôÇ%20Philosophy%20&%20Its%20History/Modern%20Philosophy/Analytic%20Philosophy/Ordinary%20Language%20Philosophy/Ludwig%20Wittgenstein.md)
‚Üó [Language & Literature](../../../../Other%20Networks%20of%20Knowledge/Arts%20&%20Humanities/üìÉ%20Language%20&%20Literature/Language%20&%20Literature.md)
- ‚Üó [Linguistics](../../../../Other%20Networks%20of%20Knowledge/Arts%20&%20Humanities/üìÉ%20Language%20&%20Literature/Linguistics/Linguistics.md)

‚Üó [Logic (and Critical Thinking)](../../../../Other%20Networks%20of%20Knowledge/‚ôÇ%20Philosophy%20&%20Its%20History/Classical%20Philosophy/Western%20Philosophy%20&%20Its%20History/üéº%20Logic%20(and%20Critical%20Thinking)/Logic%20(and%20Critical%20Thinking).md)
‚Üó [Mathematics](../../../üßÆ%20Mathematics/Mathematics.md)
- ‚Üó [Mathematical Logic (Foundations of Mathematics)](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/Mathematical%20Logic%20(Foundations%20of%20Mathematics).md)
	 - ‚Üó [Mathematical Logic Basics (Formal Logic & Its Semantics)](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/üìç%20Mathematical%20Logic%20Basics%20(Formal%20Logic)/Mathematical%20Logic%20Basics%20(Formal%20Logic%20&%20Its%20Semantics).md)
	 - ‚Üó [Theory of Computation](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/üò∂‚Äçüå´Ô∏è%20Theory%20of%20Computation/Theory%20of%20Computation.md)

‚Üó [Formal Semantics and Programming Language](../../../üîë%20CS%20Core/üë©‚Äçüíª%20Computer%20Languages%20&%20Programming%20Methodology/üê¢%20Programming%20Language%20Theory%20(PLT)/Formal%20Semantics%20and%20Programming%20Language/Formal%20Semantics%20and%20Programming%20Language.md)
‚Üó [Formal Syntax & Metasyntax (and Metalanguage)](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/üìç%20Mathematical%20Logic%20Basics%20(Formal%20Logic)/üìå%20Formal%20Syntax%20&%20Metasyntax%20(and%20Metalanguage)/Formal%20Syntax%20&%20Metasyntax%20(and%20Metalanguage).md)

‚Üó [Computer Languages & Programming Methodology](../../../üîë%20CS%20Core/üë©‚Äçüíª%20Computer%20Languages%20&%20Programming%20Methodology/Computer%20Languages%20&%20Programming%20Methodology.md)
- ‚Üó [Programming Language Theory (PLT)](../../../üîë%20CS%20Core/üë©‚Äçüíª%20Computer%20Languages%20&%20Programming%20Methodology/üê¢%20Programming%20Language%20Theory%20(PLT)/Programming%20Language%20Theory%20(PLT).md)
‚Üó [Programming Language Processing & Program Execution](../../../üîë%20CS%20Core/üõ£Ô∏è%20Programming%20Language%20Processing%20&%20Program%20Execution/Programming%20Language%20Processing%20&%20Program%20Execution.md)


### NLP Background & Classical Approaches
#### Knowledge in Speech and Language Processing
> ‚Üó [Learning English the Right Way](../../../../Other%20Networks%20of%20Knowledge/Arts%20&%20Humanities/üìÉ%20Language%20&%20Literature/üåê%20Language%20Learning%20&%20(Second)%20Language%20Acquisition/üá¨üáß%20üá∫üá∏%20Learning%20English%20the%20Right%20Way/Learning%20English%20the%20Right%20Way.md)
> - ‚Üó [English Grammar](../../../../Other%20Networks%20of%20Knowledge/Arts%20&%20Humanities/üìÉ%20Language%20&%20Literature/üåê%20Language%20Learning%20&%20(Second)%20Language%20Acquisition/üá¨üáß%20üá∫üá∏%20Learning%20English%20the%20Right%20Way/1Ô∏è‚É£%20English%20Grammar/English%20Grammar.md)
> ‚Üó [Programming Language Processing & Program Execution](../../../üîë%20CS%20Core/üõ£Ô∏è%20Programming%20Language%20Processing%20&%20Program%20Execution/Programming%20Language%20Processing%20&%20Program%20Execution.md)
> - ‚Üó [Program Language Processing & Compilation Theory (Compile-time)](../../../üîë%20CS%20Core/üõ£Ô∏è%20Programming%20Language%20Processing%20&%20Program%20Execution/üöÆ%20Program%20Language%20Processing%20&%20Compilation%20Theory%20(Compile-time)/Program%20Language%20Processing%20&%20Compilation%20Theory%20(Compile-time).md)

> üîó https://home.cs.colorado.edu/~martin/slp.html
> Speech and Language Processing, 2nd ed, 2009

To summarize, engaging in complex language behavior requires various kinds of
knowledge of language:
- Phonetics and Phonology ‚Äî knowledge about linguistic sounds
- Morphology ‚Äî knowledge of the meaningful components of words
- Syntax ‚Äî knowledge of the structural relationships between words
- Semantics ‚Äî knowledge of meaning
- Pragmatics ‚Äî knowledge of the relationship of meaning to the goals and intentions of the speaker
- Discourse ‚Äî knowledge about linguistic units larger than a single utterance
#### Ambiguity (?ü§î)
> üîó https://home.cs.colorado.edu/~martin/slp.html
> Speech and Language Processing, 2nd ed, 2009

A perhaps surprising fact about these categories of linguistic knowledge is that most
tasks in speech and language processing can be viewed as resolving **ambiguity** at one of these levels. We say some input is ambiguous if multiple, alternative linguistic structures can be built for it. Consider the spoken sentence I made her duck. Here are five different meanings this sentence could have (see if you can think of some more), each of which exemplifies an ambiguity at some level:
- (1.5) I cooked waterfowl for her.
- (1.6) I cooked waterfowl belonging to her.
- (1.7) I created the (plaster?) duck she owns.
- (1.8) I caused her to quickly lower her head or body.
- (1.9) I waved my magic wand and turned her into undifferentiated waterfowl.

We often introduce the models and algorithms we present throughout the book as ways to resolve or **disambiguate** these ambiguities. For example, deciding whether duck is a verb or a noun can be solved by **part-of-speech tagging**. Deciding whether make means ‚Äúcreate‚Äù or ‚Äúcook‚Äù can be solved by **word sense disambiguation**. Resolution of part-of speech and word sense ambiguities are two important kinds of **lexical disambiguation**. A wide variety of tasks can be framed as lexical disambiguation problems. For example, a text-to-speech synthesis system reading the word lead needs to decide whether it should be pronounced as in lead pipe or as in lead me on. By contrast, deciding whether her and duck are part of the same entity (as in (1.5) or (1.8)) or are different entities (as in (1.6)) is an example of **syntactic disambiguation** and can be addressed by **probabilistic parsing**. We also consider ambiguities that don‚Äôt arise in this particular example, such as determining whether a sentence is a statement or a question (which can be resolved by speech act interpretation).
#### Models and Algorithms
> ‚Üó [Mathematical Modeling & Real World Problem Solving](../../../üßÆ%20Mathematics/Mathematical%20Modeling%20&%20Real%20World%20Problem%20Solving.md)
> ‚Üó [Models of Computation & UTM (universal Turing Machine)](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/üò∂‚Äçüå´Ô∏è%20Theory%20of%20Computation/Models%20of%20Computation%20&%20UTM%20(universal%20Turing%20Machine)/Models%20of%20Computation%20&%20UTM%20(universal%20Turing%20Machine).md)
> ‚Üó [(Formal) Model Checking](../../../CyberSecurity/üè∞%20Cybersecurity%20Basics%20&%20InfoSec/üç¶%20Software%20Security/ü™Ü%20Software%20(Program)%20Analysis%20&%20Binary%20Engineering/üìå%20Software%20(Program)%20Analysis%20Basics/üôá‚Äç‚ôÇÔ∏è%20Formal%20Methods%20&%20Formal%20Verification%20(FV)/(Formal)%20Model%20Checking/(Formal)%20Model%20Checking.md)
> 
> ‚Üó [Automata Theory and (Formal) Language Theory](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/üò∂‚Äçüå´Ô∏è%20Theory%20of%20Computation/üçè%20Automata%20Theory%20and%20(Formal)%20Language%20Theory/Automata%20Theory%20and%20(Formal)%20Language%20Theory.md)
> ‚Üó [Classical Logic (Standard Logic)](../../../üßÆ%20Mathematics/ü§º‚Äç‚ôÄÔ∏è%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/üìç%20Mathematical%20Logic%20Basics%20(Formal%20Logic)/Classical%20Logic%20(Standard%20Logic)/Classical%20Logic%20(Standard%20Logic).md)
> ‚Üó [Probabilistic Models & Stochastic Process](../../../üßÆ%20Mathematics/üßê%20Mathematical%20Analysis%20(&%20Analytical%20Mathematics)/üìê%20Measures%20(Measure%20Theory)/üìä%20Probabilities%20&%20Statistics/üèåüèª‚Äç‚ôÇÔ∏è%20Probabilistic%20Models%20&%20Stochastic%20Process/Probabilistic%20Models%20&%20Stochastic%20Process.md)
> 
> See below "üìú A Brief History of The Technical Evolution Of Language Models"

> üîó https://home.cs.colorado.edu/~martin/slp.html
> Speech and Language Processing, 2nd ed, 2009

One of the key insights of the last 50 years of research in language processing is that the various kinds of knowledge described in the last sections can be captured through the use of a small number of formal models or theories. Fortunately, these models and theories are all drawn from the standard toolkits of computer science, mathematics, and linguistics and should be generally familiar to those trained in those fields. Among the most important models are **state machines**, **rule systems**, **logic**, **probabilistic models**, and **vector-space models**. These models, in turn, lend themselves to a small number of algorithms, among the most important of which are **state space search algorithms**, such as dynamic programming, and **machine learning algorithms**, such as classifiers and Expectation Maximization (EM) and other learning algorithms.
- In their simplest formulation, ==**state machines**== are formal models that consist of states, transitions among states, and an input representation. Some of the variations of this basic model that we will consider are deterministic and non-deterministic finite-state automata and finite-state transducers.
- Closely related to these models are their declarative counterparts: ==**formal rule systems**==. Among the more important ones we consider (in both probabilistic and non-probabilistic formulations) are **regular grammars** and regular relations, **context-free grammars**, and **feature-augmented grammars**. State machines and formal rule systems are the main tools used when dealing with knowledge of phonology, morphology, and syntax.
- A third class of models that plays a critical role in capturing knowledge of language are models based on ==**logic**==. We discuss **first-order logic**, also known as the **predicate calculus**, as well as such related formalisms as **lambda-calculus**, feature structures, and semantic primitives. These logical representations have traditionally been used for modeling semantics and pragmatics, although more recent work has tended to focus on potentially more robust techniques drawn from non-logical lexical semantics.
- ==**Probabilistic models**== are crucial for capturing every kind of linguistic knowledge. Each of the other models (state machines, formal rule systems, and logic) can be augmented with probabilities. For example, the state machine can be augmented with probabilities to become the weighted automaton, or **Markov model**. We spend a significant amount of time on **hidden Markov models or HMMs**, which are used everywhere in the field, in part-of speech tagging, speech recognition, dialogue understanding, text-to-speech, and machine translation. The key advantage of probabilistic models is their ability to solve the many kinds of ambiguity problems that we discussed earlier; almost any speech and language processing problem can be recast as ‚Äúgiven N choices for some ambiguous input, choose the most probable one‚Äù.
- Finally, ==**vector-space models**==, based on linear algebra, underlie information retrieval and many treatments of word meanings. (<a style="color:red">Neural Networks & Language Models</a>)

Processing language with any of these models typically involves a **search through a space of states** representing hypotheses about an input. 
- In speech recognition, we search through a space of phone sequences for the correct word. In parsing, we search through a space of trees for the syntactic parse of an input sentence. 
- In machine translation, we search through a space of translation hypotheses for the correct translation of a sentence into another language. 
- For non-probabilistic tasks, such as tasks involving state machines, we use well-known graph algorithms such as depth-first search. 
- For probabilistic tasks, we use heuristic variants such as best-first and A* search and rely on dynamic programming algorithms for computational tractability. 
- Machine learning tools such as classifiers and sequence models play a significant role in many language processing tasks. Based on attributes describing each object, a classifier attempts to assign a single object to a single class while a sequence model attempts to jointly classify a sequence of objects into a sequence of classes. 

For example, in the task of deciding whether a word is spelled correctly, classifiers such as decision trees, support vector machines, Gaussian mixture models, and logistic regression could be used to make a binary decision (correct or incorrect) for one word at a time. Sequence models such as hidden Markov models, maximum entropy Markov models, and conditional random fields could be used to assign correct/incorrect labels to all the words in a sentence at once.

Finally, researchers in language processing use many of the same methodological tools that are used in machine learning research‚Äîthe use of distinct training and
test sets, statistical techniques like cross-validation, and careful evaluation of trained
systems.
#### Language, Thought, and Understanding
> üîó https://home.cs.colorado.edu/~martin/slp.html
> Speech and Language Processing, 2nd ed, 2009

**Intelligence and Turing Test**
To many, the ability of computers to process language as skillfully as we humans do will signal the arrival of truly intelligent machines. The basis of this belief is the fact that the effective use of language is intertwined with our general cognitive abilities. Among the first to consider the computational implications of this intimate connection was Alan Turing (1950). In this famous paper, Turing introduced what has come to be known as the **Turing test**. Turing began with the thesis that the question of what itTuring test would mean for a machine to think was essentially unanswerable because of the inherent imprecision in the terms machine and think. Instead, he suggested an empirical test, a game, in which a computer‚Äôs use of language would form the basis for determining if the machine could think. If the machine could win the game, it would be judged intelligent.

In Turing‚Äôs game, there are three participants: two people and a computer. One of the people is a contestant who plays the role of an interrogator. To win, the interrogator must determine which of the other two participants is the machine by asking a series of questions via a teletype. The task of the machine is to fool the interrogator into believing it is a person by responding as a person would to the interrogator‚Äôs questions. The task of the second human participant is to convince the interrogator that the other participant is the machine and that she is human. 


### Language Model (LM)
> üîó [What is a language model?](https://stanford-cs324.github.io/winter2022/lectures/introduction/#what-is-a-language-model)

==The classic definition of a language model (LM) is a¬†**probability distribution over sequences of tokens**.== Suppose we have a¬†**vocabulary $\Box$**¬†of a set of tokens. A language model¬†$p$¬†assigns each sequence of tokens¬†$x_1,‚Ä¶,x_L\in\Box$¬†a probability (a number between 0 and 1): $$p(x_1,‚Ä¶,x_L).$$
The probability intuitively tells us how ‚Äúgood‚Äù a sequence of tokens is. For example, if the vocabulary is¬†$\Box=\{ate, ball, cheese, mouse, the\}$, the language model might assign ([demo](http://crfm-models.stanford.edu/static/index.html?prompt=%24%7Bprompt%7D&settings=echo_prompt%3A%20true%0Amax_tokens%3A%200&environments=prompt%3A%20%5Bthe%20mouse%20ate%20the%20cheese%2C%20the%20cheese%20ate%20the%20mouse%2C%20mouse%20the%20the%20cheese%20ate%5D)): $$\begin{align}
p(ùóçùóÅùñæ,ùóÜùóàùóéùóåùñæ,ùñ∫ùóçùñæ,ùóçùóÅùñæ,ùñºùóÅùñæùñæùóåùñæ) &= 0.02, \\
p(ùóçùóÅùñæ,ùñºùóÅùñæùñæùóåùñæ,ùñ∫ùóçùñæ,ùóçùóÅùñæ,ùóÜùóàùóéùóåùñæ) & =0.01, \\
p(ùóÜùóàùóéùóåùñæ,ùóçùóÅùñæ,ùóçùóÅùñæ,ùñºùóÅùñæùñæùóåùñæ,ùñ∫ùóçùñæ) &= 0.0001
\end{align}$$
Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: ==the ability to assign (meaningful) probabilities to all sequences requires extraordinary (but¬†_implicit_) linguistic abilities and world knowledge.==

For example, the LM should assign¬†$(ùóÜùóàùóéùóåùñæ, ùóçùóÅùñæ, ùóçùóÅùñæ, ùñºùóÅùñæùñæùóåùñæ, ùñ∫ùóçùñæ)$¬†a very low probability implicitly because it‚Äôs ungrammatical (**syntactic knowledge**). The LM should assign¬†$(ùóçùóÅùñæ, ùóÜùóàùóéùóåùñæ, ùñ∫ùóçùñæ, ùóçùóÅùñæ, ùñºùóÅùñæùñæùóåùñæ)$¬†higher probability than¬†$(ùóçùóÅùñæ, ùñºùóÅùñæùñæùóåùñæ, ùñ∫ùóçùñæ, ùóçùóÅùñæ, ùóÜùóàùóéùóåùñæ)$¬†implicitly because of¬†**world knowledge**: both sentences are the same syntactically, but they differ in semantic plausibility.

**Generation** As defined, a language model¬†$p$¬†takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequence¬†$x_{1:L}$¬†from the language model¬†$p$¬†with probability equal to¬†$p(x_{1:L})$, denoted: $$x_{1:L}‚àºp.$$
How to do this computationally efficiently depends on the form of the language model¬†$p$. In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an ‚Äúaverage‚Äù sequence but something closer to the ‚Äúbest‚Äù sequence.


**Autoregressive language models**
A common way to write the joint distribution¬†$p(x_{1:L})$¬†of a sequence¬†$x_{1:L}$¬†is using the¬†**chain rule of probability**: $$p(x_{1:L})=p(x_1)p(x_2‚à£x_1)p(x_3‚à£x_1,x_2)‚ãØp(x_L‚à£x_{1:L‚àí1})=\prod_{i=1}^{L}p(x_i‚à£x_{1:i‚àí1}).$$
For example ([demo](http://crfm-models.stanford.edu/static/index.html?prompt=the%20mouse%20ate%20the%20cheese&settings=echo_prompt%3A%20true%0Amax_tokens%3A%200%0Atop_k_per_token%3A%2010&environments=)): $$
\begin{align}
p(ùóçùóÅùñæ,ùóÜùóàùóéùóåùñæ,ùñ∫ùóçùñæ,ùóçùóÅùñæ,ùñºùóÅùñæùñæùóåùñæ)= \ & p(ùóçùóÅùñæ) \\
& p(ùóÜùóàùóéùóåùñæ‚à£ùóçùóÅùñæ) \\
& p(ùñ∫ùóçùñæ‚à£ùóçùóÅùñæ,ùóÜùóàùóéùóåùñæ) \\
& p(ùóçùóÅùñæ‚à£ùóçùóÅùñæ,ùóÜùóàùóéùóåùñæ,ùñ∫ùóçùñæ) \\
& p(ùñºùóÅùñæùñæùóåùñæ‚à£ùóçùóÅùñæ,ùóÜùóàùóéùóåùñæ,ùñ∫ùóçùñæ,ùóçùóÅùñæ)
\end{align}$$
In particular,¬†$p(x_i‚à£x_{1:i‚àí1})$¬†is a¬†**conditional probability distribution**¬†of the next token¬†$x_i$¬†given the previous tokens¬†$x_{1:i‚àí1}$.

Of course, any joint probability distribution can be written this way mathematically, but an¬†**autoregressive language model**¬†is one where each conditional distribution$p(x_i‚à£x_{1:i‚àí1})$¬†can be computed efficiently (e.g., using a feedforward neural network).

**Generation**. Now to generate an entire sequence¬†$x_{1:L}$¬†from an autoregressive language model¬†$p$, we sample one token at a time given the tokens generated so far: $$\begin{align} for¬†\ i=1,‚Ä¶,L: \\
& x_{i}‚àºp(x_i‚à£x_{1:i‚àí1})^{1/T},\end{align}$$
where¬†$T \geq 0$¬†is a¬†**temperature**¬†parameter that controls how much randomness we want from the language model:
- $T=0$: deterministically choose the most probable token¬†$x_i$¬†at each position¬†$i$
- $T=1$: sample ‚Äúnormally‚Äù from the pure language model
- $T=\infty$: sample from a uniform distribution over the entire vocabulary¬†$\Box$

However, if we just raise the probabilities to the power¬†$1/T$, the probability distribution may not sum to 1. We can fix this by **re-normalizing** the distribution. We call the normalized version¬†$p_T(x_i‚à£x_{1:i‚àí1})‚àùp(x_i‚à£x_{1:i‚àí1})^{1/T}$¬†the¬†**annealed**¬†conditional probability distribution. For example:
$$\begin{align}
& p(ùñºùóÅùñæùñæùóåùñæ)=0.4, & p(ùóÜùóàùóéùóåùñæ)=0.6 \\
& p_{T=0.5}(ùñºùóÅùñæùñæùóåùñæ)=0.31, & p_{T=0.5}(ùóÜùóàùóéùóåùñæ)=0.69 \\
& p_{T=0.2}(ùñºùóÅùñæùñæùóåùñæ)=0.12, & p_{T=0.2}(ùóÜùóàùóéùóåùñæ)=0.88 \\
& p_{T=0}(ùñºùóÅùñæùñæùóåùñæ)=0, & p_{T=0}(ùóÜùóàùóéùóåùñæ)=1
\end{align}$$
> _Aside_: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing.

> _Technical note_: sampling iteratively with a temperature¬†$T$¬†parameter applied to each conditional distribution¬†$p(x_i‚à£x_{1:i‚àí1})^{1/T}$¬†is not equivalent (except when¬†$T=1$) to sampling from the annealed distribution over length¬†$L$¬†sequences.

**Conditional generation**. More generally, we can perform conditional generation by specifying some prefix sequence¬†$x_{1:i}$¬†(called a¬†**prompt**) and sampling the rest¬†$x_{i+1:L}$¬†(called the¬†**completion**). For example, generating with¬†$T=0$ produces ([demo](http://crfm-models.stanford.edu/static/index.html?prompt=the%20mouse%20ate&settings=temperature%3A%200%0Amax_tokens%3A%202%0Atop_k_per_token%3A%2010%0Anum_completions%3A%2010&environments=)): $$\underbrace{\text{the, mouse, ate}}_{\text{prompt}}
\;\;\xrightarrow[T=0]{}
\underbrace{\text{the, cheese}}_{\text{completion}}$$
If we change the temperature to¬†$T=1$, we can get more variety ([demo](http://crfm-models.stanford.edu/static/index.html?prompt=the%20mouse%20ate&settings=temperature%3A%201%0Amax_tokens%3A%202%0Atop_k_per_token%3A%2010%0Anum_completions%3A%2010&environments=)), for example,$\{ùóÇùóçùóå, ùóÅùóàùóéùóåùñæ, and, ùóÜùóí, ùóÅùóàùóÜùñæùóêùóàùóãùóÑ \}$.

As we‚Äôll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.

**Summary**
- A language model is a probability distribution¬†$p$¬†over sequences $x_{1:L}$.
- Intuitively, a good language model should have linguistic capabilities and world knowledge.
- An autoregressive language model allows for efficient generation of a completion¬†$x_{i+1:L}$¬†given a prompt¬†$x_{1:i}$.
- The temperature can be used to control the amount of variability in generation.
#### From Language Model to (General) Task Model
> ‚Üó [Pre-Training](ü¶ë%20LLM%20(Large%20Language%20Model)/LLM%20Training,%20Utilization,%20and%20Evaluation/Pre-Training/Pre-Training.md)
> ‚Üó [Post-Training & Fine Tuning](ü¶ë%20LLM%20(Large%20Language%20Model)/LLM%20Training,%20Utilization,%20and%20Evaluation/Post-Training%20&%20Fine%20Tuning/Post-Training%20&%20Fine%20Tuning.md)
> ‚Üó [LLM Utilization & Prompt Engineering](ü¶ë%20LLM%20(Large%20Language%20Model)/LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/LLM%20Utilization%20&%20Prompt%20Engineering.md)

> üîó https://stanford-cs324.github.io/winter2022/lectures/capabilities/

Recall that a¬†**language model**¬†p¬†is a distribution over sequences of tokens¬†x1:L¬†and thus can be used to score sequences: $$p(ùóçùóÅùñæ,ùóÜùóàùóéùóåùñæ,ùñ∫ùóçùñæ,ùóçùóÅùñæ,ùñºùóÅùñæùñæùóåùñæ).$$
It can also be used to perform conditional generation of a completion given a prompt:
$$ùóçùóÅùñæ \ ùóÜùóàùóéùóåùñæ \ ùñ∫ùóçùñæ ‚áù ùóçùóÅùñæ \ ùñºùóÅùñæùñæùóåùñæ.$$

==A¬†**task**¬†is a mapping from inputs to outputs.== For example, for question answering, we might have:
> Input: What school did burne hogarth establish?  
> Output: School of Visual Arts

We use the term¬†**adaptation**¬†to refer to the process of taking a language model and turning it into a task model, given:
- a natural language¬†**description**¬†of the task, and
- a set of¬†**training instances**¬†(input-output pairs).

There are two primary ways to perform adaptation:
1. **Training**¬†(standard supervised learning): train a new model that maps inputs to outputs, either by
    1. creating a new model that uses the language model as features (probing), or
    2. starting with the language model and updating it based on the training instances (fine-tuning), or
    3. something in between (lightweight fine-tuning).
2. **Prompting**¬†(in-context learning): Construct a prompt (a string based on the description and training instances) or a set of prompts, feed those into a language model to obtain completions.
    1. Zero-shot learning: number of training examples is 0
    2. One-shot learning: number of training examples is 1
    3. Few-shot learning: number of training examples is few

Which adaptation procedure should we go with?
- **Training can be challenging due to overfitting**¬†(just imagine fine-tuning a 175 billion parameter model based on 5 examples). How to do this effectively will be the topic of the adaptation lecture.
- For now, we will be content with¬†**adaptation of GPT-3 using prompting**. Note that the limitation of prompting is that we can only leverage a only small number of training instances (as many as can fit into a prompt). This is due to a limitation of Transformers, where the prompt and the completion must fit into 2048 tokens.

The GPT-3 paper evaluated GPT-3 on a large set of tasks. We will consider a subset of these, and for each task, discuss the following:
1. **Definition**: What is the task and its motivation?
2. **Adaptation**: How do we reduce the task to language modeling (via prompting)?
3. **Results**: What are the quantitative numbers compared to task-specific state-of-the-art models?

**Size and number of examples matters**. By default, the results will based on
- the full GPT-3 model (davinci), which has 175 billion parameters
- using in-context learning with as many training instances as you can stuff into the prompt.

Along the way, we will do ablations to see if model size and number of in-context training instances matters. Spoiler: it does and more is better.

The tasks are grouped as follows:
1. [Language modeling](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling)
2. [Question answering](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#question-answering)
3. [Translation](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#translation)
4. [Arithmetic](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#arithmetic)
5. [News article generation](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#news-article-generation)
6. [Novel tasks](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#novel-tasks)


### üìú A Brief History of The Technical Evolution Of Language Models

![](../../../../Assets/Pics/Screenshot%202025-09-01%20at%2010.56.49.png)
<small>
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., ‚Ä¶ Wen, J.-R. (2025). A Survey of Large Language Models (arXiv:2303.18223). arXiv. <br><a>https://doi.org/10.48550/arXiv.2303.18223</a></small>
#### Information Theory, Entropy of English, N-Gram Models
‚Üó [Information Theory](../../../üßÆ%20Mathematics/ü•∏%20Information%20Theory/Information%20Theory.md)

> üîó https://stanford-cs324.github.io/winter2022/lectures/introduction/#a-brief-history

**Information theory**. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,¬†[A Mathematical Theory of Communication](https://dl.acm.org/doi/pdf/10.1145/584091.584093). In this paper, he introduced the¬†**entropy**¬†of a distribution as $$H(p)=\Sigma_{x}p(x)log\frac{1}{p(x)}.$$
The entropy measures the expected number of bits¬†**any algorithm**¬†needs to encode (compress) a sample¬†$x‚àºp$¬†into a bitstring: $$\text{ùóçùóÅùñæ ùóÜùóàùóéùóåùñæ ùñ∫ùóçùñæ ùóçùóÅùñæ ùñºùóÅùñæùñæùóåùñæ} \implies 0001110101.$$
- The lower the entropy, the more ‚Äústructured‚Äù the sequence is, and the shorter the code length.
- Intuitively,¬†$log\frac{1}{p(x)}$¬†is the length of the code used to represent an element¬†$x$¬†that occurs with probability¬†$p(x)$.
- If¬†$p(x)=\frac{1}{8}$, we should allocate¬†$log_2(8)=3$¬†bits (equivalently,¬†$log(8)=2.08¬†\ nats$).

> _Aside_: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.

**Entropy of English**. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a ‚Äútrue‚Äù distribution¬†$p$¬†out there (the existence of this is questionable, but it‚Äôs still a useful mathematical abstraction) that can spout out samples of English text¬†$x‚àºp$.

Shannon also defined¬†**cross entropy**: $$H(p,q)=\Sigma_xp(x)log \frac{1}{q(x)},$$
which measures the expected number of bits (`nats`) needed to encode a sample¬†$x‚àºp$¬†using the compression scheme given by the model¬†$q$¬†(representing¬†x¬†with a code of length¬†$\frac{1}{q(x)}$).


**Estimating entropy via language modeling**. A crucial property is that the cross entropy¬†$H(p,q)$¬†upper bounds the entropy¬†$H(p)$, $$H(p,q) \geq H(p),$$
which means that we can estimate¬†$H(p,q)$ by constructing a (language) model¬†$q$¬†with only samples from the true data distribution¬†$p$, whereas¬†$H(p)$¬†is generally inaccessible if¬†$p$¬†is English.

So we can get better estimates of the entropy¬†$H(p)$¬†by constructing better models¬†$q$, as measured by¬†$H(p,q)$.


**Shannon game (human language model)**. Shannon first used n-gram models as¬†$q$¬†in 1948, but in his 1951 paper¬†[Prediction and Entropy of Printed English](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6773263), he introduced a clever scheme (known as the Shannon game) where¬†$q$¬†was provided by a human: $$\text{ùóçùóÅùñæ ùóÜùóàùóéùóåùñæ ùñ∫ùóçùñæ ùóÜùóí ùóÅùóà\_}$$
Humans aren‚Äôt good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses.
##### N-gram models for downstream applications
Language models became first used in practical applications that required generation of text:
- speech recognition in the 1970s (input: acoustic signal, output: text), and
- machine translation in the 1990s (input: text in a source language, output: text in a target language).

**Noisy channel model**. The dominant paradigm for solving these tasks then was the¬†**noisy channel model**. Taking speech recognition as an example:
- We posit that there is some text sampled from some distribution¬†$p$.
- This text becomes realized to speech (acoustic signals).
- Then given the speech, we wish to recover the (most likely) text. This can be done via Bayes rule: $$p(text‚à£speech) ‚àù \underbrace{p(text)}_{\text{Language Model}} \underbrace{p(speech ‚à£ text)}_{\text{Acoustic Model}}$$
Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).

**N-gram models**. In an¬†**n-gram model**, the prediction of a token¬†xi¬†only depends on the last¬†n‚àí1¬†characters¬†xi‚àí(n‚àí1):i‚àí1¬†rather than the full history: $$p(x_i‚à£x_{1:i‚àí1})=p(x_i‚à£x_{i‚àí(n‚àí1):i‚àí1)}.$$
For example, a trigram (n=3) model would define: $$p(ùñºùóÅùñæùñæùóåùñæ‚à£ùóçùóÅùñæ,ùóÜùóàùóéùóåùñæ,ùñ∫ùóçùñæ,ùóçùóÅùñæ)=p(ùñºùóÅùñæùñæùóåùñæ‚à£ùñ∫ùóçùñæ,ùóçùóÅùñæ).$$
These probabilities are computed based on the number of times various n-grams (e.g.,¬†$\text{ùñ∫ùóçùñæ ùóçùóÅùñæ ùóÜùóàùóéùóåùñæ}$¬†and¬†$\text{ùñ∫ùóçùñæ ùóçùóÅùñæ ùñºùóÅùñæùñæùóåùñæ}$) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).

Fitting n-gram models to data is extremely¬†**computationally cheap**¬†and scalable. As a result, n-gram models were trained on massive amount of text. For example,¬†[Brants et al. (2007)](https://aclanthology.org/D07-1090.pdf)¬†trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix: $$\text{ùñ≤ùóçùñ∫ùóáùñøùóàùóãùñΩ ùóÅùñ∫ùóå ùñ∫ ùóáùñæùóê ùñºùóàùóéùóãùóåùñæ ùóàùóá ùóÖùñ∫ùóãùóÄùñæ ùóÖùñ∫ùóáùóÄùóéùñ∫ùóÄùñæ ùóÜùóàùñΩùñæùóÖùóå. ùñ®ùóç ùóêùóÇùóÖùóÖ ùñªùñæ ùóçùñ∫ùóéùóÄùóÅùóç ùñªùóí \_\_\_}$$
If¬†$n$¬†is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend on¬†ùñ≤ùóçùñ∫ùóáùñøùóàùóãùñΩ. However, if¬†n¬†is too big, it will be¬†**statistically infeasible**¬†to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in ‚Äúhuge‚Äù corpora): $$count(ùñ≤ùóçùñ∫ùóáùñøùóàùóãùñΩ,ùóÅùñ∫ùóå,ùñ∫,ùóáùñæùóê,ùñºùóàùóéùóãùóåùñæ,ùóàùóá,ùóÖùñ∫ùóãùóÄùñæ,ùóÖùñ∫ùóáùóÄùóéùñ∫ùóÄùñæ,ùóÜùóàùñΩùñæùóÖùóå)=0.$$
As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturing¬†**local dependencies**¬†(and not being able to capture long-range dependencies) wasn‚Äôt a huge problem.
#### Neural Networks
‚Üó [Deep Learning (Neural Networks)](../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/Deep%20Learning%20(Neural%20Networks).md)
‚Üó [Neural Network Models](../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/2Ô∏è‚É£%20Neural%20Network%20Models%20üóø/Neural%20Network%20Models.md)

> üîó https://stanford-cs324.github.io/winter2022/lectures/introduction/#neural-language-models

An important step forward for language models was the introduction of neural networks.¬†[Bengio et al., 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)¬†pioneered neural language models, where $p(x_i‚à£x_{i‚àí(n‚àí1):i‚àí1)}$¬†is given by a neural network: $$p(ùñºùóÅùñæùñæùóåùñæ‚à£ùñ∫ùóçùñæ,ùóçùóÅùñæ)=some-neural-network(ùñ∫ùóçùñæ,ùóçùóÅùñæ,ùñºùóÅùñæùñæùóåùñæ).$$
Note that the context length is still bounded by¬†$n$, but it is now¬†**statistically feasible**¬†to estimate neural language models for much larger values of¬†$n$.

Now, the main challenge was that training neural networks was much more¬†**computationally expensive**. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.

Since 2003, two other key developments in neural language modeling include:
- **Recurrent Neural Networks**¬†(RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a token¬†$x_i$¬†to depend on the¬†**entire context**¬†$x_{1:i‚àí1}$¬†(effectively¬†$n=\infty$), but these were hard to train.
- **Transformers**¬†are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context length¬†n, but were much¬†**easier to train**¬†(and exploited the parallelism of GPUs). Also,¬†n¬†could be made ‚Äúlarge enough‚Äù for many applications (GPT-3 used¬†n=2048).
#### Large Language Models ‚≠ê
‚Üó [LLM (Large Language Model) /The Technical Evolution of LLM & Future Directions](ü¶ë%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md#The%20Technical%20Evolution%20of%20LLM%20&%20Future%20Directions)
‚Üó [Transformers](../üóùÔ∏è%20AI%20Basics%20&%20Machine%20Learning%20(ML)/üåä%20Deep%20Learning%20(Neural%20Network)/2Ô∏è‚É£%20Neural%20Network%20Models%20üóø/Transformers/Transformers.md)



## Ref
[ÂàòÁ¶æË∞à‰∫∫Â∑•Êô∫ËÉΩÔºöË∞ÅÊääÂì≤Â≠¶Â∏¶Ëøõ‰∫ÜËÆ°ÁÆóÊú∫Ôºü]: https://www.thepaper.cn/newsDetail_forward_31542928?tg_rhash=d69a077f8e29b8
