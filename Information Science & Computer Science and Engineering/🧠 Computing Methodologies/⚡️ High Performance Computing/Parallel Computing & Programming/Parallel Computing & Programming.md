# Parallel Computing

[TOC]



## Res
### Related Topics
â†— [ILP (Instruction Level Parallelism)](../../../ğŸ”‘%20CS%20Core/ğŸ›£ï¸%20Programming%20Language%20Processing%20&%20Program%20Execution/ğŸ¤¡%20Program%20Execution%20(Runtime)/Instruction%20Execution/ILP%20(Instruction%20Level%20Parallelism)/ILP%20(Instruction%20Level%20Parallelism).md)
â†— [Multiprocessors and Multicore Processor](../../../ğŸ”‘%20CS%20Core/ğŸ‘·ğŸ¾â€â™‚ï¸%20Computer%20(Host)%20System/Computer%20Architecture/Computer%20Microarchitectures%20(Computer%20Organization)%20&%20von%20Neumann%20Model/ğŸš¦%20Computer%20Processors%20&%20Logic%20Chips/Multiprocessors%20and%20Multicore%20Processor/Multiprocessors%20and%20Multicore%20Processor.md)
â†— [Multiprocessor Architectures & Parallel Computing](../../../ğŸ”‘%20CS%20Core/ğŸ‘·ğŸ¾â€â™‚ï¸%20Computer%20(Host)%20System/Computer%20Architecture/Computer%20Microarchitectures%20(Computer%20Organization)%20&%20von%20Neumann%20Model/ğŸš¦%20Computer%20Processors%20&%20Logic%20Chips/Multiprocessors%20and%20Multicore%20Processor/Multiprocessor%20Architectures%20&%20Parallel%20Computing/Multiprocessor%20Architectures%20&%20Parallel%20Computing.md)

â†— [Distributed Computing & Systems](../../Distributed%20Computing%20&%20Systems/Distributed%20Computing%20&%20Systems.md)
â†— [Distributed Systems Implementations](../../Distributed%20Computing%20&%20Systems/ğŸ’¸%20Distributed%20Systems%20Implementations/Distributed%20Systems%20Implementations.md)

â†— [Concurrent Programming](../../../ğŸ”‘%20CS%20Core/ğŸ‘·ğŸ¾â€â™‚ï¸%20Computer%20(Host)%20System/Operating%20System%20&%20OS%20Kernel%20(Theory%20Part)/OS%20Processes%20&%20Automata%20Management%20(CPU%20+%20Main%20Memory%20Resource)/Concurrent%20Programming.md)
â†— [Compute Unified Device Architecture & CUDA Programming](../../../ğŸ”‘%20CS%20Core/ğŸ‘·ğŸ¾â€â™‚ï¸%20Computer%20(Host)%20System/Computer%20Interfaces%20&%20Hardware%20Drivers/ğŸ›%20Computer%20(IO%20Devices)%20Drivers%20&%20Programming/Graphics%20Devices%20Drivers/Compute%20Unified%20Device%20Architecture%20&%20CUDA%20Programming/Compute%20Unified%20Device%20Architecture%20&%20CUDA%20Programming.md)
â†— [Parallel Programming Libraries & SDK](../../../ğŸ”‘%20CS%20Core/ğŸ‘©â€ğŸ’»%20Computer%20Languages%20&%20Programming%20Methodology/ğŸ› ï¸%20Programming%20Tool%20Chain/ğŸš %20Application%20Runtimes%20&%20SDKs/ğŸ‘¯â€â™€ï¸%20Parallel%20Programming%20Libraries%20&%20SDK/Parallel%20Programming%20Libraries%20&%20SDK.md)


### Learning Resources
ğŸ‘¨â€ğŸ’» https://hpcwiki.io/parallel-programming/parallel-programming-intro/

![](../../../../../../Assets/Pics/Screenshot%202024-07-26%20at%201.32.38%20PM.png)
<small>å›¾ï¼š40 å¹´æ¥å¤„ç†å™¨æ€§èƒ½è¿›æ­¥ç¤ºæ„å›¾ã€‚1978-2018 å¹´çš„æ•°æ®ç”± Patterson å’Œ Hennessyï¼ˆ2017ï¼‰ç»™å‡ºï¼Œä¹‹åå‡ å¹´çš„æ•°æ®æ˜¯ç»¼åˆäº† Geekbench5 å’Œå…¶ä»–å‡ ä¸ªåŸºå‡†æµ‹è¯•ç»™å‡ºçš„å¤§è‡´å€¼ï¼ˆå…·ä½“çš„ç»†èŠ‚åœ¨è¿™é‡Œæˆ‘ä»¬ä¸ä½œè¿‡å¤šæè¿°ï¼‰ã€‚2023 å¹´çš„æ•°æ®ç»è¿‡ä¼°ç®—ï¼Œå°†å…¶è½¬åŒ–åˆ° 4 ä¸ªæ ¸æ€»çš„çš„å¹³å‡è¡¨ç°ä¸Šå¾—å‡ºã€‚</small>



## Intro
> ğŸ”— https://en.wikipedia.org/wiki/Parallel_computing

![](../../../../../../../Assets/Pics/Screenshot%202024-03-17%20at%204.49.59%20PM.png)
<small>https://en.wikipedia.org/wiki/Parallel_computing</small>

Parallel computing is a type of computation in which many calculations or processes are carried out simultaneously. Large problems can often be divided into smaller ones, which can then be solved at the same time. There are several different forms of parallel computing: bit-level, instruction-level, data, and task parallelism. Parallelism has long been employed in high-performance computing, but has gained broader interest due to the physical constraints preventing frequency scaling. As power consumption (and consequently heat generation) by computers has become a concern in recent years, parallel computing has become the dominant paradigm in computer architecture, mainly in the form of multi-core processors.

Parallel computing is closely related to concurrent computingâ€”they are frequently used together, and often conflated, though the two are distinct: it is possible to have parallelism without concurrency, and concurrency without parallelism (such as multitasking by time-sharing on a single-core CPU). In parallel computing, a computational task is typically broken down into several, often many, very similar sub-tasks that can be processed independently and whose results are combined afterwards, upon completion. In contrast, in concurrent computing, the various processes often do not address related tasks; when they do, as is typical in distributed computing, the separate tasks may have a varied nature and often require some inter-process communication during execution.

Parallel computers can be roughly classified according to the level at which the hardware supports parallelism, with multi-core and multi-processor computers having multiple processing elements within a single machine, while clusters, MPPs, and grids use multiple computers to work on the same task. Specialized parallel computer architectures are sometimes used alongside traditional processors, for accelerating specific tasks.

In some cases parallelism is transparent to the programmer, such as in bit-level or instruction-level parallelism, but explicitly parallel algorithms, particularly those that use concurrency, are more difficult to write than sequential ones, because concurrency introduces several new classes of potential software bugs, of which race conditions are the most common. Communication and synchronization between the different subtasks are typically some of the greatest obstacles to getting optimal parallel program performance.
A theoretical upper bound on the speed-up of a single program as a result of parallelization is given by Amdahl's law, which states that it is limited by the fraction of time for which the parallelization can be utilised.


### Concurrency & Parallelism
> â†— [FAQ /ğŸ‘‰ pipelining, parallelism, concurrency in HTTP Connection Management](../../../ğŸ”‘%20CS%20Core/ğŸï¸%20Computer%20Networking%20and%20Communication/ğŸ“Œ%20Computer%20Networking%20Basics%20(Protocol%20Part)/FAQ.md#ğŸ‘‰%20pipelining,%20parallelism,%20concurrency%20in%20HTTP%20Connection%20Management)
> 
> - **å¹¶å‘**ï¼šç”±äºæœ‰é™çš„èµ„æºé™åˆ¶ï¼Œåœ¨æŸä¸€åˆ’å®šçš„æ—¶é—´æ®µå†…ï¼ˆä¸€ä¸ªæ—¶åˆ»ï¼Œä¸€ä¸ªå¤„ç†å‘¨æœŸï¼Œä»»æ„å…¶ä»–è§„å®šçš„æ—¶æ®µï¼‰èµ„æºçš„è¯·æ±‚å¤§äºå¯ç”¨èµ„æºï¼Œè¿™æ—¶å°±éœ€è¦æŸç§ç­–ç•¥æ¥åè°ƒè¿™ç§ä¾›å°äºæ±‚çš„æƒ…å†µï¼Œä½¿å¾—åœ¨çº¦å®šçš„æ¡ä»¶å†…è¾¾åˆ°æœ€ä¼˜è§£ã€‚è¿™é€šå¸¸æ„å‘³ç€åœ¨æœ€çŸ­æ—¶é—´å†…æ»¡è¶³æœ€å¤šçš„èµ„æºè¯·æ±‚ã€‚
> - **å¹¶è¡Œè®¡ç®—**ï¼šå¯¹ä¸€ä¸ªå·²çŸ¥çš„ä»»åŠ¡ï¼Œé€šè¿‡æŸç§åˆ’åˆ†ç­–ç•¥ï¼Œä½¿å¾—è¿™ä¸ªä»»åŠ¡è¢«åˆ’åˆ†ä¸ºå¤šä¸ªå¯ä»¥åŒæ—¶å¤„ç†çš„å­ä»»åŠ¡ï¼Œä»è€Œè¾¾åˆ°å¯¹è¯¥ä»»åŠ¡å¤„ç†æ€§èƒ½çš„æé«˜ã€‚å¤„ç†æ€§èƒ½å¯ä»¥æ˜¯ä»»ä½•è§„å®šçš„æŒ‡æ ‡ï¼Œä¸è¿‡é€šå¸¸æ˜¯å¤„ç†æ•ˆç‡ï¼Œå³é€Ÿåº¦/æˆæœ¬ã€‚
> - **æµæ°´çº¿æŠ€æœ¯/pipelining**: ä¸€ç§ç®€å•çš„å¹¶è¡Œè®¡ç®—ä¸­çš„åˆ’åˆ†ç­–ç•¥ã€‚

Throughout the history of digital computers, two demands have been constant forces in driving improvements: we want them to do more, and we want them to run faster. Both of these factors improve when the processor does more things at once. We use the term **concurrency** to refer to the general concept of a system with multiple, simultaneous activities, and the term **parallelism** to refer to the use of concurrency to make a system run faster. Parallelism can be exploited at multiple levels of abstraction in a computer system. We highlight three levels here, working from the highest to the lowest level in the system hierarchy.
#### ğŸ‘‰ Process/Thread-level Concurrency
â†— [Concurrent Programming](../../../ğŸ”‘%20CS%20Core/ğŸ‘·ğŸ¾â€â™‚ï¸%20Computer%20(Host)%20System/Operating%20System%20&%20OS%20Kernel%20(Theory%20Part)/OS%20Processes%20&%20Automata%20Management%20(CPU%20+%20Main%20Memory%20Resource)/Concurrent%20Programming.md)

â†— [Operating Systems /Concurrency Control](../../../ğŸ”‘%20CS%20Core/ğŸ‘·ğŸ¾â€â™‚ï¸%20Computer%20(Host)%20System/Operating%20System%20&%20OS%20Kernel%20(Theory%20Part)/OS%20Processes%20&%20Automata%20Management%20(CPU%20+%20Main%20Memory%20Resource)/Concurrency%20Control/Concurrency%20Control.md)
â†— [Database Systems /Concurrency Control](../../../ğŸ”‘%20CS%20Core/ğŸ•%20Computer%20Storage%20&%20Database%20Systems/Database%20Systems/âšœï¸%20Database%20System%20Design/ğŸ“Œ%20DBMS%20Design/Physical%20Database%20Design%20(Software%20Engineering)/Transaction%20Management/Concurrency%20Control/Concurrency%20Control.md)
â†— [OS Level Programming /Concurrency](../../../Software%20Engineering/ğŸ‘‡%20System%20Software%20Engineering/OS%20Level%20Programming%20in%20Different%20Languages/OS%20Level%20Programming%20with%20C%20&%20CPP/Process%20Management/Concurrency.md)
#### ğŸ‘‰ Hardware Level Parallel Computing
â†— [Multiprocessors and Multicore Processor](../../../ğŸ”‘%20CS%20Core/ğŸ‘·ğŸ¾â€â™‚ï¸%20Computer%20(Host)%20System/Computer%20Architecture/Computer%20Microarchitectures%20(Computer%20Organization)%20&%20von%20Neumann%20Model/ğŸš¦%20Computer%20Processors%20&%20Logic%20Chips/Multiprocessors%20and%20Multicore%20Processor/Multiprocessors%20and%20Multicore%20Processor.md)
- â†— [Multiprocessor Architectures & Parallel Computing](../../../ğŸ”‘%20CS%20Core/ğŸ‘·ğŸ¾â€â™‚ï¸%20Computer%20(Host)%20System/Computer%20Architecture/Computer%20Microarchitectures%20(Computer%20Organization)%20&%20von%20Neumann%20Model/ğŸš¦%20Computer%20Processors%20&%20Logic%20Chips/Multiprocessors%20and%20Multicore%20Processor/Multiprocessor%20Architectures%20&%20Parallel%20Computing/Multiprocessor%20Architectures%20&%20Parallel%20Computing.md)
#### ğŸ‘‰ Instruction Level Parallelism
â†— [ILP (Instruction Level Parallelism)](../../../ğŸ”‘%20CS%20Core/ğŸ›£ï¸%20Programming%20Language%20Processing%20&%20Program%20Execution/ğŸ¤¡%20Program%20Execution%20(Runtime)/Instruction%20Execution/ILP%20(Instruction%20Level%20Parallelism)/ILP%20(Instruction%20Level%20Parallelism).md)
#### ğŸ‘‰ Software Level Parallel Computing
â†— [Compute Unified Device Architecture & CUDA Programming](../../../ğŸ”‘%20CS%20Core/ğŸ‘·ğŸ¾â€â™‚ï¸%20Computer%20(Host)%20System/Computer%20Interfaces%20&%20Hardware%20Drivers/ğŸ›%20Computer%20(IO%20Devices)%20Drivers%20&%20Programming/Graphics%20Devices%20Drivers/Compute%20Unified%20Device%20Architecture%20&%20CUDA%20Programming/Compute%20Unified%20Device%20Architecture%20&%20CUDA%20Programming.md)
openMP
MPI (Message Passing Interface)
mpi4pi
openacc
opencl



## Ref
[å¹¶è¡Œç¼–ç¨‹å¯¼è®º]: https://hpcwiki.io/parallel-programming/parallel-programming-intro/
æˆ‘ä»¬ä¼šçœ‹åˆ°å¤šé‡å¤„ç†å·²ç»è¾¾åˆ°äº† Amdahl å®šå¾‹çš„ä¸Šé™ã€‚ä¸ºäº†åœ¨èƒ½è€—ã€æˆæœ¬ã€æ€§èƒ½çš„ä¸‰è§’ä¸­å–å¾—å¹³è¡¡ï¼Œå”¯ä¸€çš„é€”å¾„å°±æ˜¯**ä¸“ç”¨**ã€‚æˆ‘ä»¬åœ¨å…¶ä»–ç« èŠ‚è¿˜ä¼šä»‹ç» Tensor Core çš„ä½“ç³»ç»“æ„ï¼Œå®ƒå°±æ˜¯ä¸“ç”¨å¤„ç†å™¨çš„å…¸å‹ä»£è¡¨ï¼Œèƒ½å¤Ÿå¤§å¹…æå‡çŸ©é˜µä¹˜æ³•çš„æ€§èƒ½ã€‚
