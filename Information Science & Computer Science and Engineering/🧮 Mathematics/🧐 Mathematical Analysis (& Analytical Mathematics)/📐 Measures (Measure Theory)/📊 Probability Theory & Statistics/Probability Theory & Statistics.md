# Probabilities  & Statistics

[TOC]



## Res
### Related Topics
â†— [Information Theory](../../../ğŸ¥¸%20Information%20Theory/Information%20Theory.md)

â†— [Mathematical Modeling & Real World Problem Solving](../../../Mathematical%20Modeling%20&%20Real%20World%20Problem%20Solving.md)
â†— [Models of Computation & Abstract Machines](../../../ğŸ¤¼â€â™€ï¸%20Mathematical%20Logic%20(Foundations%20of%20Mathematics)/ğŸ˜¶â€ğŸŒ«ï¸%20Theory%20of%20Computation/Models%20of%20Computation%20&%20Abstract%20Machines/Models%20of%20Computation%20&%20Abstract%20Machines.md)

â†— [AI Basics & Machine Learning (ML)](../../../../ğŸ§ %20Computing%20Methodologies/ğŸ‘½%20Artificial%20Intelligence/ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning%20(ML)/AI%20Basics%20&%20Machine%20Learning%20(ML).md)
â†— [Statistical Learning & Machine Learning Methods](../../../../ğŸ§ %20Computing%20Methodologies/ğŸ‘½%20Artificial%20Intelligence/ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning%20(ML)/Statistical%20Learning%20&%20Machine%20Learning%20Methods/Statistical%20Learning%20&%20Machine%20Learning%20Methods.md)
- â†— [Reinforcement Learning (RL) & Sequential Decision Making](../../../../ğŸ§ %20Computing%20Methodologies/ğŸ‘½%20Artificial%20Intelligence/ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning%20(ML)/Statistical%20Learning%20&%20Machine%20Learning%20Methods/Reinforcement%20Learning%20(RL)%20&%20Sequential%20Decision%20Making/Reinforcement%20Learning%20(RL)%20&%20Sequential%20Decision%20Making.md)

â†— [Data-Oriented & Human-Centered Technologies](../../../../Data-Oriented%20&%20Human-Centered%20Technologies/Data-Oriented%20&%20Human-Centered%20Technologies.md)
- â†— [Data Science](../../../../Data-Oriented%20&%20Human-Centered%20Technologies/Data%20Science/Data%20Science.md)
- â†— [Data Mining](../../../../Data-Oriented%20&%20Human-Centered%20Technologies/Data%20Science/â›ï¸%20Data%20Mining/Data%20Mining.md)


### Learning Resources
ğŸ« [UCB /CS70 Discrete Math and Probability Theory](../../../../ğŸ—º%20CS%20Overview/ğŸ’‹%20Intro%20to%20Computer%20Science/ğŸ‘©ğŸ¼â€ğŸ«%20Courses%20of%20Universities/UC%20Berkeley/CS70%20Discrete%20Math%20and%20Probability%20Theory/CS70%20Discrete%20Math%20and%20Probability%20Theory.md)
ğŸ« [UCB /CS126 Probability Theory](../../../../ğŸ—º%20CS%20Overview/ğŸ’‹%20Intro%20to%20Computer%20Science/ğŸ‘©ğŸ¼â€ğŸ«%20Courses%20of%20Universities/UC%20Berkeley/CS126%20Probability%20Theory/CS126%20Probability%20Theory.md)

æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ï¼ˆç¬¬â¼†ç‰ˆï¼‰
é™ˆé¸¿å»º èµµæ°¸çº¢ ç¿æ´‹ é«˜ç­‰æ•™è‚²å‡ºç‰ˆç¤¾
- æ¦‚ç‡è®º
	- ç¬¬ä¸€ç«  æ¦‚ç‡è®ºåŸºæœ¬çŸ¥è¯†
		- æ ·æœ¬ç©ºé—´ä¸éšæœºäº‹ä»¶
		- äº‹ä»¶å‘ç”Ÿçš„æ¦‚ç‡
		- ç­‰å¯èƒ½æ¨¡å‹
			- å¤å…¸æ¦‚å‹
			- å‡ ä½•æ¦‚å‹
		- æ¡ä»¶æ¦‚ç‡åŠæ´¾ç”Ÿçš„ä¸‰ä¸ªå…¬å¼
			- æ¡ä»¶æ¦‚ç‡
			- ä¹˜æ³•å…¬å¼
			- å…¨æ¦‚ç‡å…¬å¼ä¸è´å¶æ–¯å…¬å¼
		- äº‹ä»¶çš„ç‹¬ç«‹æ€§åŠä¼¯åŠªåˆ©æ¦‚å‹
	- ç¬¬äºŒç«  éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ
		- éšæœºå˜é‡åŠå…¶åˆ†å¸ƒå‡½æ•°
		- ç¦»æ•£å‹éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ
		- è¿ç»­å‹éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ
		- éšæœºå˜é‡å‡½æ•°çš„åˆ†å¸ƒ
	- ç¬¬ä¸‰ç«  å¤šç»´éšæœºå˜é‡åŠå…¶åˆ†å¸ƒ
		- äºŒç»´éšæœºå˜é‡åŠå…¶åˆ†å¸ƒå‡½æ•°
		- è¾¹ç¼˜åˆ†å¸ƒåŠéšæœºå˜é‡çš„ç‹¬ç«‹æ€§
		- æ¡ä»¶åˆ†å¸ƒä¸æ¡ä»¶å¯†åº¦
		- äºŒç»´éšæœºå˜é‡å‡½æ•°çš„åˆ†å¸ƒ
		- å¤šç»´éšæœºå˜é‡
			- nç»´ç¦»æ•£å‹éšæœºå˜é‡
			- nç»´è¿ç»­å‹éšæœºå˜é‡
	- ç¬¬å››ç«  éšæœºå˜é‡çš„æ•°å­—ç‰¹å¾
		- æ•°å­¦æœŸæœ›
		- æ–¹å·®
		- åæ–¹å·®å’Œç›¸å…³ç³»æ•°
	- ç¬¬äº”ç«  æ­£æ€åˆ†å¸ƒä¸è‡ªç„¶æŒ‡æ•°åˆ†å¸ƒæ—
		- æ­£æ€åˆ†å¸ƒåŠå…¶å¯†åº¦å‡½æ•°å’Œåˆ†å¸ƒå‡½æ•°
		- æ­£æ€åˆ†å¸ƒçš„æ•°å­—ç‰¹å¾å’Œçº¿æ€§æ€§è´¨
		- äºŒç»´æ­£æ€åˆ†å¸ƒ
		- è‡ªç„¶æŒ‡æ•°åˆ†å¸ƒæ—
	- ç¬¬å…­ç«  æé™å®šç†
		- å¤§æ•°å®šå¾‹ /å¤§æ•°å®šç†
			- åˆ‡æ¯”é›ªå¤«ä¸ç­‰å¼
			- å¤§æ•°å®šå¾‹
		- ä¸­å¿ƒæé™å®šå¾‹
- æ•°ç†ç»Ÿè®¡
	- ç¬¬ä¸ƒç«  æ•°ç†ç»Ÿè®¡çš„åŸºç¡€çŸ¥è¯†
		- æ€»ä½“ä¸æ ·æœ¬
		- $\chi^2$ åˆ†å¸ƒï¼Œ$t$ åˆ†å¸ƒï¼ŒåŠ$F$åˆ†å¸ƒ
		- ç»Ÿè®¡é‡å’ŒæŠ½æ ·åˆ†å¸ƒå®šç†
			- ç»Ÿè®¡é‡
			- æŠ½æ ·åˆ†å¸ƒå®šç†
	- ç¬¬å…«ç«  å‚æ•°ä¼°è®¡
		- ç‚¹ä¼°è®¡
		- ä¼°è®¡é‡çš„è¯„é€‰æ ‡å‡†
		- åŒºé—´ä¼°è®¡
	- ç¬¬ä¹ç«  å‡è®¾æ£€éªŒ
		- å‡è®¾æ£€éªŒçš„åŸºæœ¬æ¦‚å¿µ
		- æ­£æ€æ€»ä½“ä¸‹å‚æ•°çš„å‡è®¾æ£€éªŒ
		- è‡ªç„¶æŒ‡æ•°åˆ†å¸ƒæ—å‡å€¼å‚æ•°çš„æ£€éªŒ
		- æ€»ä½“åˆ†å¸ƒçš„$\chi^2$æ‹Ÿåˆä¼˜åº¦æ£€éªŒ
	- ç¬¬åç«  çº¿å½¢å›å½’åˆ†æå’Œæ–¹å·®åˆ†æ
		- çº¿å½¢å›å½’åˆ†æ
		- å•å› ç´ è¯•éªŒçš„æ–¹å·®åˆ†æ
		- åŒå› ç´ æ— é‡å¤è¯•éªŒçš„æ–¹å·®åˆ†æ
		- åŒå› ç´ æœ‰é‡å¤è¯•éªŒçš„æ–¹å·®åˆ†æ
	- ç¬¬åä¸€ç«  SPSS 13.0 For Windows ç®€ä»‹

ğŸ“– ä½•ä¹¦å…ƒã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹
- ç¬¬ä¸€ç«  å¤å…¸æ¦‚å‹å’Œæ¦‚ç‡ç©ºé—´
	- 1.1 è¯•éªŒä¸äº‹ä»¶. . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
	- 1.2 å¤å…¸æ¦‚å‹ä¸å‡ ä½•æ¦‚å‹. . . . . . . . . . . . . . . . . . . . . . . 7
		- 1.2.1 å¤å…¸æ¦‚å‹. . . . . . . . . . . . . . . . . . . . . . . . . 7
		- 1.2.2 å‡ ä½•æ¦‚å‹. . . . . . . . . . . . . . . . . . . . . . . . . 14
	- 1.3 æ¦‚ç‡çš„å…¬ç†åŒ–å’ŒåŠ æ³•å…¬å¼. . . . . . . . . . . . . . . . . . . . 15
		- 1.3.1 æ¦‚ç‡çš„å…¬ç†åŒ–. . . . . . . . . . . . . . . . . . . . . . . 15
		- 1.3.2 æ¦‚ç‡çš„åŠ æ³•å…¬å¼. . . . . . . . . . . . . . . . . . . . . . 17
		- 1.3.3 æ¦‚ç‡çš„è¿ç»­æ€§. . . . . . . . . . . . . . . . . . . . . . . 18
	- 1.4 æ¡ä»¶æ¦‚ç‡å’Œä¹˜æ³•å…¬å¼. . . . . . . . . . . . . . . . . . . . . . . 18
	- 1.5 äº‹ä»¶çš„ç‹¬ç«‹æ€§. . . . . . . . . . . . . . . . . . . . . . . . . . . 21
	- 1.6 å…¨æ¦‚ç‡å…¬å¼ä¸ Bayes å…¬å¼. . . . . . . . . . . . . . . . . . . . 24
		- 1.6.1 å…¨æ¦‚ç‡å…¬å¼. . . . . . . . . . . . . . . . . . . . . . . . 24
		- 1.6.2 Bayes å…¬å¼. . . . . . . . . . . . . . . . . . . . . . . . 28
	- 1.7 æ¦‚ç‡ä¸é¢‘ç‡. . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
- ç¬¬äºŒç«  éšæœºå˜é‡å’Œæ¦‚ç‡åˆ†å¸ƒ
	- 2.1 éšæœºå˜é‡. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
	- 2.2 ç¦»æ•£å‹éšæœºå˜é‡. . . . . . . . . . . . . . . . . . . . . . . . . . 35
	- 2.3 è¿ç»­å‹éšæœºå˜é‡. . . . . . . . . . . . . . . . . . . . . . . . . . 43
	- 2.4  æ¦‚ç‡åˆ†å¸ƒå‡½æ•°. . . . . . . . . . . . . . . . . . . . . . . . . . . 51
		- 2.4.1 æ¦‚ç‡åˆ†å¸ƒå‡½æ•°. . . . . . . . . . . . . . . . . . . . . . . 51
		- 2.4.2 å¸¸è§åˆ†å¸ƒçš„åˆ†å¸ƒå‡½æ•°. . . . . . . . . . . . . . . . . . . 54
	- 2.5 éšæœºå˜é‡å‡½æ•°çš„åˆ†å¸ƒ. . . . . . . . . . . . . . . . . . . . . . . 56
- ç¬¬ä¸‰ç«  éšæœºå‘é‡åŠå…¶åˆ†å¸ƒ
	- 3.1 éšæœºå‘é‡åŠå…¶è”åˆåˆ†å¸ƒ. . . . . . . . . . . . . . . . . . . . . . 63
	- 3.2 ç¦»æ•£å‹éšæœºå‘é‡åŠå…¶åˆ†å¸ƒ. . . . . . . . . . . . . . . . . . . . 65
	- 3.3 è¿ç»­å‹éšæœºå‘é‡åŠå…¶åˆ†å¸ƒ. . . . . . . . . . . . . . . . . . . . 68
	- 3.4 éšæœºå‘é‡å‡½æ•°çš„åˆ†å¸ƒ. . . . . . . . . . . . . . . . . . . . . . . 75
	- 3.5 æå¤§æå°å€¼çš„åˆ†å¸ƒ. . . . . . . . . . . . . . . . . . . . . . . . 81
	- 3.6 æ¡ä»¶åˆ†å¸ƒå’Œæ¡ä»¶å¯†åº¦. . . . . . . . . . . . . . . . . . . . . . . 84
- ç¬¬å››ç«  æ•°å­¦æœŸæœ›å’Œæ–¹å·®
	- 4.1 æ•°å­¦æœŸæœ›. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
		- 4.1.1 æ•°å­¦æœŸæœ›æ¦‚å¿µ. . . . . . . . . . . . . . . . . . . . . . . 91
		- 4.1.2 å¸¸è§åˆ†å¸ƒæ•°å­¦æœŸæœ›. . . . . . . . . . . . . . . . . . . . 96
	- 4.2 æ•°å­¦æœŸæœ›çš„æ€§è´¨. . . . . . . . . . . . . . . . . . . . . . . . . . 99
		- 4.2.1 éšæœºå‘é‡å‡½æ•°çš„æ•°å­¦æœŸæœ›. . . . . . . . . . . . . . . . 99
		- 4.2.2 æ•°å­¦æœŸæœ›çš„æ€§è´¨. . . . . . . . . . . . . . . . . . . . . . 102
	- 4.3 éšæœºå˜é‡çš„æ–¹å·®. . . . . . . . . . . . . . . . . . . . . . . . . . 106
	- 4.4 åæ–¹å·®å’Œç›¸å…³ç³»æ•°. . . . . . . . . . . . . . . . . . . . . . . . 115
- ç¬¬äº”ç«  å¤šå…ƒæ­£æ€åˆ†å¸ƒå’Œæé™å®šç†
	- 5.1 å¤šå…ƒæ­£æ€åˆ†å¸ƒ. . . . . . . . . . . . . . . . . . . . . . . . . . . 119
	- 5.2 å¤§æ•°å¾‹. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
	- 5.3 ä¸­å¿ƒæé™å®šç†. . . . . . . . . . . . . . . . . . . . . . . . . . . 126
- ç¬¬å…­ç«  æè¿°æ€§ç»Ÿè®¡
	- 6.1 æ€»ä½“å’Œå‚æ•°. . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
	- 6.2 æŠ½æ ·è°ƒæŸ¥æ–¹æ³•. . . . . . . . . . . . . . . . . . . . . . . . . . . 133
	- 6.3 ç”¨æ ·æœ¬ä¼°è®¡æ€»ä½“åˆ†å¸ƒ. . . . . . . . . . . . . . . . . . . . . . . 141
	- 6.4 ä¼—æ•°å’Œä¸­ä½æ•°. . . . . . . . . . . . . . . . . . . . . . . . . . . 148
	- 6.5 éšæœºå¯¹ç…§è¯•éªŒ. . . . . . . . . . . . . . . . . . . . . . . . . . . 152
- ç¬¬ä¸ƒç«  å‚æ•°ä¼°è®¡
	- 7.1 ç‚¹ä¼°è®¡å’ŒçŸ©ä¼°è®¡. . . . . . . . . . . . . . . . . . . . . . . . . . 159
	- 7.2 æœ€å¤§ä¼¼ç„¶ä¼°è®¡. . . . . . . . . . . . . . . . . . . . . . . . . . . 166
		- 7.2.1 ç¦»æ•£å‹éšæœºå˜é‡çš„æƒ…å†µ. . . . . . . . . . . . . . . . . . 166
		- 7.2.2 è¿ç»­å‹éšæœºå˜é‡çš„æƒ…å†µ. . . . . . . . . . . . . . . . . . 168
	- 7.3 æŠ½æ ·åˆ†å¸ƒåŠå…¶ä¸Š Î± åˆ†ä½æ•°. . . . . . . . . . . . . . . . . . . . 173
		- 7.3.1 æŠ½æ ·åˆ†å¸ƒ. . . . . . . . . . . . . . . . . . . . . . . . . 174
		- 7.3.2 æŠ½æ ·åˆ†å¸ƒçš„ä¸Š Î± åˆ†ä½æ•°. . . . . . . . . . . . . . . . . 179
	- 7.4 æ­£æ€æ€»ä½“çš„åŒºé—´ä¼°è®¡. . . . . . . . . . . . . . . . . . . . . . . 182
		- 7.4.1 å·²çŸ¥ Ïƒ æ—¶, Âµ çš„ç½®ä¿¡åŒºé—´. . . . . . . . . . . . . . . . . 183
		- 7.4.2 æœªçŸ¥ Ïƒ æ—¶ Âµ çš„ç½®ä¿¡åŒºé—´. . . . . . . . . . . . . . . . . 185
		- 7.4.3 æ–¹å·® $Ïƒ_2$ çš„ç½®ä¿¡åŒºé—´. . . . . . . . . . . . . . . . . . . 187
		- 7.4.4 å‡å€¼å·® $Âµ_1âˆ’Âµ_2$ çš„ç½®ä¿¡åŒºé—´. . . . . . . . . . . . . . . 189
		- 7.4.5 æ–¹å·®æ¯” $Ïƒ^2_1 /Ïƒ^2_2$ çš„ç½®ä¿¡åŒºé—´. . . . . . . . . . . . . . . . 191
		- 7.4.6 å•ä¾§ç½®ä¿¡åŒºé—´. . . . . . . . . . . . . . . . . . . . . . . 191
	- 7.5 éæ­£æ€æ€»ä½“å’Œæ¯”ä¾‹ p çš„ç½®ä¿¡åŒºé—´. . . . . . . . . . . . . . . . 192
		- 7.5.1 æ­£æ€é€¼è¿‘æ³•. . . . . . . . . . . . . . . . . . . . . . . . 192
		- 7.5.2 æ¯”ä¾‹ p çš„ç½®ä¿¡åŒºé—´. . . . . . . . . . . . . . . . . . . . 194
- ç¬¬å…«ç«  å‡è®¾æ£€éªŒ
	- 8.1 å‡è®¾æ£€éªŒçš„æ¦‚å¿µ. . . . . . . . . . . . . . . . . . . . . . . . . . 197
	- 8.2 æ­£æ€å‡å€¼çš„å‡è®¾æ£€éªŒ. . . . . . . . . . . . . . . . . . . . . . . 201
		- 8.2.1 å·²çŸ¥ Ïƒ æ—¶, Âµ çš„æ­£æ€æ£€éªŒæ³•. . . . . . . . . . . . . . . 201
		- 8.2.2 p å€¼æ£€éªŒæ³•. . . . . . . . . . . . . . . . . . . . . . . . 203
		- 8.2.3 æœªçŸ¥ Ïƒ æ—¶, å‡å€¼ Âµ çš„ t æ£€éªŒæ³•. . . . . . . . . . . . . . 204
		- 8.2.4 æœªçŸ¥ Ïƒ æ—¶, Âµ çš„å•è¾¹æ£€éªŒæ³•. . . . . . . . . . . . . . . 205
		- 8.2.5 æ­£æ€è¿‘ä¼¼æ³•. . . . . . . . . . . . . . . . . . . . . . . . 208
	- 8.3 æ ·æœ¬é‡çš„é€‰æ‹©. . . . . . . . . . . . . . . . . . . . . . . . . . . 209
	- 8.4 å‡å€¼æ¯”è¾ƒçš„æ£€éªŒ. . . . . . . . . . . . . . . . . . . . . . . . . . 210
		- 8.4.1 å·²çŸ¥ $Ïƒ^2_1 \text{, } Ïƒ^2_2$ æ—¶, $Âµ_1 \text{, } Âµ_2$ çš„æ£€éªŒ. . . . . . . . . . . . . . 211
		- 8.4.2 æœªçŸ¥ $Ïƒ^2_1 \text{, } Ïƒ^2_2$, ä½†å·²çŸ¥ $Ïƒ^2_1 = Ïƒ^2_2$ æ—¶, $Âµ_1 - Âµ_2$ çš„æ£€éªŒ. . . . 213
		- 8.4.3 æˆå¯¹æ•°æ®çš„å‡è®¾æ£€éªŒ. . . . . . . . . . . . . . . . . . . 214
		- 8.4.4 æœªçŸ¥ $Ïƒ^2_1 \text{, } Ïƒ^2_2$ æ—¶, $Âµ_1 \text{, } Âµ_2$ çš„æ£€éªŒ. . . . . . . . . . . . . . 216
	- 8.5 æ–¹å·®çš„å‡è®¾æ£€éªŒ. . . . . . . . . . . . . . . . . . . . . . . . . . 217
	- 8.6 æ¯”ä¾‹çš„å‡è®¾æ£€éªŒ. . . . . . . . . . . . . . . . . . . . . . . . . . 219
		- 8.6.1 å°æ ·æœ¬æƒ…å†µä¸‹çš„å‡è®¾æ£€éªŒ. . . . . . . . . . . . . . . . 219
		- 8.6.2 å¤§æ ·æœ¬æƒ…å†µä¸‹å•ä¸ªæ¯”ä¾‹çš„å‡è®¾æ£€éªŒ. . . . . . . . . . . 221
		- 8.6.3 å¤§æ ·æœ¬æƒ…å†µä¸‹ä¸¤ä¸ªæ€»ä½“æ¯”ä¾‹çš„æ¯”è¾ƒ. . . . . . . . . . . 224
	- 8.7 æ€»ä½“åˆ†å¸ƒçš„å‡è®¾æ£€éªŒ. . . . . . . . . . . . . . . . . . . . . . . 227
- ç¬¬ä¹ç«  çº¿æ€§å›å½’åˆ†æ 233
	- 9.1 æ•°æ®çš„ç›¸å…³æ€§. . . . . . . . . . . . . . . . . . . . . . . . . . . 233
		- 9.1.1 æ ·æœ¬ç›¸å…³ç³»æ•°. . . . . . . . . . . . . . . . . . . . . . . 234
		- 9.1.2 ç›¸å…³æ€§æ£€éªŒ. . . . . . . . . . . . . . . . . . . . . . . . 236
	- 9.2 å›å½’ç›´çº¿. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
	- 9.3 ä¸€å…ƒçº¿æ€§å›å½’. . . . . . . . . . . . . . . . . . . . . . . . . . . 242
		- 9.3.1 æœ€å¤§ä¼¼ç„¶ä¼°è®¡å’Œæœ€å°äºŒä¹˜ä¼°è®¡. . . . . . . . . . . . . . 243
		- 9.3.2 å¹³æ–¹å’Œåˆ†è§£å…¬å¼. . . . . . . . . . . . . . . . . . . . . . 247
		- 9.3.3 æ–œç‡ b çš„æ£€éªŒ. . . . . . . . . . . . . . . . . . . . . . . 248
		- 9.3.4 é¢„æµ‹çš„ç½®ä¿¡åŒºé—´. . . . . . . . . . . . . . . . . . . . . . 249
	- 9.4 å¤šå…ƒçº¿æ€§å›å½’. . . . . . . . . . . . . . . . . . . . . . . . . . . 251
		- 9.4.1 æœ€å°äºŒä¹˜ä¼°è®¡. . . . . . . . . . . . . . . . . . . . . . . 252
		- 9.4.2 å›å½’æ˜¾è‘—æ€§æ£€éªŒ. . . . . . . . . . . . . . . . . . . . . . 253
		- 9.4.3 å•ä¸ªç³»æ•°çš„æ˜¾è‘—æ€§æ£€éªŒ. . . . . . . . . . . . . . . . . . 254
		- 9.4.4 æ®‹å·®è¯Šæ–­. . . . . . . . . . . . . . . . . . . . . . . . . 255


ğŸ¬ã€Šæ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡ã€‹æ•™å­¦è§†é¢‘å…¨é›†ï¼ˆå®‹æµ© https://www.bilibili.com/video/BV1ot411y7mU?p=9&share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d

ğŸ¬ã€æ¯”åˆ·å‰§è¿˜çˆ½!ã€‘ä¸€ç”Ÿæ¨ï¼ï¼ã€éº»çœç†å·¥å…¬å¼€è¯¾ã€‘å¬è¯´ä½ æ¦‚ç‡è®ºæŒ‚äº†ï¼Ÿ MIT æ¦‚ç‡è®º (ä¸­è‹±åŒè¯­å­—å¹•)å®Œæ•´ç‰ˆå…¨25è®²ï¼Œæ¦‚ç‡è®ºåº”è¯¥è¿™æ ·å­¦ https://www.bilibili.com/video/BV1MV4y1W73J?share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d

ğŸ“– æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡, é™ˆå¸Œå­º
https://github.com/sanbuphy/-statistics-
é™ˆå¸Œå­º æ¦‚ç‡è®ºä¸æ•°ç†ç»Ÿè®¡åŸºç¡€Â [å‚è€ƒè¯¾ç¨‹è§†é¢‘åœ°å€](https://www.bilibili.com/video/BV12k4y1m78w)  
ã€å¦‚æœè§‰å¾—è¿™ä¸ªè¿˜æ˜¯æ¯”è¾ƒéš¾ è¯·ä¸€å®šè¦çœ‹çœ‹å°æ¹¾è€å¸ˆçš„è¯¾ç¨‹ï¼Œä¸å¯èƒ½å¬ä¸æ‡‚ã€‘  
[å°æ¹¾å¤§å­¦ - é ‘æƒ³å­¸æ¦‚ç‡ï¼šæ©Ÿç‡ä¸€ (Probability (1))](https://www.bilibili.com/video/BV1nK4y1U7QM)  
[å°æ¹¾å¤§å­¦ - é ‘æƒ³å­¸æ¦‚ç‡ï¼šæ©Ÿç‡äºŒ (Probability (2))](https://www.bilibili.com/video/BV1CX4y1V7oN?p=23)  
[å°æ¹¾äº¤é€šå¤§å­¦ - çµ±è¨ˆå­¸ Statistics](https://ocw.nctu.edu.tw/course_detail-v.php?bgid=1&gid=1&nid=270)Â [å°æ¹¾äº¤é€šå¤§å­¦ - é«˜ç­‰çµ±è¨ˆå­¸ Advanced Statistics](https://ocw.nctu.edu.tw/course_detail-v.php?bgid=1&gid=4&nid=536)

ğŸ“– ç»Ÿè®¡å­¦ä¹ æ–¹æ³•, æèˆª

ğŸ“– INTRODUCTION TO PROBABILITY AND STATISTICS FOR ENGINEERS AND SCIENTISTS 
ğŸ“– A FIRST COURSE IN PROBABILITY
ğŸ“– Introduction to Probability Model (PM)
ğŸ“– Stochastic Process (SP)
Sheldon M. Ross

ğŸ“– è´å¶æ–¯åæ¼”

ğŸ“– H. Pishro-Nik, "Introduction to probability, statistics, and random processes", available at [https://www.probabilitycourse.com](https://www.probabilitycourse.com/), Kappa Research LLC, 2014.

ğŸ‘ https://www.math.wm.edu/~leemis/chart/UDR/UDR.html
![](../../../../../Assets/Pics/Screenshot%202025-10-05%20at%2023.37.13.png)

ğŸ‘ https://stanford.edu/~shervine/teaching/cme-106/
CME 106 â€• Introduction to Probability and Statistics for Engineers  
My twin brotherÂ [Afshine](https://twitter.com/afshinea)Â andÂ [I](https://twitter.com/shervinea)Â ([Afshine Amidi](https://twitter.com/afshinea)Â andÂ [Shervine Amidi](https://twitter.com/shervinea)) created this set of cheatsheets when I was a TA for Stanford's CME 106 class in Winter 2018. They can (hopefully!) be useful to all future students taking this course as well as to anyone else interested in learning the fundamentals of Probabilities and Statistics.
- [Probability cheatsheet](https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-probability)
- [Statistics cheatsheet](https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics)

ğŸ‘ https://www.wzchen.com/probability-cheatsheet
https://github.com/wzchen/probability_cheatsheet
- This cheatsheet is a 10-page reference in probability that covers a semester's worth of introductory probability.
- The cheatsheet is based off of Harvard's introductory probability course, Stat 110. It is co-authored by former Stat 110 Teaching Fellow William Chen and Stat 110 Professor Joe Blitzstein.


### Other Resources



## Intro
### Probability ğŸ†š Statistics?
#probability #statistics

> ğŸ¤– Gemini 2.5

Probability and statistics are two different, though related, fields that work in opposite directions:Â ==probability uses existing models and rules to predict future events, while statistics uses observed data to infer the underlying rules or models==.Â In essence, probability moves from model to data (future event of possibility), and statistics moves from data to model. 
$$\text{Data} \ \overset{Statistics}{\to} \ \text{Models (Rules)} \ \overset{Probability}{\to} \ Data ()$$
Note that probability models are included in statistical models. Probability is one specific feature characterized by statistical model (?)

> ğŸ”— https://zh.wikipedia.org/zh-hans/Portal:%E6%A6%82%E7%8E%87%E4%B8%8E%E7%BB%9F%E8%AE%A1

æ¦‚ç‡è®ºæ˜¯é›†ä¸­ç ”ç©¶æ¦‚ç‡åŠéšæœºç°è±¡çš„æ•°å­¦åˆ†æ”¯ï¼Œä¸»è¦ç ”ç©¶å¯¹è±¡ä¸ºéšæœºäº‹ä»¶ã€éšæœºå˜é‡ä»¥åŠéšæœºè¿‡ç¨‹ã€‚å¯¹äºéšæœºäº‹ä»¶æ˜¯ä¸å¯èƒ½å‡†ç¡®é¢„æµ‹å…¶ç»“æœçš„ï¼Œç„¶è€Œå¯¹äºä¸€ç³»åˆ—çš„ç‹¬ç«‹éšæœºäº‹ä»¶â€”â€”ä¾‹å¦‚æ·éª°å­ã€æ‰”ç¡¬å¸ã€æŠ½æ‰‘å…‹ç‰Œä»¥åŠè½®ç›˜ç­‰ï¼Œä¼šå‘ˆç°å‡ºä¸€å®šçš„ã€å¯ä»¥è¢«ç”¨äºç ”ç©¶åŠé¢„æµ‹çš„è§„å¾‹ï¼Œä¸¤ä¸ªç”¨æ¥æè¿°è¿™äº›è§„å¾‹çš„æœ€å…·ä»£è¡¨æ€§çš„æ•°å­¦ç»“è®ºåˆ†åˆ«æ˜¯å¤§æ•°å®šå¾‹å’Œä¸­å¿ƒæé™å®šç†ã€‚

ä½œä¸ºç»Ÿè®¡å­¦çš„æ•°å­¦åŸºç¡€ï¼Œæ¦‚ç‡è®ºå¯¹è¯¸å¤šæ¶‰åŠå¤§é‡æ•°æ®å®šé‡åˆ†æçš„äººç±»æ´»åŠ¨æä¸ºé‡è¦ï¼Œæ¦‚ç‡è®ºçš„æ–¹æ³•åŒæ ·é€‚ç”¨äºå…¶ä»–æ–¹é¢ï¼Œä¾‹å¦‚æ˜¯å¯¹åªçŸ¥é“ç³»ç»Ÿéƒ¨åˆ†çŠ¶æ€çš„å¤æ‚ç³»ç»Ÿçš„æè¿°â€”â€”ç»Ÿè®¡åŠ›å­¦ï¼Œè€ŒäºŒåä¸–çºªç‰©ç†å­¦çš„é‡å¤§å‘ç°æ˜¯ä»¥é‡å­åŠ›å­¦æ‰€æè¿°çš„åŸå­å°ºåº¦ä¸Šç‰©ç†ç°è±¡çš„æ¦‚ç‡æœ¬è´¨ã€‚

ç»Ÿè®¡å­¦æ˜¯å¯¹æ•°æ®çš„æ”¶é›†ã€åˆ†æã€è§£é‡Šã€å±•ç¤ºã€æ•´ç†è¿›è¡Œç ”ç©¶çš„å­¦ç§‘ï¼Œå¹¿æ³›åœ°åº”ç”¨åœ¨å„é—¨å­¦ç§‘ï¼Œä»è‡ªç„¶ç§‘å­¦ã€ç¤¾ä¼šç§‘å­¦åˆ°äººæ–‡å­¦ç§‘ï¼Œç”šè‡³è¢«ç”¨æ¥å·¥å•†ä¸šåŠæ”¿åºœçš„æƒ…æŠ¥å†³ç­–ä¹‹ä¸Šã€‚å…¶ä¸­ç”¨æ¥æè¿°ã€æ‘˜è¦æ•°æ®æƒ…å†µçš„ç»Ÿè®¡æ–¹æ³•ç§°ä¸ºæè¿°ç»Ÿè®¡å­¦ï¼›è€Œå¯¹è§‚æµ‹ä¸­éšæœºæ€§å’Œä¸ç¡®å®šæ€§ï¼Œå¯ä»¥é€šè¿‡å¯¹è§‚æµ‹æ•°æ®è¿›è¡Œæ•°å­¦å»ºæ¨¡æ‰€å¾—çš„è§„å¾‹è¿›è¡Œè§£é‡Šï¼Œç„¶ååˆ©ç”¨è¿™äº›è§„å¾‹å¯¹æ‰€ç ”ç©¶çš„è¿‡ç¨‹æˆ–æ€»ä½“è¿›è¡Œæ¨æ–­ï¼Œè¿™æ ·çš„ç»Ÿè®¡æ–¹æ³•ç§°ä¸ºæ¨è®ºç»Ÿè®¡å­¦ã€‚

> ğŸ”— https://www.zhihu.com/question/438873440/answer/1675856602

æ¦‚ç‡è®ºæ˜¯å·²çŸ¥æ€»ä½“æœä»ä»€ä¹ˆåˆ†å¸ƒï¼Œä»è€Œæ¨æ–­å‡ºè¿™ä¸ªåˆ†å¸ƒæœ‰ä»€ä¹ˆæ ·çš„æ€§è´¨ï¼Œæ¯”å¦‚å·²çŸ¥åˆ†å¸ƒï¼Œæ±‚æœŸæœ›æ–¹å·®ï¼›

æ•°ç†ç»Ÿè®¡å¥½æ¯”æ€»ä½“æ˜¯æœªçŸ¥çš„ï¼Œé€šè¿‡ä»æ€»ä½“ä¸­æŠ½å–çš„æ ·æœ¬ï¼Œ**ç›®çš„**æ˜¯æ¥æ¨æ–­æ€»ä½“å…·æœ‰ä»€ä¹ˆæ ·çš„ç‰¹ç‚¹ã€‚æ¯”å¦‚ï¼Œæ•°ç†ç»Ÿè®¡ä¸­æ ·æœ¬çš„ç»Ÿè®¡é‡ï¼ˆ[æ ·æœ¬æ–¹å·®](https://zhida.zhihu.com/search?content_id=334778127&content_type=Answer&match_order=1&q=%E6%A0%B7%E6%9C%AC%E6%96%B9%E5%B7%AE&zhida_source=entity)ï¼Œ[æ ·æœ¬å‡å€¼](https://zhida.zhihu.com/search?content_id=334778127&content_type=Answer&match_order=1&q=%E6%A0%B7%E6%9C%AC%E5%9D%87%E5%80%BC&zhida_source=entity)ï¼‰å·²çŸ¥ï¼Œä½†æ˜¯æ€»ä½“çš„åˆ†å¸ƒæ˜¯æœªçŸ¥çš„ã€‚è¿™ä¸ª[æ€»ä½“åˆ†å¸ƒ](https://zhida.zhihu.com/search?content_id=334778127&content_type=Answer&match_order=1&q=%E6%80%BB%E4%BD%93%E5%88%86%E5%B8%83&zhida_source=entity)æœªçŸ¥æœ‰æ—¶å€™æ˜¯æ€»ä½“åˆ†å¸ƒç±»å‹æœªçŸ¥ï¼Œæœ‰æ—¶å€™æ˜¯æ€»ä½“åˆ†å¸ƒç±»å‹å·²çŸ¥ä½†åˆ†å¸ƒçš„å‚æ•°æœªçŸ¥ã€‚ï¼ˆå¦‚å‡å®šæ€»ä½“æœä»æ­£æ€åˆ†å¸ƒï¼Œä½†å‡å€¼æ–¹å·®æœªçŸ¥ï¼‰

ä¸€èˆ¬æ¦‚ç‡è®ºæ•°ç†ç»Ÿè®¡è¿™æœ¬æ•™æä¸­æ•°ç†ç»Ÿè®¡å†…å®¹ä¸»è¦å›´ç»•æ¨æ–­åˆ†å¸ƒå‚æ•°å±•å¼€ã€‚

æ•°ç†ç»Ÿè®¡åˆ†ä¸¤å—ï¼Œ[å‚æ•°ä¼°è®¡](https://zhida.zhihu.com/search?content_id=334778127&content_type=Answer&match_order=1&q=%E5%8F%82%E6%95%B0%E4%BC%B0%E8%AE%A1&zhida_source=entity)å’Œ[å‡è®¾æ£€éªŒ](https://zhida.zhihu.com/search?content_id=334778127&content_type=Answer&match_order=1&q=%E5%81%87%E8%AE%BE%E6%A3%80%E9%AA%8C&zhida_source=entity)ï¼Œè€Œè¿™ä¸¤ç±»éƒ½æ˜¯å…³äºæ€»ä½“åˆ†å¸ƒçš„æ¨æ–­ã€‚
- å‚æ•°ä¼°è®¡ï¼š
	- ç‚¹ä¼°è®¡ï¼šä¸ºæœªçŸ¥å‚æ•°æŒ‘é€‰å‡ºä¸€ä¸ªå¯èƒ½å€¼
	- åŒºé—´ä¼°è®¡ï¼šä¸ºæœªçŸ¥å‚æ•°æŒ‘é€‰å‡ºä¸€ä¸ªå¯èƒ½å–å€¼åŒºé—´
- å‡è®¾æ£€éªŒï¼šæ£€éªŒæœªçŸ¥å‚æ•°æ˜¯å¦ä¸ºè¿™ä¸ªå€¼æˆ–è€…åœ¨è¿™ä¸ªåŒºé—´å†…ã€‚

ä¸€è¨€ä»¥è”½ä¹‹ï¼Œæ•°ç†ç»Ÿè®¡å¥½æ¯”æ˜¯æ¦‚ç‡è®ºçš„â€œé€†è¿ç®—â€ã€‚

> ğŸ”— https://www3.cs.stonybrook.edu/~skiena/jaialai/excerpts/node12.html#

Probability and statistics are related areas of mathematics which concern themselves with analyzing the relative frequency of events. Still, there are fundamental differences in the way they see the world:
- _Probability_Â deals with predicting the likelihood of future events, whileÂ _statistics_Â involves the analysis of the frequency of past events.Â Â Â 
- _Probability_Â is primarily a theoretical branch of mathematics, which studies the consequences of mathematical definitions.Â _Statistics_Â is primarily an applied branch of mathematics, which tries to make sense of observations in the real world.

Both subjects are important, relevant, and useful. But they are different, and understanding the distinction is crucial in properly interpreting the relevance of mathematical evidence. Many a gambler has gone to a cold and lonely grave for failing to make the proper distinction between probability and statistics.

This distinction will perhaps become clearer if we trace the thought process of a mathematician encountering her first craps game:
- If this mathematician were a probabilist, she would see the dice and think `Six-sided dice? Presumably each face of the dice is equally likely to land face up. NowÂ _assuming_Â that each face comes up with probability 1/6, I can figure out what my chances of crapping out are.`
- If instead a statistician wandered by, she would see the dice and think `Those dice may look OK, but how do IÂ _know_Â that they are not loaded? I'll watch a while, and keep track of how often each number comes up. Then I can decide if my observations are consistent with the assumption of equal-probability faces. Once I'm confident enough that the dice are fair, I'll call a probabilist to tell me how to play.`

In summary, probability theory enables us to find the consequences of a given ideal world, while statistical theory enables us to to measure the extent to which our world is ideal.

Modern probability theory emerged from the dice tables of France in 1654. Chevalier de MÃ©rÃ©, a French nobleman, wondered whether the player or the house had the advantage in a variation of the following betting game.[6.1](https://www3.cs.stonybrook.edu/~skiena/jaialai/excerpts/footnode.html#foot389)Â In the basic version, the player rolls four dice, and wins provided none of them are a six. The house collects on the even money bet if at least one six appears.Â Â 

De MÃ©rÃ© brought this problem to attention of the French mathematicians Blaise Pascal and Pierre de Fermat, most famous as the source of Fermat's Last Theorem. Together, these men worked out the basics of probability theory, along the way establishing that the house wins the basic version with probabilityÂ $p = 1 - (5/6)^4 \approx 0.517$, where the probabilityÂ $pÂ = 0.5$ would denote a fair game where the house wins exactly half the time. The jai-alai world of our Monte Carlo simulation assumes that we decide the outcome of a point between two teams by flipping a suitably biased coin. If this world were reality, our simulation will compute the correct probability of each possible betting outcome. But all players are not created equal, of course. By doing a statistical study of the outcome of all the matches involving a particular player, we can determine an appropriate amount to bias the coin.

But such computations only make sense if our simulated jai-alai world is a model consistent with the real world. John von Neuman once said that  "the valuation of a poker hand can be sheer mathematics." We have to reduce our evaluation of a pelotari to sheer mathematics.



## ğŸ¯ Probability Theory
### Probability Theory Basics
#### Sampling Space & Random Event
![](../../../../../../Assets/Pics/Screenshot%202025-11-15%20at%2022.46.56.png)
#### Probability of Random Event

**1. Frequency of Event**

**2. Axiomatization of Probability**

#### Equally Likely Model (ç­‰å¯èƒ½æ¨¡å‹)
##### Classical Probability Model / Classical Probability (å¤å…¸æ¦‚å‹)
##### Geometric Probability Model (å‡ ä½•æ¦‚å‹)

#### Conditional Probability
Conditional Probability & Multiplication Formula (æ¡ä»¶æ¦‚ç‡å’Œä¹˜æ³•å…¬å¼)

Bayes Ruleï¼ˆè´å¶æ–¯å…¬å¼ï¼‰

Total Probability Theoremï¼ˆå…¨æ¦‚ç‡å…¬å¼ï¼‰
Prior Probabilityï¼ˆå…ˆéªŒæ¦‚ç‡ï¼‰
Posterior Probabilityï¼ˆåéªŒæ¦‚ç‡ï¼‰
#### Independence of Event & Bernoulli Trial (Independent and Repeated Trials)


### Random Variables & Probabilistic Distribution â­
â†— [Random (Stochastic) Variable & Probability Distribution](Random%20(Stochastic)%20Variable%20&%20Probability%20Distribution/Random%20(Stochastic)%20Variable%20&%20Probability%20Distribution.md)

â†— [Continuous Probability Distribution](Random%20(Stochastic)%20Variable%20&%20Probability%20Distribution/Continuous%20Probability%20Distribution/Continuous%20Probability%20Distribution.md)
â†— [Discrete Probability Distribution](Random%20(Stochastic)%20Variable%20&%20Probability%20Distribution/Discrete%20Probability%20Distribution/Discrete%20Probability%20Distribution.md)


### Law of Large Numbers â­
> ğŸ”— https://en.wikipedia.org/wiki/Law_of_large_numbers

In probability theory, the law of large numbers is a mathematical law that states that the average of the results obtained from a large number of independent random samples converges to the true value, if it exists.[1] More formally, the law of large numbers states that given a sample of independent and identically distributed values, the sample mean converges to the true mean.

The law of large numbers is important because it guarantees stable long-term results for the averages of some random events.[1][2] For example, while a casino may lose money in a single spin of the roulette wheel, its earnings will tend towards a predictable percentage over a large number of spins. Any winning streak by a player will eventually be overcome by the parameters of the game. Importantly, the law applies (as the name indicates) only when a large number of observations are considered. There is no principle that a small number of observations will coincide with the expected value or that a streak of one value will immediately be "balanced" by the others (see the gambler's fallacy).

The law of large numbers only applies to the average of the results obtained from repeated trials and claims that this average converges to the expected value; it does not claim that the sum of n results gets close to the expected value times n as n increases.

Throughout its history, many mathematicians have refined this law. Today, the law of large numbers is used in many fields including statistics, probability theory, economics, and insurance.[3]
#### Forms
##### Weak Law

##### Strong Law

##### Differences Between The Weak Law And The Strong Law

##### Uniform Laws of Large Numbers

##### Borel's Law of Large Numbers

#### Proofs
##### Proof of The Weak Law
###### Proof Using Chebyshev's Inequality Assuming Finite Variance

###### Proof Using Convergence of Characteristic Functions

##### Proof of The Strong Law


### Central Limit Theorem (CLT) â­
> ğŸ”— https://en.wikipedia.org/wiki/Central_limit_theorem

InÂ [probability theory](https://en.wikipedia.org/wiki/Probability_theory "Probability theory"), theÂ **central limit theorem**Â (**CLT**) states that, under appropriate conditions, theÂ [distribution](https://en.wikipedia.org/wiki/Probability_distribution "Probability distribution")Â of a normalized version of the sample mean converges to aÂ [standard normal distribution](https://en.wikipedia.org/wiki/Normal_distribution#Standard_normal_distribution "Normal distribution"). This holds even if the original variables themselves are notÂ [normally distributed](https://en.wikipedia.org/wiki/Normal_distribution "Normal distribution"). There are several versions of the CLT, each applying in the context of different conditions.

The theorem is a key concept in probability theory because it implies that probabilistic and statistical methods that work for normal distributions can be applicable to many problems involving other types of distributions.

This theorem has seen many changes during the formal development of probability theory. Previous versions of the theorem date back to 1811, but in its modern form it was only precisely stated in the 1920s.

InÂ [statistics](https://en.wikipedia.org/wiki/Statistics "Statistics"), the CLT can be stated as: letÂ $X_{1},X_{2},\dots ,X_{n}$Â denote aÂ [statistical sample](https://en.wikipedia.org/wiki/Sampling_\(statistics\) "Sampling (statistics)")Â of sizeÂ $n$Â from a population withÂ [expected value](https://en.wikipedia.org/wiki/Expected_value "Expected value")Â (average)Â $\mu$Â and finite positiveÂ [variance](https://en.wikipedia.org/wiki/Variance "Variance")Â $\sigma^2$, and letÂ ${\bar {X}}_{n}$Â denote the sample mean (which is itself aÂ [random variable](https://en.wikipedia.org/wiki/Random_variable "Random variable")). Then theÂ [limit asÂ nâ†’âˆÂ of the distribution](https://en.wikipedia.org/wiki/Convergence_of_random_variables#Convergence_in_distribution "Convergence of random variables")Â ofÂ $({\bar {X}}_{n}-\mu ){\sqrt {n}}$Â is a normal distribution with meanÂ 0Â and varianceÂ $\sigma ^{2}$.

In other words, suppose that a large sample ofÂ [observations](https://en.wikipedia.org/wiki/Random_variate "Random variate")Â is obtained, each observation being randomly produced in a way that does not depend on the values of the other observations, and the average ([arithmetic mean](https://en.wikipedia.org/wiki/Arithmetic_mean "Arithmetic mean")) of the observed values is computed. If this procedure is performed many times, resulting in a collection of observed averages, the central limit theorem says that if the sample size is large enough, theÂ [probability distribution](https://en.wikipedia.org/wiki/Probability_distribution "Probability distribution")Â of these averages will closely approximate a normal distribution.

The central limit theorem has several variants. In its common form, the random variables must beÂ [independent and identically distributed](https://en.wikipedia.org/wiki/Independent_and_identically_distributed "Independent and identically distributed")Â (i.i.d.). This requirement can be weakened; convergence of the mean to the normal distribution also occurs for non-identical distributions or for non-independent observations if they comply with certain conditions.

The earliest version of this theorem, that the normal distribution may be used as an approximation to theÂ [binomial distribution](https://en.wikipedia.org/wiki/Binomial_distribution "Binomial distribution"), is theÂ [de Moivreâ€“Laplace theorem](https://en.wikipedia.org/wiki/De_Moivre%E2%80%93Laplace_theorem "De Moivreâ€“Laplace theorem").
#### Beyond the Classical Framework
> ğŸ”— https://en.wikipedia.org/wiki/Central_limit_theorem#Beyond_the_classical_framework



## ğŸ¯ Statistics & Statistical Analysis
### Statistics Basics

#### Population & Sample (æ€»ä½“ä¸æ ·æœ¬)

#### Statistic & Sampling Distribution Theorem (ç»Ÿè®¡é‡ä¸æŠ½æ ·åˆ†å¸ƒå®šç†)


### Bayesian Statistics
> ğŸ¤– Gemini 3.0 

**Bayesian statistics** is a system of data analysis based on the idea that probability represents a "degree of belief" rather than a frequency of occurrence.

In simple terms, instead of looking at data in isolation, Bayesian statistics allows you to combine your **prior beliefs** (what you thought before measuring) with **new data** (what you just measured) to update your knowledge.

â†— [Bayesian Statistics & Statistical Analysis](Bayesian%20Statistics%20&%20Statistical%20Analysis/Bayesian%20Statistics%20&%20Statistical%20Analysis.md)

â†— [Descriptive Statistics (Analysis)](Bayesian%20Statistics%20&%20Statistical%20Analysis/Descriptive%20Statistics%20(Analysis)/Descriptive%20Statistics%20(Analysis).md)
- â†— [Statistics (Data) Analyzing Methods & Statistical Model](ğŸ’%20Statistics%20(Data)%20Analyzing%20Methods%20&%20Statistical%20Model/Statistics%20(Data)%20Analyzing%20Methods%20&%20Statistical%20Model.md)

â†— [Inferential Statistics (Analysis) & Statistical Inference](Bayesian%20Statistics%20&%20Statistical%20Analysis/Inferential%20Statistics%20(Analysis)%20&%20Statistical%20Inference/Inferential%20Statistics%20(Analysis)%20&%20Statistical%20Inference.md)
- â†— [Statistics (Data) Analyzing Methods & Statistical Model](ğŸ’%20Statistics%20(Data)%20Analyzing%20Methods%20&%20Statistical%20Model/Statistics%20(Data)%20Analyzing%20Methods%20&%20Statistical%20Model.md)
	- â†— [Estimation Theory (Parametric Estimation)](ğŸ’%20Statistics%20(Data)%20Analyzing%20Methods%20&%20Statistical%20Model/ğŸ“Œ%20Estimation%20Theory%20(Parametric%20Estimation)/Estimation%20Theory%20(Parametric%20Estimation).md)
	- â†— [Statistical Hypothesis (Significance) Test](ğŸ’%20Statistics%20(Data)%20Analyzing%20Methods%20&%20Statistical%20Model/ğŸ“Œ%20Statistical%20Hypothesis%20(Significance)%20Test/Statistical%20Hypothesis%20(Significance)%20Test.md)
	- â†— [Regression (Correlation) Analysis](ğŸ’%20Statistics%20(Data)%20Analyzing%20Methods%20&%20Statistical%20Model/Regression%20(Correlation)%20Analysis/Regression%20(Correlation)%20Analysis.md)



## Ref
[ğŸ¤” Zero probability and impossibility | StackExchange, Mathematics]: https://math.stackexchange.com/q/41107/1230830
I Â think the crux of the matter is what probability actuallyÂ _is_:
- **The Bayesian view**Â - probabilities are measures of (personal) confidence or belief, so it's quite obvious why an event with probability zero is not the same thing as an impossible event. But perhaps this isn't such a satisfactory answer.
- **The frequentist view**Â - probabilities are the asymptotic frequency of events as the number of independent trials tends to infinity. Here again wee see that something that happens with probability zero is not the same as something impossible; it's just something that happens so infrequently that the numerator inÂ $\frac{occurences}{trials}$Â is dominated by the denominator.

---
LetÂ ğ´Â be an event,Â PrÂ be the probability measure.

ğ´Â has zero probability ifÂ Pr(ğ´)=0.

ğ´Â is impossible ifÂ ğ´=âˆ….

**Impossibility implies zero probability, but the reverse is false.** Consider the real lineÂ â„; if you randomly select a numberÂ ğ‘¥, the probability thatÂ ğ‘¥=0Â isÂ 0, but this is not impossible. In fact, the probability thatÂ ğ‘¥Â belongs to some countable set, e.gÂ â„š, is alsoÂ 0.

From a purely mathematical point of view, impossibility is simply a stronger statement, so impossibilityÂ _cannot_Â be described by probability measure. However, another way of thinking might shed some light. That is, if the probability that something exists has probability greater thanÂ 0, then it exists. This notion has been used for some mathematical arguments.

---
Zero probability isn't impossibility. If you were to choose a random number from the real line, 1 has zero probability of being chosen, but still it's possible to choose 1.

---
Mathematicians generally formalize probability using the notion of aÂ [probability space](http://en.wikipedia.org/wiki/Probability_space)Â and measure theory. In this formalism it is possible for an event to have probabilityÂ 0Â without being the empty event. Perhaps the simplest "realistic" (and I use the word loosely) example of such an event is the event of flipping only heads infinitely many times. This event has probabilityÂ 0, but it is not empty, which is what one might call a formal definition of "impossible."

The underlying probability space is the set of possible ways to flip a coin infinitely many times. An example of an impossible event here is that you flip, say, cat. The coin has only a heads side and a tails side; it doesn't have a cat side, so flipping cat is impossible.

(Whether this formalism says anything reasonable about the real world is debatable. In practice, events of sufficiently small probability are already impossible. The above is just a statement about a certain mathematical formalism that has proven to be useful in certain contexts. In mathematics, we want to prove statements about some class of objects. Sometimes we can prove that the statement holds with probabilityÂ 1, but this does not imply that it holds for all objects, and since we actually care aboutÂ _all_Â objects this distinction really does need to be made in mathematics.)


[From ANOVA to regression: 10 key statistical analysis methods explained | medium]: https://dovetail.com/research/key-statistical-analysis-methods-explained/
