# Hyper-parameters

[TOC]



## Res


## Intro
The weights that are being adjusted by our optimization algorithms are called “parameters”, and neural networks are known as “parameterized models”.

There is another dimension of neural networks that must be “optimized” as well to get good results, called ”hyperparameters”. This is related to the design of the neural network. Important hyperparameters include:

- Architecture (Dense networks, Long-Short-Term Memories, Convolutional Neural Networks, Autoencoders, Generative-Adversarial Networks, etc)
- \# of input nodes (usually constraints by the problem itself) §Input and output encoding.
- \# of hidden layers.
- Size of each hidden layer.
- Loss functions
- Transfer functions.
- Optimization functions and their parameters (learning rate, momentum, etc.)
- Dropouts and Regularizers.



## Ref

