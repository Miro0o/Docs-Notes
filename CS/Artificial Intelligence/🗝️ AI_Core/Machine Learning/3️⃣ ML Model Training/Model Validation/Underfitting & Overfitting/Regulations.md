# Regulations

[TOC]



## Res


## Intro




## Ref
[🔥 机器学习中使用正则化来防止过拟合是什么原理？ - 俞扬的回答 - 知乎]: https://www.zhihu.com/question/20700829/answer/64824761

过拟合是一种现象。当我们提高在训练数据上的表现时，在测试数据上反而下降，这就被称为过拟合，或过配。

过拟合发生的本质原因，是由于监督学习问题的不适定：在高中数学我们知道，从n个（线性无关）方程可以解n个变量，解n+1个变量就会解不出。在监督学习中，往往数据(对应了方程）远远少于模型空间(对应了变量）。因此过拟合现象的发生，可以分解成以下三点：
1. ﻿﻿﻿有限的训练数据不能完全反映出一个模型的好坏，然而我们却不得不在这有限的数据上挑选模型，因此我们完全有可能挑选到在训练数据上表现很好而在测试数据上表现很差的模型，因为我们完全无法知道模型在测试数据上的表现。
2. ﻿﻿﻿如果模型空问很大，也就是有很多很多模型可以给我们挑选，那么挑到对的模型的机会就会很小。
3. ﻿﻿与此同时，如果我们要在训练数据上表现良好，最为直接的方法就是要在足够大的模型空间中挑选模型，否则如果模型空问很小，就不存在能够拟合数据很好的模型。

由上3点可见，要拟合训练数据，就要足够大的模型空问；用了足够大的模型空问，挑选到测试性能好的模型的概率就会下降。因此，就会出现训练数据拟合越好，测试性能越差的过拟合现象。

过拟合现象有多种解释:
- ﻿经典的是bias-variance decomposition， 但个人认为这种解释更加倾向于直观理解；
- ﻿PAC-learning 泛化界解释，这种解释是最透彻，最fundamental的；
- ﻿Bayes先验解释，这种解释把正则变成先验，在我看来等于没解释。

另外值得一提的是，不少人会用“模型复杂度"替代上面我讲的”模型空间”。这其实是一回事，但"模型复杂度"往往容易给人一个误解，认为是一个模型本身长得复杂。例如5次多项式就要比2次多项式复杂，这是错的。因此我更愿意用"模型空间”，强调"复杂度"是候选模型的"数量”，而不是模型本事的“长相"。

最后回答为什么正则化能够避免过拟合：因为正则化就是控制模型空间的一种办法。

[🔥 机器学习中使用正则化来防止过拟合是什么原理？ - 慧航的回答 - 知乎]: https://www.zhihu.com/question/20700829/answer/586902014


[👍 机器学习中使用正则化来防止过拟合是什么原理？ - 蛤蟆仙人的回答 - 知乎]: https://www.zhihu.com/question/20700829/answer/52064924

最简单的解释就是加了先验。在数据少的时候，先验知识可以防止过拟合。
举2个例子：
1. 抛硬币，推断正面朝上的概率。如果只能抛5次，很可能5次全正面朝上，这样你就得出错误的结论：正面朝上的概率是1--------过拟合！如果你在模型里加正面朝上概率是0.5的先验，结果就不会那么离谱。这其实就是正则。

2. 最小二乘回归问题：**加2范数正则等价于加了高斯分布的先验，加1范数正则相当于加拉普拉斯分布先验**。



