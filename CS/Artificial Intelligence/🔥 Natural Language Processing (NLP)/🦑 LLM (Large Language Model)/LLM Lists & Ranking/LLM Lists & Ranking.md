# LLM Lists & Ranking

[TOC]



## Res
ğŸ  
ğŸš§ 


### Related Topics



## Intro
### Language Models Basics
> ğŸ“ https://cameronrwolfe.substack.com/p/understanding-and-using-supervised

- Transformer Architecture](https://cameronrwolfe.substack.com/i/136366740/the-transformer-from-top-to-bottom): Nearly all modern language modelsâ€”_and many other deep learning models_â€”are based upon this architecture.
- [Decoder-only Transformers]https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)Â : This is the specific variant of the transformer architecture that is used by most generative LLMs.
- [Brief History of LLMs](https://twitter.com/cwolferesearch/status/1639378997627826176?s=20): LLMs have gone through several phases from the creation ofÂ [GPT](https://cameronrwolfe.substack.com/i/85568430/improving-language-understanding-by-generative-pre-training-gpt)Â  to the release of ChatGPT.Â 
- [Next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction): thisÂ [self-supervised](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)Â training objective underlies nearly all LLM functionality and is used by SFT!
- [Language Model Pretraining](https://cameronrwolfe.substack.com/i/136638774/language-model-pretraining): language models are pretrained over a massive, unlabeled textual corpus.Â 
- [Language Model Inference](https://cameronrwolfe.substack.com/i/136638774/autoregressive-inference-process): language models can be used to generate coherent sequences of text via autoregressive next token prediction.



## A list of popular Large Language Models
> ğŸ”— https://github.com/Hannibal046/Awesome-LLM/tree/main


- [Gemma](https://blog.google/technology/developers/gemma-open-models/)Â - Gemma is built for responsible AI development from the same research and technology used to create Gemini models.
- [Mistral](https://mistral.ai/)Â - Mistral-7B-v0.1 is a small, yet powerful model adaptable to many use-cases including code and 8k sequence length. Apache 2.0 licence.
- [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts/)Â - a high-quality sparse mixture of experts model (SMoE) with open weights.
- [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)Â &Â [LLaMA-2](https://ai.meta.com/llama/)Â - A foundational large language model.Â [LLaMA.cpp](https://github.com/ggerganov/llama.cpp)Â [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama)
    - [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html)Â - A model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations.Â [Alpaca.cpp](https://github.com/antimatter15/alpaca.cpp)Â [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)
    - [Flan-Alpaca](https://github.com/declare-lab/flan-alpaca)Â - Instruction Tuning from Humans and Machines.
    - [Baize](https://github.com/project-baize/baize-chatbot)Â - Baize is an open-source chat model trained withÂ [LoRA](https://github.com/microsoft/LoRA). It uses 100k dialogs generated by letting ChatGPT chat with itself.
    - [Cabrita](https://github.com/22-hours/cabrita)Â - A portuguese finetuned instruction LLaMA.
    - [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/)Â - An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality.
    - [Llama-X](https://github.com/AetherCortex/Llama-X)Â - Open Academic Research on Improving LLaMA to SOTA LLM.
    - [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna)Â - A Chinese Instruction-following LLaMA-based Model.
    - [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa)Â - 4 bits quantization ofÂ [LLaMA](https://arxiv.org/abs/2302.13971)Â usingÂ [GPTQ](https://arxiv.org/abs/2210.17323).
    - [GPT4All](https://github.com/nomic-ai/gpt4all)Â - Demo, data, and code to train open-source assistant-style large language model based on GPT-J and LLaMa.
    - [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/)Â - A Dialogue Model for Academic Research
    - [BELLE](https://github.com/LianjiaTech/BELLE)Â - Be Everyone's Large Language model Engine
    - [StackLLaMA](https://huggingface.co/blog/stackllama)Â - A hands-on guide to train LLaMA with RLHF.
    - [RedPajama](https://github.com/togethercomputer/RedPajama-Data)Â - An Open Source Recipe to Reproduce LLaMA training dataset.
    - [Chimera](https://github.com/FreedomIntelligence/LLMZoo)Â - Latin Phoenix.
    - [WizardLM|WizardCoder](https://github.com/nlpxucan/WizardLM)Â - Family of instruction-following LLMs powered by Evol-Instruct: WizardLM, WizardCoder.
    - [CaMA](https://github.com/zjunlp/CaMA)Â - a Chinese-English Bilingual LLaMA Model.
    - [Orca](https://aka.ms/orca-lm)Â - Microsoft's finetuned LLaMA model that reportedly matches GPT3.5, finetuned against 5M of data, ChatGPT, and GPT4
    - [BayLing](https://github.com/ictnlp/BayLing)Â - an English/Chinese LLM equipped with advanced language alignment, showing superior capability in English/Chinese generation, instruction following and multi-turn interaction.
    - [UltraLM](https://github.com/thunlp/UltraChat)Â - Large-scale, Informative, and Diverse Multi-round Chat Models.
    - [Guanaco](https://github.com/artidoro/qlora)Â - QLoRA tuned LLaMA
    - [ChiMed-GPT](https://github.com/synlp/ChiMed-GPT)Â - A Chinese medical large language model.
    - [RAFT](https://aka.ms/raft-blog)Â - RAFT: A new way to teach LLMs to be better at RAG ([paper](https://arxiv.org/abs/2403.10131)).
    - [Gorilla LLM](https://github.com/ShishirPatil/gorilla)Â - Gorilla: Large Language Model Connected with Massive APIs
    - [LLaVa](https://github.com/haotian-liu/LLaVA)Â - LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.
- [BLOOM](https://huggingface.co/bigscience/bloom)Â - BigScience Large Open-science Open-access Multilingual Language ModelÂ [BLOOM-LoRA](https://github.com/linhduongtuan/BLOOM-LORA)
    - [BLOOMZ&mT0](https://huggingface.co/bigscience/bloomz)Â - a family of models capable of following human instructions in dozens of languages zero-shot.
    - [Phoenix](https://github.com/FreedomIntelligence/LLMZoo)
- [Deepseek](https://github.com/deepseek-ai/)
    - [Coder](https://github.com/deepseek-ai/DeepSeek-Coder)Â - Let the Code Write Itself.
    - [LLM](https://github.com/deepseek-ai/DeepSeek-LLM)Â - Let there be answers.
    - çŸ¥åç§å‹Ÿå·¨å¤´å¹»æ–¹é‡åŒ–æ——ä¸‹çš„äººå·¥æ™ºèƒ½å…¬å¸æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeekï¼‰è‡ªä¸»ç ”å‘çš„å¤§è¯­è¨€æ¨¡å‹å¼€å‘çš„æ™ºèƒ½åŠ©æ‰‹ã€‚åŒ…æ‹¬Â [7B-base](https://modelscope.cn/models/deepseek-ai/deepseek-llm-7b-base/summary),Â [67B-base](https://modelscope.cn/models/deepseek-ai/deepseek-llm-67b-base/summary),
- [Yi](https://github.com/01-ai/Yi)Â - A series of large language models trained from scratch by developers @01-ai.
- [T5](https://arxiv.org/abs/1910.10683)Â - Text-to-Text Transfer Transformer
    - [T0](https://arxiv.org/abs/2110.08207)Â - Multitask Prompted Training Enables Zero-Shot Task Generalization
- [OPT](https://arxiv.org/abs/2205.01068)Â - Open Pre-trained Transformer Language Models.
- [UL2](https://arxiv.org/abs/2205.05131v1)Â - a unified framework for pretraining models that are universally effective across datasets and setups.
- [GLM](https://github.com/THUDM/GLM)- GLM is a General Language Model pretrained with an autoregressive blank-filling objective and can be finetuned on various natural language understanding and generation tasks.
    - [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B)Â - ChatGLM-6B æ˜¯ä¸€ä¸ªå¼€æºçš„ã€æ”¯æŒä¸­è‹±åŒè¯­çš„å¯¹è¯è¯­è¨€æ¨¡å‹ï¼ŒåŸºäºÂ [General Language Model (GLM)](https://github.com/THUDM/GLM)Â æ¶æ„ï¼Œå…·æœ‰ 62 äº¿å‚æ•°.
    - [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B)Â - An Open Bilingual Chat LLM | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹
    - [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3)Â - An Open Bilingual Chat LLMs | å¼€æºåŒè¯­å¯¹è¯è¯­è¨€æ¨¡å‹ ; IncludingÂ [ChatGLM3-6B-32k](https://huggingface.co/THUDM/chatglm3-6b-32k),Â [ChatGLM3-6B-128k](https://huggingface.co/THUDM/chatglm3-6b-128k).
- [RWKV](https://github.com/BlinkDL/RWKV-LM)Â - Parallelizable RNN with Transformer-level LLM Performance.
    - [ChatRWKV](https://github.com/BlinkDL/ChatRWKV)Â - ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model.
    - [Trending Demo](https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2)Â - RWKV-5 trained on 100+ world languages (70% English, 15% multilang, 15% code).
- [StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models)Â - Stability AI Language Models.
- [YaLM](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6)Â - a GPT-like neural network for generating and processing text. It can be used freely by developers and researchers from all over the world.
- [GPT-Neo](https://github.com/EleutherAI/gpt-neo)Â - An implementation of model & data parallelÂ [GPT3](https://arxiv.org/abs/2005.14165)-like models using theÂ [mesh-tensorflow](https://github.com/tensorflow/mesh)Â library.
- [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b)Â - A 6 billion parameter, autoregressive text generation model trained onÂ [The Pile](https://pile.eleuther.ai/).
    - [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html)Â - a cheap-to-build LLM that exhibits a surprising degree of the instruction following capabilities exhibited by ChatGPT.
- [Pythia](https://github.com/EleutherAI/pythia)Â - Interpreting Autoregressive Transformers Across Time and Scale
    - [Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm)Â - the first open source, instruction-following LLM, fine-tuned on a human-generated instruction dataset licensed for research and commercial use.
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo)Â - an open-source reproduction of DeepMind's Flamingo model.
- [Cerebras-GPT](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/)Â - A Family of Open, Compute-efficient, Large Language Models.
- [GALACTICA](https://github.com/paperswithcode/galai/blob/main/docs/model_card.md)Â - The GALACTICA models are trained on a large-scale scientific corpus.
    - [GALPACA](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b)Â - GALACTICA 30B fine-tuned on the Alpaca dataset.
- [Palmyra](https://huggingface.co/Writer/palmyra-base)Â - Palmyra Base was primarily pre-trained with English text.
- [Camel](https://huggingface.co/Writer/camel-5b-hf)Â - a state-of-the-art instruction-following large language model designed to deliver exceptional performance and versatility.
- [h2oGPT](https://github.com/h2oai/h2ogpt)
- [PanGu-Î±](https://openi.org.cn/pangu/)Â - PanGu-Î± is a 200B parameter autoregressive pretrained Chinese language model develped by Huawei Noah's Ark Lab, MindSpore Team and Peng Cheng Laboratory.
- [MOSS](https://github.com/OpenLMLab/MOSS)Â - MOSSæ˜¯ä¸€ä¸ªæ”¯æŒä¸­è‹±åŒè¯­å’Œå¤šç§æ’ä»¶çš„å¼€æºå¯¹è¯è¯­è¨€æ¨¡å‹.
- [Open-Assistant](https://github.com/LAION-AI/Open-Assistant)Â - a project meant to give everyone access to a great chat based large language model.
    - [HuggingChat](https://huggingface.co/chat/)Â - Powered by Open Assistant's latest model â€“ the best open source chat model right now and @huggingface Inference API.
- [StarCoder](https://huggingface.co/blog/starcoder)Â - Hugging Face LLM for Code
- [MPT-7B](https://www.mosaicml.com/blog/mpt-7b)Â - Open LLM for commercial use by MosaicML
- [Falcon](https://falconllm.tii.ae/)Â - Falcon LLM is a foundational large language model (LLM) with 40 billion parameters trained on one trillion tokens. TII has now released Falcon LLM â€“ a 40B model.
- [XGen](https://github.com/salesforce/xgen)Â - Salesforce open-source LLMs with 8k sequence length.
- [Baichuan](https://github.com/baichuan-inc)Â - A series of large language models developed by Baichuan Intelligent Technology.
- [Aquila](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila)Â - æ‚Ÿé“Â·å¤©é¹°è¯­è¨€å¤§æ¨¡å‹æ˜¯é¦–ä¸ªå…·å¤‡ä¸­è‹±åŒè¯­çŸ¥è¯†ã€æ”¯æŒå•†ç”¨è®¸å¯åè®®ã€å›½å†…æ•°æ®åˆè§„éœ€æ±‚çš„å¼€æºè¯­è¨€å¤§æ¨¡å‹ã€‚
- [phi-1](https://arxiv.org/abs/2306.11644)Â - a new large language model for code, with significantly smaller size than competing models.
- [phi-1.5](https://arxiv.org/abs/2309.05463)Â - a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which achieves common sense reasoning benchmark results comparable to models ten times its size that were trained on datasets more than ten times larger.
- [phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/)Â - a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters.
- [InternLM / ä¹¦ç”ŸÂ·æµ¦è¯­](https://github.com/InternLM/InternLM)Â - Official release of InternLM2 7B and 20B base and chat models. 200K context support.Â [Homepage](https://internlm.intern-ai.org.cn/)Â |Â [ModelScope](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-7b/summary)
- [BlueLM-7B](https://github.com/vivo-ai-lab/BlueLM)Â - BlueLM(è“å¿ƒå¤§æ¨¡å‹): Open large language models developed by vivo AI Lab.Â [Homepage](https://developers.vivo.com/product/ai/bluelm)Â |Â [ModelScope](https://modelscope.cn/models/vivo-ai/BlueLM-7B-Base/summary)Â [MoE-16B-base](https://modelscope.cn/models/deepseek-ai/deepseek-moe-16b-base), ç­‰. |Â [Chat with DeepSeek (Beta)](https://chat.deepseek.com/sign_in)
- [Qwen series](https://huggingface.co/Qwen)Â - The large language model series proposed by Alibaba Cloud. ï½œ é˜¿é‡Œäº‘ç ”å‘çš„é€šä¹‰åƒé—®å¤§æ¨¡å‹ç³»åˆ—. åŒ…æ‹¬Â [7B](https://huggingface.co/Qwen/Qwen-7B),Â [72B](https://huggingface.co/Qwen/Qwen-72B), åŠå„ç§é‡åŒ–å’ŒChatç‰ˆæœ¬.Â [Chat Demo](https://huggingface.co/spaces/Qwen/Qwen-72B-Chat-Demo)
- [XVERSE series](https://github.com/xverse-ai)Â - Multilingual large language model developed by XVERSE Technology Inc | ç”±æ·±åœ³å…ƒè±¡ç§‘æŠ€è‡ªä¸»ç ”å‘çš„æ”¯æŒå¤šè¯­è¨€çš„å¤§è¯­è¨€æ¨¡å‹. åŒ…æ‹¬[7B](https://github.com/xverse-ai/XVERSE-7B),Â [13B](https://github.com/xverse-ai/XVERSE-13B),Â [65B](https://github.com/xverse-ai/XVERSE-65B)ç­‰.
- [Skywork series](https://github.com/SkyworkAI/Skywork)Â - A series of large models developed by the Kunlun Group Â· Skywork team | æ˜†ä»‘ä¸‡ç»´é›†å›¢Â·å¤©å·¥å›¢é˜Ÿå¼€å‘çš„ä¸€ç³»åˆ—å¤§å‹æ¨¡å‹.
- [Command-R series](https://huggingface.co/CohereForAI)Â - Two multilingual large language models intended for retrieval augmented generation (RAG) and conversational use, atÂ [35](https://huggingface.co/CohereForAI/c4ai-command-r-v01)Â andÂ [104](https://huggingface.co/CohereForAI/c4ai-command-r-plus)Â billion parameters. 128k context support.
- [Jamba](https://huggingface.co/ai21labs/Jamba-v0.1)Â - A Hybrid Transformer-Mamba MoE model, with 52B params, first production grade mamba based LLM, 256K context support.



## Ref
