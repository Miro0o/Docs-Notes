# JAX

[TOC]



## Res
ğŸ  https://github.com/google/jax
ğŸ  https://jax.readthedocs.io/en/latest/



## Intro
JAX isÂ **[Autograd](https://github.com/hips/autograd)Â andÂ [XLA](https://www.tensorflow.org/xla)**, brought together for high-performance machine learning research.

With its updated version ofÂ [Autograd](https://github.com/hips/autograd), JAX can automatically differentiate native Python and NumPy functions. It can differentiate through loops, branches, recursion, and closures, and it can take derivatives of derivatives of derivatives. It supports reverse-mode differentiation (a.k.a. backpropagation) viaÂ [`grad`](https://github.com/google/jax#automatic-differentiation-with-grad)Â as well as forward-mode differentiation, and the two can be composed arbitrarily to any order.

Whatâ€™s new is that JAX usesÂ [XLA](https://www.tensorflow.org/xla)Â to compile and run your NumPy programs on GPUs and TPUs. Compilation happens under the hood by default, with library calls getting just-in-time compiled and executed. But JAX also lets you just-in-time compile your own Python functions into XLA-optimized kernels using a one-function API,Â [`jit`](https://github.com/google/jax#compilation-with-jit). Compilation and automatic differentiation can be composed arbitrarily, so you can express sophisticated algorithms and get maximal performance without leaving Python. You can even program multiple GPUs or TPU cores at once usingÂ [`pmap`](https://github.com/google/jax#spmd-programming-with-pmap), and differentiate through the whole thing.

Dig a little deeper, and you'll see that JAX is really an extensible system forÂ [composable function transformations](https://github.com/google/jax#transformations). BothÂ [`grad`](https://github.com/google/jax#automatic-differentiation-with-grad)Â andÂ [`jit`](https://github.com/google/jax#compilation-with-jit)Â are instances of such transformations. Others areÂ [`vmap`](https://github.com/google/jax#auto-vectorization-with-vmap)Â for automatic vectorization and[`pmap`](https://github.com/google/jax#spmd-programming-with-pmap)Â for single-program multiple-data (SPMD) parallel programming of multiple accelerators, with more to come.

==This is a research project, not an official Google product. Expect bugs andÂ [sharp edges](https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html).== Please help by trying it out,Â [reporting bugs](https://github.com/google/jax/issues), and letting us know what you think!

```python
import jax.numpy as jnp
from jax import grad, jit, vmap

def predict(params, inputs):
  for W, b in params:
    outputs = jnp.dot(inputs, W) + b
    inputs = jnp.tanh(outputs)  # inputs to the next layer
  return outputs                # no activation on last layer

def loss(params, inputs, targets):
  preds = predict(params, inputs)
  return jnp.sum((preds - targets)**2)

grad_loss = jit(grad(loss))  # compiled gradient evaluation function
perex_grads = jit(vmap(grad_loss, in_axes=(None, 0, 0)))  # fast per-example grads
```


## Ref

