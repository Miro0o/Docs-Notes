# RLHF (Reinforcement learning from Human Feedback)

[TOC]



## Res
ğŸ  
ğŸš§ 


### Related Topics



## Intro
> ğŸ”— https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/

Reinforcement learning from human feedback (RLHF) is a machine learning (ML) technique that uses human feedback to optimize ML models to self-learn more efficiently. Reinforcement learning (RL) techniques train software to make decisions that maximize rewards, making their outcomes more accurate. RLHF incorporates human feedback in the rewards function, so the ML model can perform tasks more aligned with human goals, wants, and needs. RLHF is used throughout generative artificial intelligence (generative AI) applications, including in large language models (LLM).

![](../../../../../../Assets/Pics/Pasted%20image%2020240520132208.png)



## Ref
[ğŸ‘ What is RLHF? | AWS]: https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/

[ğŸ‘ Illustrating Reinforcement Learning from Human Feedback (RLHF) | Hugging Face]: https://huggingface.co/blog/rlhf

![](../../../../../../Assets/Pics/Pasted%20image%2020240520132243.png)

![](../../../../../../Assets/Pics/Pasted%20image%2020240520132257.png)

[ğŸ‘ 2023å¹´ç¥ç§˜è€Œéš¾ä»¥ç†è§£çš„å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼šRLHF PPOï¼ŒDPOï¼Œä»¥åŠInstructGPTï¼ŒDeepSpeed-Chatï¼Œ LLama2ï¼ŒBaichuan2çš„RLHF - æ˜¯å¿µçš„æ–‡ç«  - çŸ¥ä¹]: https://zhuanlan.zhihu.com/p/662753985

![](../../../../../../Assets/Pics/Pasted%20image%2020240605224024.png)

