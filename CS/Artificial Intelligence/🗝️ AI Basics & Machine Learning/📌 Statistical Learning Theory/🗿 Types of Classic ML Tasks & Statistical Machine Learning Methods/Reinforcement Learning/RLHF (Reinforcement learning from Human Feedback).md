# RLHF (Reinforcement learning from Human Feedback)

[TOC]



## Res
🏠 
🚧 


### Related Topics



## Intro
> 🔗 https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/

Reinforcement learning from human feedback (RLHF) is a machine learning (ML) technique that uses human feedback to optimize ML models to self-learn more efficiently. Reinforcement learning (RL) techniques train software to make decisions that maximize rewards, making their outcomes more accurate. RLHF incorporates human feedback in the rewards function, so the ML model can perform tasks more aligned with human goals, wants, and needs. RLHF is used throughout generative artificial intelligence (generative AI) applications, including in large language models (LLM).

![](../../../../../../Assets/Pics/Pasted%20image%2020240520132208.png)



## Ref
[👍 What is RLHF? | AWS]: https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/

[👍 Illustrating Reinforcement Learning from Human Feedback (RLHF) | Hugging Face]: https://huggingface.co/blog/rlhf

![](../../../../../../Assets/Pics/Pasted%20image%2020240520132243.png)

![](../../../../../../Assets/Pics/Pasted%20image%2020240520132257.png)

[👍 2023年神秘而难以理解的大模型强化学习技术：RLHF PPO，DPO，以及InstructGPT，DeepSpeed-Chat， LLama2，Baichuan2的RLHF - 是念的文章 - 知乎]: https://zhuanlan.zhihu.com/p/662753985

![](../../../../../../Assets/Pics/Pasted%20image%2020240605224024.png)

