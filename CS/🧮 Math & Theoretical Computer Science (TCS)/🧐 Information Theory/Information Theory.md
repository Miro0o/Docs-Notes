# Information Theory

[TOC]



## Res
### Related Topics


### Learning Resources
#### Courses
 **ðŸ« [Information and Entropy](https://ocw.mit.edu/courses/6-050j-information-and-entropy-spring-2008/)**
This subject is designed for MIT freshmen. 

Spring 2008 is the sixth offering of this subject. It was offered in Spring [2003](https://mtlsites.mit.edu/Courses/6.050/2003/index.html), [2004](https://mtlsites.mit.edu/Courses/6.050/2004/index.html), [2005](https://mtlsites.mit.edu/Courses/6.050/2005/index.html), [2006](https://mtlsites.mit.edu/Courses/6.050/2006/index.html), [2007](https://mtlsites.mit.edu/Courses/6.050/2007/index.html) and, before then, three times while being developed, under another number, in Spring 2000, 2001, and 2002.

This subject is offered jointly by the Department of Electrical Engineering and Computer Science and the Department of Mechanical Engineering. Students may sign up for either 2.110J or 6.050J.

ðŸŽ¬ã€mitéº»çœä¿¡æ¯ä¸Žç†µã€‘ https://www.bilibili.com/video/BV1D441177hA/?p=3&share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d


**ðŸ« æ¶ˆæ¯ç†è«– Information Theory**
The purpose of this course is to present a concise, yet mathematically rigorous, introduction to the main pillars of information theory. It thus naturally focuses on the foundational concepts and indispensable results of the subject for single-user systems, where a single data source or message needs to be reliably processed and communicated over a noiseless or noisy point-to-point channel. At the first part of this course, six meticulously core chapters with accompanying problems, emphasizing the key topics of information measures, lossless and lossy data compression, channel coding, and joint source-channel coding. Two appendices covering necessary and supplementary material in real analysis and in probability and stochastic processes are included. At the second part of the course, advanced topics concerning the information theoretic limits of discrete-time single-user stochastic systems with arbitrary statistical memory (i.e., systems that are not necessarily stationary, ergodic or information stable) will be covered.

ðŸŽ¬ã€ä¿¡æ¯è®ºï¼ˆ2019å¹´æ˜¥ï¼‰å°æ¹¾äº¤é€šå¤§å­¦é™ˆä¼¯å®ã€‘ https://www.bilibili.com/video/BV14N41197bN/?p=2&share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d


**ðŸ« [Information Theory, Inference, and Learning Algorithms](http://www.inference.org.uk/mackay/itila/)**
> An instant classic, covering everything from Shannon's fundamental theorems to the postmodern theory of LDPC codes. You'll want two copies of this astonishing book, one for the office and one for the fireside at home.
>
> --- Bob McEliece, California Institute of Technology

**for teachers:** all the figures [available for download](http://www.inference.org.uk/mackay/itila/Figures.html) (as well as [the whole book](http://www.inference.org.uk/mackay/itila/book.html))

ðŸŽ¬ã€ã€Šä¿¡æ¯è®ºï¼ŒæŽ¨ç†ä¸Žå­¦ä¹ ç®—æ³•ã€‹Information Theory, Inference and Learning Algorithmsã€‘ https://www.bilibili.com/video/BV14b411G7wn/?share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d


**ðŸ« The Information Theory, Pattern Recognition, and Neural Networks**
- è¯¾ç¨‹ç½‘ç«™ï¼šhttp://www.inference.org.uk/mackay/itila/
- è¯¾ç¨‹è§†é¢‘ï¼šhttps://www.bilibili.com/video/BV1rs411T71e
- è¯¾ç¨‹æ•™æï¼šInformation Theory, Inference, and Learning Algorithms åœ¨è¯¾ç¨‹ç½‘ç«™å¯ä»¥ä¸‹è½½åˆ°å…è´¹çš„ç”µå­ç‰ˆ
- è¯¾ç¨‹ä½œä¸šï¼šåœ¨æ¯ä¸€èŠ‚è¯¾è§†é¢‘çš„æœ€åŽä¼šç•™æ•™æä¸Šçš„è¯¾åŽä¹ é¢˜
#### Books
Information Theory and Network Coding
https://iest2.ie.cuhk.edu.hk/~whyeung/post/draft2.pdf
> An undergraduate level course on probability is the only prerequisite for this book. For a non-technical introduction to information theory, we refer the reader to Encyclopedia Britannica.
> 
> For biographies of Claude Shannon, a legend of the 20th Century who had made fundamental contribution to the Information Age, we refer the readers to [53] and [307]. The latter is also a complete collection of Shannonâ€™s papers.
> [53] R. Calderbank and N. J. A. Sloane, â€œObituary: Claude Shannon (1916-2001),â€ Nature, 410: 768, April 12, 2001.
> [307] N. J. A. Sloane and A. D. Wyner, Ed., Claude Elwood Shannon Collected Papers, IEEE Press, New York, 1993.

Information, Physics, and Computation
https://web.stanford.edu/~montanar/RESEARCH/book.html
This is an introduction to a rich and rapidly evolving research field at the interface between statistical physics, theretical computer science, discrete mathematics, and coding information theory. It should be accessible to graduate students and researchers without specific training in any of these three fields.



## Intro



## Ref
