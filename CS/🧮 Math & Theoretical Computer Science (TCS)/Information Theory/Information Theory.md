# Information Theory

[TOC]



## Res
### Courses
#### ğŸ« [Information and Entropy](https://ocw.mit.edu/courses/6-050j-information-and-entropy-spring-2008/)
This subject is designed for MIT freshmen. 

Spring 2008 is the sixth offering of this subject. It was offered in Spring [2003](https://mtlsites.mit.edu/Courses/6.050/2003/index.html), [2004](https://mtlsites.mit.edu/Courses/6.050/2004/index.html), [2005](https://mtlsites.mit.edu/Courses/6.050/2005/index.html), [2006](https://mtlsites.mit.edu/Courses/6.050/2006/index.html), [2007](https://mtlsites.mit.edu/Courses/6.050/2007/index.html) and, before then, three times while being developed, under another number, in Spring 2000, 2001, and 2002.

This subject is offered jointly by the Department of Electrical Engineering and Computer Science and the Department of Mechanical Engineering. Students may sign up for either 2.110J or 6.050J.

ğŸ¬ã€mitéº»çœä¿¡æ¯ä¸ç†µã€‘ https://www.bilibili.com/video/BV1D441177hA/?p=3&share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d


#### ğŸ« æ¶ˆæ¯ç†è«– Information Theory
The purpose of this course is to present a concise, yet mathematically rigorous, introduction to the main pillars of information theory. It thus naturally focuses on the foundational concepts and indispensable results of the subject for single-user systems, where a single data source or message needs to be reliably processed and communicated over a noiseless or noisy point-to-point channel. At the first part of this course, six meticulously core chapters with accompanying problems, emphasizing the key topics of information measures, lossless and lossy data compression, channel coding, and joint source-channel coding. Two appendices covering necessary and supplementary material in real analysis and in probability and stochastic processes are included. At the second part of the course, advanced topics concerning the information theoretic limits of discrete-time single-user stochastic systems with arbitrary statistical memory (i.e., systems that are not necessarily stationary, ergodic or information stable) will be covered.

ğŸ¬ã€ä¿¡æ¯è®ºï¼ˆ2019å¹´æ˜¥ï¼‰å°æ¹¾äº¤é€šå¤§å­¦é™ˆä¼¯å®ã€‘ https://www.bilibili.com/video/BV14N41197bN/?p=2&share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d


#### ğŸ« [Information Theory, Inference, and Learning Algorithms](http://www.inference.org.uk/mackay/itila/)

> An instant classic, covering everything from Shannon's fundamental theorems to the postmodern theory of LDPC codes. You'll want two copies of this astonishing book, one for the office and one for the fireside at home.
>
> --- Bob McEliece, California Institute of Technology

**for teachers:** all the figures [available for download](http://www.inference.org.uk/mackay/itila/Figures.html) (as well as [the whole book](http://www.inference.org.uk/mackay/itila/book.html))

ğŸ¬ã€ã€Šä¿¡æ¯è®ºï¼Œæ¨ç†ä¸å­¦ä¹ ç®—æ³•ã€‹Information Theory, Inference and Learning Algorithmsã€‘ https://www.bilibili.com/video/BV14b411G7wn/?share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d


#### ğŸ« The Information Theory, Pattern Recognition, and Neural Networks

- è¯¾ç¨‹ç½‘ç«™ï¼šhttp://www.inference.org.uk/mackay/itila/
- è¯¾ç¨‹è§†é¢‘ï¼šhttps://www.bilibili.com/video/BV1rs411T71e
- è¯¾ç¨‹æ•™æï¼šInformation Theory, Inference, and Learning Algorithms åœ¨è¯¾ç¨‹ç½‘ç«™å¯ä»¥ä¸‹è½½åˆ°å…è´¹çš„ç”µå­ç‰ˆ
- è¯¾ç¨‹ä½œä¸šï¼šåœ¨æ¯ä¸€èŠ‚è¯¾è§†é¢‘çš„æœ€åä¼šç•™æ•™æä¸Šçš„è¯¾åä¹ é¢˜



## Intro



## Ref
