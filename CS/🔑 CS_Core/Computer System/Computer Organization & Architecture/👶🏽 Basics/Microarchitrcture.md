# Microarchitecture

[TOC]



Also check out this one â†— [von Neumann Based Microarchitecture](../ğŸ§ğŸ»â€â™€ï¸%20von%20Neumann%20Based%20Microarchitecture/von%20Neumann%20Based%20Microarchitecture.md) for more info.

## Overview
![](../../../../../Assets/Pics/Pasted%20image%2020230302132847.png)
<small>Microarchitecture in computer system hierarchy</small>

> ğŸ”— https://en.wikipedia.org/wiki/Microarchitecture

InÂ computer engineering,Â ==**microarchitecture**, also calledÂ **computer organization**== and sometimes abbreviated asÂ **Âµarch**Â orÂ **uarch**, is the way a givenÂ [instruction set architecture](https://en.wikipedia.org/wiki/Instruction_set_architecture "Instruction set architecture")Â (ISA) is implemented in a particularÂ processor.Â A given ISA may be implemented with different microarchitectures;implementations may vary due to different goals of a given design or due to shifts in technology.

[Computer architecture](https://en.wikipedia.org/wiki/Computer_architecture "Computer architecture")Â is the combination of microarchitecture and instruction set architecture.

> **å¾®æ¶æ„ï¼ˆMicroarchitectureï¼‰æ˜¯ISAåœ¨å¤„ç†å™¨çš„å®ç°**ï¼Œæè¿°å¤„ç†å™¨æ˜¯æ€æ ·å®ç°åŠŸèƒ½çš„ï¼Œå…¶æœ¬è´¨å°±æ˜¯ä¸€ç³»åˆ—ç¡¬ä»¶å®ç°ä»¥æ»¡è¶³å„ç§æŒ‡ä»¤é›†ã€‚è€ŒMicroarchitectureæ˜¯ISAçš„å…·ä½“å®ç°ï¼Œè€Œä¸”å¯¹äºåŒä¸€ä¸ªISAï¼Œå¯ä»¥ä½¿ç”¨ä¸åŒæŠ€æœ¯çš„å¾®æ¶æ„ ï¼Œæ¯”å¦‚å•å‘¨æœŸã€å¤šå‘¨æœŸä»¥åŠæµæ°´çº¿ã€‚æ¯”å¦‚è¯´x86 ISAæœ‰286ï¼Œ386ï¼Œ486ï¼ŒPretiumï¼ŒPretium Proç­‰å®ç°ã€‚ç›®å‰ï¼Œå¾®æ¶æ„æ¶‰åŠä»¥ä¸‹éƒ¨åˆ†ï¼š**æµæ°´çº¿ã€å¹¶è¡Œã€å­˜å‚¨ç³»ç»Ÿåˆ†å±‚ç»“æ„**.



## Microarchitecture Technologies
### Pipeline


### Parallel 

> â†— [von Neumann Model /Multiprocessor and Multicore Orgnization](../ğŸ§ğŸ»â€â™€ï¸%20von%20Neumann%20Based%20Microarchitecture/Processor/Multiprocessor%20and%20Multicore%20Orgnization.md)

#### Flynn's Taxonomy 

![](../../../../../Assets/Pics/Pasted%20image%2020230304154759.png)
<small>Flynn's Taxonomy of Computer Architectures</small>

> ğŸ”— Further reading: [Duncan's Taxonomy](https://en.wikipedia.org/wiki/Duncan%27s_taxonomy)


#### SISD
![](../../../../../Assets/Pics/Pasted%20image%2020230304155503.png)

> æ—©æœŸçš„**å†¯Â·è¯ºä¾æ›¼ç»“æ„**é‡‡ç”¨çš„éƒ½æ˜¯SISDï¼Œå†¯Â·è¯ºä¾æ›¼ç»“æ„ä¹Ÿç§°**å­˜å‚¨ç¨‹åºè®¡ç®—æœº**ï¼Œå®ƒæœ‰ä¸¤ä¸ªé‡è¦ç‰¹æ€§ï¼š1. æŒ‡ä»¤å’Œæ•°æ®è¢«å­˜å‚¨åœ¨å†…å­˜ä¸­ï¼›2. æŒ‡ä»¤è¢«é¡ºåºæ‰§è¡Œï¼Œåœ¨ä¸€ä¸ªæ—¶åˆ»åªæœ‰ä¸€æ¡æŒ‡ä»¤åœ¨æ‰§è¡Œï¼Œé€šè¿‡PCï¼ˆProgram Counterï¼‰è¯†åˆ«å½“å‰æŒ‡ä»¤ã€‚


#### SIMD
![](../../../../../Assets/Pics/Pasted%20image%2020230304155409.png)

> SIMD å°±æ˜¯å•æŒ‡ä»¤å¤šæ•°æ®çš„ç¼©å†™ï¼Œå°†ä¸€ä¸ªæŒ‡ä»¤å¹¿æ’­åˆ°å¤šä¸ªå¤„ç†å™¨ä¸Šï¼Œä½†æ¯ä¸ªå¤„ç†å™¨éƒ½æœ‰è‡ªå·±çš„æ•°æ®ã€‚ä¹Ÿå°±æ˜¯è¯´åŒä¸€ä»½æ§åˆ¶æŒ‡ä»¤åŒæ—¶è¿è¡Œä¸åŒçš„æ•°æ®ï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºï¼Œè¿™æ ·åšçš„å¥½å¤„å°±æ˜¯å‡è½»çš„æ§åˆ¶æˆæœ¬ã€‚ç›®å‰ï¼ŒGPUs(Graphics Processor Units)é‡‡ç”¨çš„å°±æ˜¯SIMDæ¶æ„ã€‚


#### MISD
Not implemented.


#### MIMD
![](../../../../../Assets/Pics/Pasted%20image%2020230304155426.png)

> MIMDæŒ‡çš„æ˜¯ä¸åŒæ—¶åˆ»ä¸åŒCPUæ‰§è¡Œçš„æŒ‡ä»¤ä¸åŒï¼Œå¹¶ä¸”æ•°æ®ä¹Ÿä¸åŒï¼Œå¦‚ä¸Šå›¾æ‰€ç¤ºã€‚ç›®å‰è¶…çº§è®¡ç®—æœºï¼Œç½‘ç»œå¹¶è¡Œè®¡ç®—æœºé›†ç¾¤å’Œâ€œç½‘æ ¼â€ï¼Œå¤šå¤„ç†å™¨SMPè®¡ç®—æœºï¼Œå¤šæ ¸PCéƒ½å±äºè¿™ä¸€ç±»ã€‚è€ŒMIMDåˆå¯ä»¥æ ¹æ®å†…å­˜ç»“æ„ï¼Œå¯ä»¥åˆ†ä¸º**å…±äº«å†…å­˜å’Œæ¶ˆæ¯é©±åŠ¨**ã€‚å…±äº«å†…å­˜å°±æ˜¯å¤„ç†å™¨ä¹‹é—´å…±äº«å†…å­˜ï¼Œé€šè¿‡å†…å­˜æ¥è¿›è¡Œé€šä¿¡ï¼Œè€Œæ¶ˆæ¯é©±åŠ¨æŒ‡çš„æ˜¯å¤„ç†å™¨é€šè¿‡æ¶ˆæ¯é©±åŠ¨æ¥è¿›è¡Œé€šä¿¡ã€‚ç›®å‰**å…±äº«å†…å­˜æœ‰SMPã€NUMAä¸¤ç§ï¼Œæ¶ˆæ¯é©±åŠ¨å¯¹åº”DM**ã€‚


### Hierarchical Stroage 

![](../../../../../../Assets/Pics/Pasted%20image%2020230301122408.png)
<small>Simplified Computer Memory Hierarchy </small>



## Microarchitecture Models
### Von Neumann Based Models
![](../../../../../Assets/Pics/Pasted%20image%2020230302132111.png)

![](../../../../../Assets/Pics/Screenshot%202023-03-02%20at%204.11.10%20PM.png)

Todayâ€™s version of the stored-program machine architecture satisfies at least the following characteristics:

1. Consists of three hardware systems: 
	1. a central processing unit (CPU) with 
		1. a control unit;
		2. an arithmetic logic unit (ALU);
		3. registers (small storage areas);
		4. a program counter; 
	2. a main memory system, which holds programs that control the computerâ€™s operation; 
	3. an I/O system.

3. Has the capacity to carry out sequential instruction processing.

4. Contains a single path, either physically or logically, between the main memory system and the control unit of the CPU, forcing alternation of instruction and execution cycles. This single path is often referred to as the ==von Neumann bottleneck==.



### Non-Von Neumann Models
Nonâ€“von Neumann architectures are those in which the model of computation varies from the characteristics listed for the von Neumann architecture. 
- For example, an architecture that does not store programs and data in memory or does not process a program sequentially would be considered a nonâ€“von Neumann machine. 
- Also, a computer that has two buses, one for data and a separate one for instructions, would be considered a nonâ€“von Neumann machine.

Many nonâ€“von Neumann machines are **designed for special purposes**.


A number of different subfields fall into the nonâ€“von Neumann category, including: 
1. Neural networks (using ideas from models of the brain as a computing paradigm) implemented in silicon, cellular automata, cognitive computers (machines that learn by experience rather than through programming, e.g., IBMâ€™s SyNAPSE computer, a machine that models the human brain);
2. Quantum computation (a combination of computing and quantum physics)
3. Dataflow computation;
4. Parallel computers. 
These all have something in common -- the computation is distributed among different processing units that act in parallel. They differ in how weakly or strongly the various components are connected. 

Of these, parallel computing is currently the most popular.


#### Harvard Based Models

![](../../../../../Assets/Pics/Pasted%20image%2020230302132344.png)

Harvard architecture have two buses, thus allowing data and instructions to be transferred simultaneously, but also have separate storage for data and instructions.

Many modern general-purpose computers use a modified version of the
Harvard architecture in which they have separate pathways for data and instructions but not separate storage.

Pure Harvard architectures are typically used in microcontrollers (an entire computer system on a chip), such as those found in embedded systems, as in appliances, toys, and cars.


#### â­ï¸ Parallel Processing Computer
Term "parallel" here describes a general conception of how computing is conducted. It concieves a way to improve computing proficiency with limited resources.

The engineering implementation to parallel computing or parallel processing can be roughly ascribed to two level: the hardware and the software.

1. On the hardware level, there are two ways to parallel compute as well:
	1. multiprocessing with multiple processors;
	2. multiprocessing with processors of multiple cores, or multicore processor.

> For more at â†— [von Neumann Model /Multiprocessor and Multicore Orgnization](../ğŸ§ğŸ»â€â™€ï¸%20von%20Neumann%20Based%20Microarchitecture/Processor/Multiprocessor%20and%20Multicore%20Orgnization.md)
> 

2. On the software level, there are two ways to parallel compute as well:
	1. multiprocessing with multiple processes, or multitasking. It means running multiple process for multiple tasks;
	2. multiprocessing with mnultiple threads, or multithreading. It means running multipe threads for divided sequential steps of tasks.

Very often, designing parallel programs to maximumally utilize the parallel designed hardware is the most difficult part in achieving parallel processing. Hence it's why even when parallel processing architectures is getting more and more complicated it is still not a common standard in daily lives to parallel compute: there are not efficient enough programs to support that set of hardwares.

> For more at â†— [Operating System Design /OS Design for Multiprocessor and Multicore](../../Operating%20System/ğŸ¦º%20Basics/Operating%20System%20Design.md)

##### Amdahlâ€™s Law
Even parallel computing has its limits, however. As the number of processors increases, so does the overhead of managing how tasks are distributed to those processors.

No matter how many processors we place in a system, or how many resources we assign to them, somehow, somewhere, a bottleneck is bound to develop. The best we can do, however, is make sure the slowest parts of the system are the ones that are used the least. This is the idea of Amdahlâ€™s Law.

Amdahlâ€™s law states that the performance enhancement possible with a given improvement is limited by the amount that the improved feature is used. The underlying premise is that every algorithm has a sequential part that ultimately limits the speedup that can be achieved by multiprocessor implementation.

#### Quantuem Computer
#TODO 


#### Dataflow Architecture
#TODO 



## Microprocessor
Microprocessor (or simple processor) is at the core of a computer system. It provides the very fountaional computing power. 

Though it varies on different computer architectures, microprocessor can be implemented as CPU (a general purpose processors unit), GPU, DSP or the latest SoC (System on a Chip).

More is noted at â†— [Microprocessor](Microprocessor.md).
Other processors at â†— [Processor](Processor.md).



## Ref
[æ¼«è°ˆè®¡ç®—æœºæ¶æ„]: https://segmentfault.com/a/1190000014885126
[Microarchitecture]: https://en.wikipedia.org/wiki/Microarchitecture#See_also
[Flynn's Taxonomy]: https://en.wikipedia.org/wiki/Flynn%27s_taxonomy