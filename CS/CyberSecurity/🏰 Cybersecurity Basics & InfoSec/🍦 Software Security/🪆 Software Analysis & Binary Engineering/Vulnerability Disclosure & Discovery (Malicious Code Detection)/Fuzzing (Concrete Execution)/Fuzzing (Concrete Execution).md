# Fuzzing (Concrete Execution)

[TOC]



## Res
### Related Topics
â†— [LLM & Fuzzing](../../../../../../../Academics%20ğŸ“/ğŸ—’ï¸%20My%20Academic%20Projects%20Workspace/LLM%20&%20Software%20Analysis/LLM%20&%20Fuzzing.md) ğŸ“
â†— [OSS-Fuzz](../../../../../â˜ ï¸%20Kill%20Chain%20&%20Security%20Tool%20Box/ğŸ”%20Software%20Analysis%20Tools/Fuzzers%20&%20Fuzzing%20Project/OSS-Fuzz.md)


### Learning Resources
#### Online Resources
ğŸ¬ã€æ¨¡ç³Šæµ‹è¯•Fuzzingå…¥é—¨-AFL(++)-fuzz-å“”å“©å“”å“©ã€‘ https://b23.tv/WioZP4F

ğŸš§ https://github.com/secfigo/Awesome-Fuzzing
A curated list of fuzzing resources ( Books, courses - free and paid, videos, tools, tutorials and vulnerable applications to practice on ) for learning Fuzzing and initial phases of Exploit Development like root cause analysis.

ğŸš§ https://github.com/google/fuzzing
This project aims at hosting tutorials, examples, discussions, research proposals, and other resources related toÂ [fuzzing](https://en.wikipedia.org/wiki/Fuzzing).

ğŸš§ https://github.com/google/oss-fuzz
ğŸ“‚ https://google.github.io/oss-fuzz/
To learn more about fuzzing in general, we recommend readingÂ [libFuzzer tutorial](https://github.com/google/fuzzing/blob/master/tutorial/libFuzzerTutorial.md)Â and the other docs inÂ [google/fuzzing](https://github.com/google/fuzzing/tree/master/docs)Â repository. These and some other resources are listed on theÂ [useful links](https://google.github.io/oss-fuzz/reference/useful-links/#tutorials)Â page.
- [libFuzzer documentation](https://llvm.org/docs/LibFuzzer.html)
- [libFuzzer tutorial](https://github.com/google/fuzzing/blob/master/tutorial/libFuzzerTutorial.md)
- [libFuzzer workshop](https://github.com/Dor1s/libfuzzer-workshop)
- [Structure-Aware Fuzzing with libFuzzer](https://github.com/google/fuzzer-test-suite/blob/master/tutorial/structure-aware-fuzzing.md)
- [Chromium Fuzzing Page](https://chromium.googlesource.com/chromium/src/testing/libfuzzer/)
- [Chromium Efficient Fuzzing Guide](https://chromium.googlesource.com/chromium/src/testing/libfuzzer/+/HEAD/efficient_fuzzing.md)
- [ClusterFuzz documentation](https://google.github.io/clusterfuzz/)
#### Survey Papers
ğŸ“„ ğŸ‘ Manes V J M, Han H S, Han C, et al. Fuzzing: Art, Science, and Engineering[J]. arXiv preprint arXiv:1812.00140, 2018.
https://arxiv.org/pdf/1812.00140.pdf
https://ieeexplore.ieee.org/document/8863940
https://doi.org/10.1109/TSE.2019.2946563

ğŸ“„ Li J, Zhao B, Zhang C. Fuzzing: a survey[J]. Cybersecurity, 2018, 1(1): 6.
https://link.springer.com/article/10.1186/s42400-018-0002-y
[https://doi.org/10.1186/s42400-018-0002-y](https://doi.org/10.1186/s42400-018-0002-y).

ğŸ“„ Schloegel, Moritz, et al. "Sok: Prudent evaluation practices for fuzzing."Â _2024 IEEE Symposium on Security and Privacy (SP)_. IEEE, 2024.

ğŸ“„ Mallissery, Sanoop, and Yu-Sung Wu. "Demystify the fuzzing methods: A comprehensive survey."Â _ACM Computing Surveys_Â 56.3 (2023): 1-38.

ğŸ“„ Liang H, Pei X, Jia X, et al. Fuzzing: State of the art[J]. IEEE Transactions on Reliability, 2018, 67(3): 1199-1218.
https://ieeexplore.ieee.org/document/8371326

ğŸ“„ Chen, Chen, Baojiang Cui, Jinxin Ma, Runpu Wu, Jianchao Guo, and Wenqian Liu. â€œA Systematic Review of Fuzzing Techniques.â€ _Computers & Security_ 75 (June 2018): 118â€“37. https://doi.org/10.1016/j.cose.2018.02.002

ğŸ“„ Godefroid, Patrice. â€œFuzzing: Hack, Art, and Science.â€ _Communications of the ACM_ 63, no. 2 (January 22, 2020): 70â€“76. https://doi.org/10.1145/3363824

ğŸ“„ Wang, Yan, Peng Jia, Luping Liu, Cheng Huang, and Zhonglin Liu. â€œA Systematic Review of Fuzzing Based on Machine Learning Techniques.â€ Edited by Tao Song. _PLOS ONE_ 15, no. 8 (August 18, 2020): e0237749. https://doi.org/10.1371/journal.pone.0237749

ğŸ“„ ğŸ‘ Huang, Linghan, Peizhou Zhao, Huaming Chen, and Lei Ma. "Large language models based fuzzing techniques: A survey."Â _arXiv preprint arXiv:2402.00350_Â (2024).
https://arxiv.org/abs/2402.00350

|                                  |                                                                                            |                                                          |          |           |
| -------------------------------- | ------------------------------------------------------------------------------------------ | -------------------------------------------------------- | -------- | --------- |
| **DOI**                          | **Title**                                                                                  | **Authors**                                              | **Year** | **Cited** |
| 10.1109/SP54263.2024.00137       | SoK: Prudent Evaluation Practices for Fuzzing                                              | Schloegel, Bars, Schiller, Bernhard, Scharnowski, et al. | 2024     | 6         |
| 10.1145/3623375                  | Demystify the Fuzzing Methods: A Comprehensive Survey                                      | S. Mallissery, Yu-Sung Wu                                | 2023     | 12        |
| 10.1186/S42400-018-0002-Y        | Fuzzing: a survey                                                                          | Jun Li, Bodong Zhao, Chaomo Zhang                        | 2018     | 235       |
| 10.1109/CECIT53797.2021.00035    | A systematic review of fuzzy testing for information systems and applications              | Shen, Wen, Zhang, Wang, Shen, Cheng                      | 2021     | 4         |
| 10.1007/S00500-023-09306-2       | A systematic review of fuzzing                                                             | Zhao, Qu, Jianliang Xu, Li,Â  Lv, Wang                    | 2023     | 5         |
| 10.1109/ISSSR61934.2024.00024    | A Comprehensive Review of Learning-based Fuzz Testing Techniques                           | Cheng, Li, Zhao, Li, Wong                                | 2024     | 0         |
| 10.1145/3512345                  | Fuzzing: A Survey for Roadmap                                                              | Zhu, Wen, Camtepe, Yang Xiang                            | 2022     | 139       |
| 10.1109/ACCESS.2023.3347652      | Machine Learning-Based Fuzz Testing Techniques: A Survey                                   | Zhang, Zhang, Xu, Wang,Â  Li                              | 2024     | 1         |
| 10.48550/ARXIV.2402.00350        | Large Language Models Based Fuzzing Techniques: A Survey                                   | Huang, Zhao, Chen, Ma                                    | 2024     | 8         |
| 10.1109/ICSIP61881.2024.10671554 | A Review of Fuzz Testing for Configuration-Sensitive Software                              | Chu, Huang, Li, Nie                                      | 2024     | 0         |
| 10.47857/IRJMS.2024.V05I04.01451 | A Systematic Review of AI Based Software Test Case Optimization                            | Padmanabhan                                              | 2024     | 0         |
| 10.1145/3243734.3243804          | Evaluating Fuzz Testing                                                                    | Klees, Ruef, Cooper, Wei, Hicks                          | 2018     | 673       |
| 10.1016/J.JSS.2022.111423        | A systematic literature review on benchmarks for evaluating debugging approaches           | Hirsch, Hofer                                            | 2022     | 10        |
| 10.1145/3649476.3658697          | The Fuzz Odyssey: A Survey on Hardware Fuzzing Frameworks for Hardware Design Verification | Saravanan, Dinakarrao                                    | 2024     | 0         |


### Trending Research
https://mp.weixin.qq.com/s/DL4pGH-7nPi3eSRD-rlD-w
å¤§æ¨¡å‹ä¸æ¨¡ç³Šæµ‹è¯•è¿›è¡Œç»“åˆçš„ç ”ç©¶è®ºæ–‡æ±‡æ€»ï½œæŠ€æœ¯è¿›å±•

https://mp.weixin.qq.com/s/Dn5ooUahCT7IqXIMevjVug
2024ä¿¡æ¯å®‰å…¨é¢†åŸŸå››å¤§é¡¶ä¼šFuzzè®ºæ–‡æ±‡æ€»ï½œæŠ€æœ¯è¿›å±•

https://mp.weixin.qq.com/s/-ZYX-G_jX1AN8lzbHvW21g
2024å¹´è½¯ä»¶å·¥ç¨‹é¡¶ä¼šFuzzè®ºæ–‡æ±‡æ€»

https://github.com/wcventure/FuzzingPaper
https://wcventure.github.io/FuzzingPaper/
1. This website is only used for collecting and grouping the related paper. If there are any paper need to be updated, you can contribute PR.
2. Please check the webÂ [https://wcventure.github.io/FuzzingPaper/](https://wcventure.github.io/FuzzingPaper/), as theÂ `.md`Â file shown in Github is cropped.
3. Advertisement:Â [Our ICTT (Guangzhou) research group](https://xidian-ictt-gz.github.io/)Â is accepting applications for masterâ€™s, doctoral, postdoctoral, and research assistant positions. We welcome hardworking, serious, and innovative young people who are interested in joining our research group.

https://github.com/juyongjiang/CodeLLMSurvey
The official GitHub page for the survey paper "A Survey on Large Language Models for Code Generation".
![](../../../../../../../../Assets/Pics/Pasted%20image%2020250412104958.png)

https://github.com/EdPuth/LLMs-based-Fuzzer-Survey
This repo list the core literature in the field of fuzzing test, large language model, and LLM-based fuzzer. Most of papers are selected from authoritative platform such as google scholar, and was published recently. It will be helpful for the researchers who wants to develop LLMs-based fuzzer. Feel free to send a pull request.


### Tools & Projects
ğŸš§ https://github.com/Microsvuln/Awesome-AFL
A curated list of different AFL forks and AFL inspired fuzzers with detailed equivalent academic papers including AFL-fuzzing tutorials

ğŸ  https://llvm.org/docs/LibFuzzer.html
LibFuzzer is an in-process, coverage-guided, evolutionary fuzzing engine.

LibFuzzer is linked with the library under test, and feeds fuzzed inputs to the library via a specific fuzzing entrypoint (aka â€œtarget functionâ€); the fuzzer then tracks which areas of the code are reached, and generates mutations on the corpus of input data in order to maximize the code coverage. The code coverage information for libFuzzer is provided by LLVMâ€™sÂ [SanitizerCoverage](https://clang.llvm.org/docs/SanitizerCoverage.html)Â instrumentation.

ğŸš§ https://github.com/google/AFL
ğŸ  https://lcamtuf.coredump.cx/afl/
american fuzzy lop - a security-oriented fuzzer
- ğŸš§ https://github.com/AFLplusplus/AFLplusplus
	- The fuzzer afl++ is afl with community patches, qemu 5.1 upgrade, collision-free coverage, enhanced laf-intel & redqueen, AFLfast++ power schedules, MOpt mutators, unicorn_mode, and a lot more!
- ğŸš§ https://gitee.com/opengauss/dbms-fuzzing-scu
	- DBMS-Fuzzing-SCU | å››å·å¤§å­¦è´¾é¹å›¢é˜Ÿæ ¹æ®AFLä¿®æ”¹çš„æ•°æ®åº“fuzzingè½¯ä»¶

ğŸš§ https://github.com/google/honggfuzz
A security oriented, feedback-driven, evolutionary, easy-to-use fuzzer with interesting analysis options. See theÂ [Usage document](https://github.com/google/honggfuzz/blob/master/docs/USAGE.md)Â for a primer on Honggfuzz use.

ğŸš§ https://github.com/google/oss-fuzz
ğŸ“‚ https://google.github.io/oss-fuzz/
In cooperation with theÂ [Core Infrastructure Initiative](https://www.coreinfrastructure.org/)Â and theÂ [OpenSSF](https://www.openssf.org/), OSS-Fuzz aims to make common open source software more secure and stable by combining modern fuzzing techniques with scalable, distributed execution. Projects that do not qualify for OSS-Fuzz (e.g. closed source) can run their own instances ofÂ [ClusterFuzz](https://github.com/google/clusterfuzz)Â orÂ [ClusterFuzzLite](https://google.github.io/clusterfuzzlite/).

We support theÂ [libFuzzer](https://llvm.org/docs/LibFuzzer.html),Â [AFL++](https://github.com/AFLplusplus/AFLplusplus), andÂ [Honggfuzz](https://github.com/google/honggfuzz)Â fuzzing engines in combination withÂ [Sanitizers](https://github.com/google/sanitizers), as well asÂ [ClusterFuzz](https://github.com/google/clusterfuzz), a distributed fuzzer execution environment and reporting tool.

Currently, OSS-Fuzz supports C/C++, Rust, Go, Python, Java/JVM, and JavaScript code. Other languages supported byÂ [LLVM](https://llvm.org/)Â may work too. OSS-Fuzz supports fuzzing x86_64 and i386 builds.



## Intro: Fuzz & Fuzzing Test
> ğŸ”— https://en.wikipedia.org/wiki/Fuzzing

In programming and software development, fuzzing or fuzz testing is an automated software testing technique that involves providing invalid, unexpected, or random data as inputs to a computer program. The program is then monitored for exceptions such as crashes, failing built-in code assertions, or potential memory leaks. Typically, fuzzers are used to test programs that take structured inputs. This structure is specified, e.g., in a file format or protocol and distinguishes valid from invalid input. An effective fuzzer generates semi-valid inputs that are "valid enough" in that they are not directly rejected by the parser, but do create unexpected behaviors deeper in the program and are "invalid enough" to expose corner cases that have not been properly dealt with.

For the purpose of security, input that crosses a trust boundary is often the most useful. For example, it is more important to fuzz code that handles the upload of a file by any user than it is to fuzz the code that parses a configuration file that is accessible only to a privileged user. 

> V. J. M. ManÃ¨s et al., "The Art, Science, and Engineering of Fuzzing: A Survey," in IEEE Transactions on Software Engineering, vol. 47, no. 11, pp. 2312-2331, 1 Nov. 2021, doi: 10.1109/TSE.2019.2946563. (2018)
> https://ieeexplore.ieee.org/document/8863940
> https://arxiv.org/pdf/1812.00140

The term â€œfuzzâ€ was originally coined by Miller et al. in 1990 to refer to a program that â€œgenerates a stream of random characters to be consumed by a target programâ€ [152,p. 4]. Since then, the concept of fuzz as well as its actionâ€”â€œfuzzingâ€â€”has appeared in a wide variety of contexts, including dynamic symbolic execution [90], [226], grammar-based test case generation [88], [105], [213], permission testing [24], [80], behavioral testing [122], [175], [224], complexity testing [135], [222], kernel testing [216], [196], [186], representation dependence testing [121], function detection [227], robustness evaluation [223], exploit development [111], GUI testing [197], signature generation [72], and penetration testing [81], [156]. To systematize the knowledge from the vast literature of fuzzing, let us first present a terminology of fuzzing extracted from modern uses.

> ğŸ”— https://itea.org/journals/volume-45-4/review-of-fuzz-testing-to-find-system-vulnerabilities/

Fundamentally, fuzzing shifts the focus from functional test cases (i.e., what systems must do) to include the effects of the non-functional factors (i.e., incidental characteristics of systems or invalid values or rarely used values). It does so in various systematic ways of mixing structured experimentation (i.e., design of experiment or combinatorial test designs) with randomization. Such randomization can be applied within runs to the uncontrolled aspects. Finally, it often systematically and concurrently explores variations in hardware and software. Put another way by Mallissery and Wu[10]:

> _â€˜Fuzzing is a vulnerability discovery solution that resonates with random-mutation, feedback-driven, coverage-guided, constraint-guided, seed-scheduling, and target-oriented strategies. â€¦ Most topline companies and organizations utilize fuzzing to ensure quality control and cybersecurity operations. For example, Google uses fuzzing to verify and ensure that the millions of Lines of Code (LOC) in Google Chrome are bug-free [67]. It was challenging to admit that Google could find 20K vulnerabilities in Chrome using fuzz testing [67]. The dominant software from Microsoft has to pass the fuzz test stage in the software development cycle to ensure no code vulnerabilities and confirm stability [136]. The DoD Enterprise DevSecOps Reference Design document [40] from the United States has mentioned that continuous testing across the software development cycle is necessary for the test tools support. Therefore, it is essential to use fuzzing to discover Distributed Denial of Service attacks and malware exploit possibilities, validate system security, and reduce the risk of system degradation [40].â€™ [pp. 71:1-2]_



### Why Fuzzing?
> ğŸ”— FuzzingæŠ€æœ¯æ€»ç»“ï¼ˆBrief Surveys on Fuzz Testingï¼‰ - wcventureçš„æ–‡ç«  - çŸ¥ä¹ https://zhuanlan.zhihu.com/p/43432370

ä¸ºä»€ä¹ˆä¼šæœ‰ä¸ç¡®å®šçš„æµ‹è¯•ç”¨ä¾‹ï¼Œæˆ‘æƒ³ä¸»è¦çš„åŸå› æ˜¯ä¸‹é¢å‡ ç‚¹ï¼š
1. æˆ‘ä»¬æ— æ³•ç©·ä¸¾æ‰€æœ‰çš„è¾“å…¥ä½œä¸ºæµ‹è¯•ç”¨ä¾‹ã€‚æˆ‘ä»¬ç¼–å†™æµ‹è¯•ç”¨ä¾‹çš„æ—¶å€™ï¼Œä¸€èˆ¬è€ƒè™‘æ­£å‘æµ‹è¯•ã€åå‘æµ‹è¯•ã€è¾¹ç•Œå€¼ã€è¶…é•¿ã€è¶…çŸ­ç­‰ä¸€äº›å¸¸è§çš„åœºæ™¯ï¼Œä½†æˆ‘ä»¬æ˜¯æ²¡æœ‰åŠæ³•æŠŠæ‰€æœ‰çš„è¾“å…¥éƒ½éå†è¿›è¡Œæµ‹è¯•çš„ã€‚
2. æˆ‘ä»¬æ— æ³•æƒ³åˆ°æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸åœºæ™¯ã€‚ç”±äºäººç±»è„‘åŠ›çš„é™åˆ¶ï¼Œæˆ‘ä»¬æ²¡æœ‰åŠæ³•æƒ³åˆ°æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸ç»„åˆï¼Œå°¤å…¶æ˜¯ç°åœ¨çš„è½¯ä»¶è¶Šæ¥è¶Šå¤šçš„ä¾èµ–æ“ä½œç³»ç»Ÿã€ä¸­é—´ä»¶ã€ç¬¬ä¸‰æ–¹ç»„ä»¶ï¼Œè¿™äº›ç³»ç»Ÿé‡Œçš„bugæˆ–è€…ç»„åˆåå½¢æˆçš„bugï¼Œæ˜¯æˆ‘ä»¬æŸä¸ªé¡¹ç›®ç»„çš„å¼€å‘äººå‘˜ã€æµ‹è¯•äººå‘˜æ— æ³•é¢„çŸ¥çš„ã€‚
3. Fuzzingè½¯ä»¶ä¹ŸåŒæ ·æ— æ³•éå†æ‰€æœ‰çš„å¼‚å¸¸åœºæ™¯ã€‚éšç€ç°åœ¨è½¯ä»¶è¶Šæ¥è¶Šå¤æ‚ï¼Œå¯é€‰çš„è¾“å…¥å¯ä»¥è®¤ä¸ºæœ‰æ— é™ä¸ªç»„åˆï¼Œæ‰€ä»¥å³ä½¿æ˜¯ä½¿ç”¨è½¯ä»¶æ¥éå†ä¹Ÿæ˜¯ä¸å¯èƒ½å®ç°çš„ï¼Œå¦åˆ™ä½ çš„ç‰ˆæœ¬å¯èƒ½å°±æ°¸è¿œä¹Ÿå‘å¸ƒä¸äº†ã€‚FuzzingæŠ€æœ¯æœ¬è´¨æ˜¯ä¾é éšæœºå‡½æ•°ç”Ÿæˆéšæœºæµ‹è¯•ç”¨ä¾‹æ¥è¿›è¡Œæµ‹è¯•éªŒè¯ï¼Œæ‰€ä»¥æ˜¯ä¸ç¡®å®šçš„ã€‚

è¿™äº›ä¸ç¡®å®šçš„æµ‹è¯•ç”¨ä¾‹ä¼šèµ·åˆ°æˆ‘ä»¬æƒ³è¦çš„æµ‹è¯•ç»“æœä¹ˆï¼Ÿèƒ½å‘ç°çœŸæ­£çš„Bugä¹ˆï¼Ÿ
1. FuzzingæŠ€æœ¯é¦–å…ˆæ˜¯ä¸€ç§è‡ªåŠ¨åŒ–æŠ€æœ¯ï¼Œå³è½¯ä»¶è‡ªåŠ¨æ‰§è¡Œç›¸å¯¹éšæœºçš„æµ‹è¯•ç”¨ä¾‹ã€‚å› ä¸ºæ˜¯ä¾é è®¡ç®—æœºè½¯ä»¶è‡ªåŠ¨æ‰§è¡Œï¼Œæ‰€ä»¥æµ‹è¯•æ•ˆç‡ç›¸å¯¹äººæ¥è®²è¿œè¿œé«˜å‡ºå‡ ä¸ªæ•°é‡çº§ã€‚æ¯”å¦‚ï¼Œä¸€ä¸ªä¼˜ç§€çš„æµ‹è¯•äººå‘˜ï¼Œä¸€å¤©èƒ½æ‰§è¡Œçš„æµ‹è¯•ç”¨ä¾‹æ•°é‡æœ€å¤šä¹Ÿå°±æ˜¯å‡ åä¸ªï¼Œå¾ˆéš¾è¾¾åˆ°100ä¸ªã€‚è€ŒFuzzingå·¥å…·å¯èƒ½å‡ åˆ†é’Ÿå°±å¯ä»¥è½»æ¾æ‰§è¡Œä¸Šç™¾ä¸ªæµ‹è¯•ç”¨ä¾‹ã€‚
2. FuzzingæŠ€æœ¯æœ¬è´¨æ˜¯ä¾èµ–éšæœºå‡½æ•°ç”Ÿæˆéšæœºæµ‹è¯•ç”¨ä¾‹ï¼Œéšæœºæ€§æ„å‘³ç€ä¸é‡å¤ã€ä¸å¯é¢„æµ‹ï¼Œå¯èƒ½æœ‰æ„æƒ³ä¸åˆ°çš„è¾“å…¥å’Œç»“æœã€‚
3. æ ¹æ®æ¦‚ç‡è®ºé‡Œé¢çš„â€œå¤§æ•°å®šå¾‹â€ï¼Œåªè¦æˆ‘ä»¬é‡å¤çš„æ¬¡æ•°å¤Ÿå¤šã€éšæœºæ€§å¤Ÿå¼ºï¼Œé‚£äº›æ¦‚ç‡æä½çš„å¶ç„¶äº‹ä»¶å°±å¿…ç„¶ä¼šå‡ºç°ã€‚FuzzingæŠ€æœ¯å°±æ˜¯å¤§æ•°å®šå¾‹çš„å…¸èŒƒåº”ç”¨ï¼Œè¶³å¤Ÿå¤šçš„æµ‹è¯•ç”¨ä¾‹å’Œéšæœºæ€§ï¼Œå°±å¯ä»¥è®©é‚£äº›éšè—çš„å¾ˆæ·±å¾ˆéš¾å‡ºç°çš„Bugæˆä¸ºå¿…ç„¶ç°è±¡ã€‚

ç›®å‰ï¼ŒFuzzingæŠ€æœ¯å·²ç»æ˜¯è½¯ä»¶æµ‹è¯•ã€æ¼æ´æŒ–æ˜é¢†åŸŸçš„æœ€æœ‰æ•ˆçš„æ‰‹æ®µä¹‹ä¸€ã€‚FuzzingæŠ€æœ¯ç‰¹åˆ«é€‚åˆç”¨äºå‘ç°0 Dayæ¼æ´ï¼Œä¹Ÿæ˜¯ä¼—å¤šé»‘å®¢æˆ–é»‘å¸½å­å‘ç°è½¯ä»¶æ¼æ´çš„é¦–é€‰æŠ€æœ¯ã€‚Fuzzingè™½ç„¶ä¸èƒ½ç›´æ¥è¾¾åˆ°å…¥ä¾µçš„æ•ˆæœï¼Œä½†æ˜¯Fuzzingéå¸¸å®¹æ˜“æ‰¾åˆ°è½¯ä»¶æˆ–ç³»ç»Ÿçš„æ¼æ´ï¼Œä»¥æ­¤ä¸ºçªç ´å£æ·±å…¥åˆ†æï¼Œå°±æ›´å®¹æ˜“æ‰¾åˆ°å…¥ä¾µè·¯å¾„ï¼Œè¿™å°±æ˜¯é»‘å®¢å–œæ¬¢FuzzingæŠ€æœ¯çš„åŸå› ã€‚


### Terminology â­
> V. J. M. ManÃ¨s et al., "The Art, Science, and Engineering of Fuzzing: A Survey," in IEEE Transactions on Software Engineering, vol. 47, no. 11, pp. 2312-2331, 1 Nov. 2021, doi: 10.1109/TSE.2019.2946563. (2018)
> https://ieeexplore.ieee.org/document/8863940
> https://arxiv.org/pdf/1812.00140

Intuitively, fuzzing is the action of running a **Program Under Test (PUT)** with â€œfuzz inputsâ€. Honoring Miller et al., we consider a fuzz input to be an input that the PUT may not be expecting, i.e., an input that the PUT may process incorrectly and trigger a behavior that was unintended by the PUT developer. To capture this idea, we define the term fuzzing
as follows.

==**Definition 1 (Fuzzing)**==
Fuzzing is the execution of the PUT using input(s) sampled from an input space (the â€œfuzz input spaceâ€) that protrudes the expected input space of the PUT. 

Three remarks are in order. First, although it may be common to see the fuzz input space to contain the expected input space, this is not necessaryâ€”it suffices for the former to contain an input not in the latter. Second, in practice fuzzing almost surely runs for many iterations; thus writing â€œrepeated executionsâ€ above would still be largely accurate. Third, the sampling process is not necessarily randomized, as we will see in Â§5.

> Fuzz testing is a form of software testing technique that utilizes fuzzing. To differentiate it from others and to honor what we consider to be its most prominent purpose, we deem it to have a specific goal of finding security-related bugs, which include program crashes. In addition, we also define fuzzer and fuzz campaign, both of which are common terms in fuzz testing:

==**Definition 2 (Fuzz Testing)**==
Fuzz testing is the use of fuzzing to test if a PUT violates a security policy.

==**Definition 3 (Fuzzer)**==
A fuzzer is a program that performs fuzz testing on a PUT.

==**Definition 4 (Fuzz Campaign)**==
A fuzz campaign is a specific execution of a fuzzer on a PUT with a specific security
policy. 

The goal of running a PUT through a fuzzing campaign is to find bugs [26] that violate the specified security policy . For example, a security policy employed by early fuzzers tested only whether a generated inputâ€”the test caseâ€”crashed the PUT. However, fuzz testing can actually be used to test any security policy observable from an execution, i.e., EM-enforceable [183]. The specific mechanism that decides whether an execution violates the security policy is called the bug oracle.

==**Definition 5 (Bug Oracle)**==
A bug oracle is a program, perhaps as part of a fuzzer, that determines whether a given execution of the PUT violates a specific security policy. 

We refer to the algorithm implemented by a fuzzer simply as its â€œfuzz algorithmâ€. Almost all fuzz algorithms depend on some parameters beyond (the path to) the PUT. Each concrete setting of the parameters is a fuzz configuration:

==**Definition 6 (Fuzz Configuration)**==
A fuzz configuration of a fuzz algorithm comprises the parameter value(s) that control(s) the fuzz algorithm.

The definition of a fuzz configuration is intended to be broad. Note that the type of values in a fuzz configuration depend on the type of the fuzz algorithm. For example, a fuzz algorithm that sends streams of random bytes to the PUT [152] has a simple configuration space {(PUT)}. On the other hand, sophisticated fuzzers contain algorithms that accept a set of configurations and evolve the set over timeâ€”this includes adding and removing configurations. For example, CERT BFF [49] varies both the mutation ratio and the seed over the course of a campaign, and thus its configuration space is {(PUT, s1, r1), (PUT, s2, r2), . . .}. A seed is a (commonly well-structured) input to the PUT, used to generate test cases by modifying it. Fuzzers typically maintain a collection of seeds, and some fuzzers evolve the collection as the fuzz campaign progresses. This collection is called a seed pool. Finally, a fuzzer is able to store some data  within each configuration. For instance, coverage-guided fuzzers may store the attained coverage in each configuration.


### Taxonomy
> ğŸ”— https://en.wikipedia.org/wiki/Fuzzing

A fuzzer can be categorized in several ways:
1. (Reuse of existing input seeds) A fuzzer can be generation-based or mutation-based depending on whether inputs are generated from scratch or by modifying existing inputs.
2. (Aware of input structure) A fuzzer can be dumb (unstructured) or smart (structured) depending on whether it is aware of input structure.
3. (Aware of program structure) A fuzzer can be white-, grey-, or black-box, depending on whether it is aware of program structure.

> ğŸ”— https://itea.org/journals/volume-45-4/review-of-fuzz-testing-to-find-system-vulnerabilities/

Fuzzing is defined by Microsoft as â€˜a program analysis technique that looks for inputs causing error conditions that have a high chance of being exploitable, such as buffer overflows, memory access violations and null pointer dereferences.â€™ They go on to characterize fuzz testing as being in the following three categories:
- Blackbox fuzzers, also called â€œdumb fuzzers,â€ rely solely on the sample input files to generate new inputs.
- Whitebox fuzzers analyze the target program either statically or dynamically to guide the search for new inputs aimed at exploring as many code paths as possible.
- Greybox fuzzers, just like blackbox fuzzers, donâ€™t have any knowledge of the structure of the target program, but make use of a feedback loop to guide their search based on observed behavior from previous executions of the program.

**Table 1: Comparison of fuzz test types**

|                          |                                               |                                              |                                      |
| ------------------------ | --------------------------------------------- | -------------------------------------------- | ------------------------------------ |
|                          | **Black-box (BB)**                            | **Grey-box (GB)**                            | **White-box (WB)**                   |
| **Goal**                 | Mimic external cyber-attack                   | Mimic insider threats                        | Mimic threats with privileged access |
| **Access & Information** | Zero                                          | Some                                         | Complete                             |
| **Advantage**            | Realistic start                               | More efficient than BB                       | More comprehensive than GB           |
| **Disadvantage**         | May miss vulnerabilities & resource-intensive | Limited assessment of penetration resistance | Requires extensive release at cost   |


### Genealogy
![](../../../../../../../../Assets/Pics/Screenshot%202025-03-05%20at%2015.52.12.png)
<small>Genealogy tracing significant fuzzersâ€™ lineage back to Miller et al.â€™s seminal work. Each node in the same row represents a set of fuzzers appeared in the same year. A solid arrow from X to Y indicates that Y cites, references, or otherwise uses techniques from X. ğŸ“— denotes that a paper describing the work was published.</small>
<small>V. J. M. ManÃ¨s et al., "The Art, Science, and Engineering of Fuzzing: A Survey," in IEEE Transactions on Software Engineering, vol. 47, no. 11, pp. 2312-2331, 1 Nov. 2021, doi: 10.1109/TSE.2019.2946563. <a>https://ieeexplore.ieee.org/document/8863940</a></small>


### ğŸ“‹ List of Fuzzers
#### 1ï¸âƒ£ Traditional Fuzzers
![](../../../../../../../../Assets/Pics/Screenshot%202025-03-05%20at%2015.55.03.png)
<small>V. J. M. ManÃ¨s et al., "The Art, Science, and Engineering of Fuzzing: A Survey," in IEEE Transactions on Software Engineering, vol. 47, no. 11, pp. 2312-2331, 1 Nov. 2021, doi: 10.1109/TSE.2019.2946563. <a>https://ieeexplore.ieee.org/document/8863940</a> (2018)</small>
#### 2ï¸âƒ£ Traditional Machine Learning & Neural Network Based Fuzzers


#### 3ï¸âƒ£ LLM Based Fuzzers
![](../../../../../../../../Assets/Pics/Screenshot%202025-03-05%20at%2021.11.11.png)
<small>Huang, Linghan, Peizhou Zhao, Huaming Chen, and Lei Ma. "Large language models based fuzzing techniques: A survey."Â arXiv preprint arXiv:2402.00350 (2024).
<a>https://arxiv.org/abs/2402.00350</a></small>

![](../../../../../../../../Assets/Pics/Screenshot%202025-04-11%20at%2016.19.05.png)
<small>Huang, Linghan, Peizhou Zhao, Huaming Chen, and Lei Ma. "Large language models based fuzzing techniques: A survey."Â arXiv preprint arXiv:2402.00350 (2024).
<a>https://arxiv.org/abs/2402.00350</a></small>

![](../../../../../../../../Assets/Pics/Screenshot%202025-04-12%20at%2010.33.38.png)
![](../../../../../../../../Assets/Pics/Screenshot%202025-04-12%20at%2010.35.25.png)
<small>LiÂ Y,Â YangÂ WZ,Â ZhangÂ Y,Â XueÂ YX.Â SurveyÂ onÂ FuzzingÂ BasedÂ onÂ LargeÂ LanguageÂ Model.Â RuanÂ JianÂ XueÂ Bao/Journal ofÂ SoftwareÂ (inÂ Chinese).Â <a>http://www.jos.org.cn/1000-9825/7323.htm</a></small>



## Fuzzing Algorithm & General Working Procedure
### General Fuzzing Procedure
![](../../../../../../../../Assets/Pics/Screenshot%202025-04-11%20at%2020.17.58.png)
<small>å¦‚å›¾æ‰€ç¤º,Â æ¨¡ç³Šæµ‹è¯•çš„ä¸€èˆ¬å·¥ä½œæµç¨‹å¯åˆ†ä¸º5ä¸ªåŸºæœ¬æ­¥éª¤,Â å³é¢„å¤„ç†ã€æµ‹è¯•è¾“å…¥ç”Ÿæˆã€æµ‹è¯•æ‰§è¡Œã€ç¼ºé™·æ£€ æµ‹å’Œåæ¨¡ç³Šå¤„ç†. <a>LiÂ Y,Â YangÂ WZ,Â ZhangÂ Y,Â XueÂ YX.Â SurveyÂ onÂ FuzzingÂ BasedÂ onÂ LargeÂ LanguageÂ Model.Â RuanÂ JianÂ XueÂ Bao/Journal ofÂ SoftwareÂ (inÂ Chinese).Â http://www.jos.org.cn/1000-9825/7323.htm</a></small>

> ğŸ”— https://itea.org/journals/volume-45-4/review-of-fuzz-testing-to-find-system-vulnerabilities/

Different fuzzing engines employ different methods to look for vulnerabilities and there is a growing repertoire of types and names depending on their target systems and what languages they operate in. According to Mallissery and Wu [10] there are six fundamental steps in a fuzzing engine as illustrated in Figure 1:

1. System readiness is the primary step toward fuzzing (supplying initial seeds, setting iterations, memory allocation, etc.).
2. Automate the fuzzing process and instrument wherever required.
3. Generated test cases will take to the target software for performing an execution using the delivered test case.[sic6Â ]
4. Continue with new program states or check for potential crashes in the software while following the execution path.
5. Report all the fuzz progress (bugs/crashes/hangs/paths/edges) to the fuzz engine and the user.
6. User involvement/backtracking is optional to improve fuzzing and explore all the unvisited program control flow paths.

![](../../../../../../../../Assets/Pics/Pasted%20image%2020250419122414.png)


> V. J. M. ManÃ¨s et al., "The Art, Science, and Engineering of Fuzzing: A Survey," in IEEE Transactions on Software Engineering, vol. 47, no. 11, pp. 2312-2331, 1 Nov. 2021, doi: 10.1109/TSE.2019.2946563. (2018)
> https://ieeexplore.ieee.org/document/8863940
> https://arxiv.org/pdf/1812.00140

![](../../../../../../../../Assets/Pics/Screenshot%202025-03-05%20at%2019.30.41.png)

We present a generic algorithm for fuzz testing, Algorithm 1, which we imagine to have been implemented in a model fuzzer. It is general enough to accommodate existing fuzzing techniques, including black-, grey-, and white-box fuzzing as defined in Â§2.4. Algorithm 1 takes a set of fuzz configurations $\mathbb{C}$ and a timeout $t_{limit}$ as input, and outputs a set of discovered bugs $\mathbb{B}$. It consists of two parts. The first part is the `PREPROCESS` function, which is executed at the beginning of a fuzz campaign. The second part is a series of five functions inside a loop: `SCHEDULE`, `INPUTGEN`, `INPUTEVAL`, `CONFUPDATE`, and `CONTINUE`. Each execution of this loop is called a fuzz iteration and each time `INPUTEVAL` executes the PUT on a test case is called a fuzz run. Note that some fuzzers do not implement all five functions. For example, to model Radamsa [102], which never updates the set of fuzz configurations, `CONFUPDATE` always returns the current set of configurations unchanged.

**`PREPROCESS` ($\mathbb{C}$) â†’ $\mathbb{C}$**
A user supplies `PREPROCESS` with a set of fuzz configurations as input, and it returns a potentially-modified set of fuzz configurations. Depending on the fuzz algorithm, PREPROCESS may perform a variety of actions such as inserting instrumentation code to PUTs, or measuring the execution speed of seed files. See Â§3.

**`SCHEDULE` ($\mathbb{C}$, $t_{elapsed}$, $t_{limit}$) â†’ $conf$**
`SCHEDULE` takes in the current set of fuzz configurations, the current time $t_{elapsed}$, and a timeout $t_{limit}$ as input, and selects a fuzz configuration to be used for the current fuzz iteration. See Â§4.

**`INPUTGEN` ($conf$) â†’ $tcs$**
`INPUTGEN` takes a fuzz configuration as input and returns a set of concrete test cases $tcs$ as output. When generating test cases, `INPUTGEN` uses specific parameter(s) in $conf$. Some fuzzers use a seed in $conf$ for generating test cases, while others use a model or grammar as a parameter. See Â§5.

**`INPUTEVAL` (`conf`, $tcs$, $O_{bug}$) â†’ $\mathbb{B}$ , $execinfos$**
`INPUTEVAL` takes a fuzz configuration conf, a set of test cases $tcs$, and a bug oracle $O_{bug}$ as input. It executes the PUT on $tcs$ and checks if the executions violate the security policy using the bug oracle $O_{bug}$. It then outputs the set of bugs found $\mathbb{B}$ and information about each of the fuzz runs $execinfos$, which may be used to update the fuzz configurations. We assume $O_{bug}$ is embedded in our model fuzzer. See Â§6.

**`CONFUPDATE` ($\mathbb{C}$, $conf$, $execinfos$) â†’ $\mathbb{C}$**
`CONFUPDATE` takes a set of fuzz configurations $\mathbb{C}$, the current configuration conf, and the information about each of the fuzz runs $execinfos$, as input. It may update the set of fuzz configurations $\mathbb{C}$. For example, many grey-box fuzzers reduce the number of fuzz configurations in $\mathbb{C}$ based on $execinfos$. See Â§7.

**`CONTINUE` ($\mathbb{C}$) â†’ {$True$, $False$}**
`CONTINUE` takes a set of fuzz configurations $\mathbb{C}$ as input and outputs a boolean indicating whether a new fuzz iteration should occur. This function is useful to model white-box fuzzers that can terminate when there are no more paths to discover.


### Hybrid Fuzzing
> ğŸ”— https://itea.org/journals/volume-45-4/review-of-fuzz-testing-to-find-system-vulnerabilities/

Hybrid fuzzing merges fuzzing, assessed as â€˜concreteâ€™ because it is actual test, and â€˜symbolicâ€™ execution, which inculcates code-based paths, where the portmanteau of concrete and symbolic is abbreviated to â€˜concolic.â€™ These two types of techniques are complementary and concolic is symbolic with hybrid fuzzing. One of the limits of earlier fuzzing is that the generational or mutation algorithms can constrain solutions to previously detected vulnerabilities, whereas symbolic logic execution combined with fuzzing can force unusual paths to discover additional vulnerabilities. An illustration of hybrid fuzzing is Figure 4 (below).

![](../../../../../../../../Assets/Pics/Pasted%20image%2020250419123223.png)

From the presentation Senator and Allen [4], a fuzzing engine known as â€˜Drillerâ€™ was used from around 2016 to 2019 to escape compartments and access code used infrequently (i.e., Zhang, et al. [15, p. 4-16]). Major developments since 2020 that were highlighted in the ITEA workshop were â€˜Fuzzolicâ€™ [16], â€˜Paint Aware Taint Analysis (PATA)â€™ [17] and Speedy-Automatic-Vulnerability-Incentivized Oracle (SAVIOR) [18].

...


### LLM-based Fuzzing Procedure
![](../../../../../../../../Assets/Pics/Screenshot%202025-04-11%20at%2020.42.35.png)
<small>LLMé©±åŠ¨ç¼ºé™·æ£€æµ‹çš„ä¸€èˆ¬æµç¨‹ <a>LiÂ Y,Â YangÂ WZ,Â ZhangÂ Y,Â XueÂ YX.Â SurveyÂ onÂ FuzzingÂ BasedÂ onÂ LargeÂ LanguageÂ Model.Â RuanÂ JianÂ XueÂ Bao/Journal ofÂ SoftwareÂ (inÂ Chinese).Â http://www.jos.org.cn/1000-9825/7323.htm</a></small>
#### LLM-Driven Test Cases Generation

#### LLM-Driven Bug/Vulnerability Discovery

#### LLM-Driven Post-Fuzzing Processing


## Fuzzer Evaluation & Metrics
### Common Used Metrics
> Huang, Linghan, Peizhou Zhao, Huaming Chen, and Lei Ma. "Large language models based fuzzing techniques: A survey."Â _arXiv preprint arXiv:2402.00350_Â (2024).
> https://arxiv.org/abs/2402.00350

In addition, we summarize the most commonly used metrics based on all technologies to evaluate the performance of LLMs-based fuzzers. They can be generally categorized into three types, the metrics related to code, performance and time. 

For the ==code-related metrics==, the representative ones are code coverage and the number of bugs retrieved, as they reflect the testing coverage and vulnerability detection capability of the fuzzer most directly. 
- **Number of bugs retrieved**
- **Code coverage**
- **Number of APIs covered**
- **Number of valid programs generated**

For the ==performance-related metrics==, the hit rate is a commonly used metric. It refers to the efficiency of inputs generated by the fuzzer in targeting the test objectives. The mutation effectiveness is closely related to the mutation score and serves as a standard for measuring the quality of seed mutation. It is worth mentioning that some techniques also use F1 Score as a metric. It is calculated using precision and recall and ranges from 0 to 1, with a higher value indicating better performance in terms of both precision and recall. 
- **Number of unique crashes**
- **Hit rate**
- **Accuracy**
- **Recall**
- **F1 score**
- **Mutation Effectiveness**

 For the ==time-related metrics==, the 1) execution time represents the speed at which the fuzzer runs. On the other hand, 2) average detection time is a standard measure used to gauge the fuzzerâ€™s ability to find vulnerabilities in the test objectives.
- **Execution time**
- **Average bug detection time**

> Sanoop Mallissery and Yu-Sung Wu. Demystify the fuzzing methods: A comprehensive survey. ACM Computing Surveys, 56(3):1â€“38, 2023.

[Mallissery and Wu, 2023] present a discussion of the criteria that a good fuzzer should meet, including:
1. Able to detect all vulnerabilities of the test targets 
2. Performing in-depth analysis of multiple targets when detecting code-level vulnerabilities from the interaction between multiple targets 
3. A fuzzer should identify different types of bugs
#### Code-related Metrics
##### Code Coverage
> ğŸ”— https://www.cnblogs.com/tomyyyyy/articles/13608791.html

ä»£ç è¦†ç›–ç‡æ˜¯æ¨¡ç³Šæµ‹è¯•ä¸­ä¸€ä¸ªæå…¶é‡è¦çš„æ¦‚å¿µï¼Œ**ä½¿ç”¨ä»£ç è¦†ç›–ç‡å¯ä»¥è¯„ä¼°å’Œæ”¹è¿›æµ‹è¯•è¿‡ç¨‹ï¼Œæ‰§è¡Œåˆ°çš„ä»£ç è¶Šå¤šï¼Œæ‰¾åˆ°bugçš„å¯èƒ½æ€§å°±è¶Šå¤§**ï¼Œæ¯•ç«Ÿï¼Œåœ¨è¦†ç›–çš„ä»£ç ä¸­å¹¶ä¸èƒ½100%å‘ç°bugï¼Œåœ¨æœªè¦†ç›–çš„ä»£ç ä¸­å´æ˜¯100%æ‰¾ä¸åˆ°ä»»ä½•bugçš„ï¼Œæ‰€ä»¥æœ¬èŠ‚ä¸­å°±å°†è¯¦ç»†ä»‹ç»ä»£ç è¦†ç›–ç‡çš„ç›¸å…³æ¦‚å¿µã€‚

**ä»£ç è¦†ç›–ç‡ï¼ˆCode Coverageï¼‰**
ä»£ç è¦†ç›–ç‡æ˜¯ä¸€ç§åº¦é‡ä»£ç çš„è¦†ç›–ç¨‹åº¦çš„æ–¹å¼ï¼Œä¹Ÿå°±æ˜¯æŒ‡æºä»£ç ä¸­çš„æŸè¡Œä»£ç æ˜¯å¦å·²æ‰§è¡Œï¼›å¯¹äºŒè¿›åˆ¶ç¨‹åºï¼Œè¿˜å¯å°†æ­¤æ¦‚å¿µç†è§£ä¸ºæ±‡ç¼–ä»£ç ä¸­çš„æŸæ¡æŒ‡ä»¤æ˜¯å¦å·²æ‰§è¡Œã€‚å…¶è®¡é‡æ–¹å¼å¾ˆå¤šï¼Œä½†æ— è®ºæ˜¯GCCçš„GCOVè¿˜æ˜¯LLVMçš„SanitizerCoverageï¼Œéƒ½æä¾›å‡½æ•°ï¼ˆfunctionï¼‰ã€åŸºæœ¬å—ï¼ˆbasic-blockï¼‰ã€è¾¹ç•Œï¼ˆedgeï¼‰ä¸‰ç§çº§åˆ«çš„è¦†ç›–ç‡æ£€æµ‹ï¼Œæ›´å…·ä½“çš„ç»†èŠ‚å¯ä»¥å‚è€ƒLLVMçš„[å®˜æ–¹æ–‡æ¡£](https://clang.llvm.org/docs/SanitizerCoverage.html)ã€‚

**åŸºæœ¬å—ï¼ˆBasic Blockï¼‰**
ç¼©å†™ä¸ºBBï¼ŒæŒ‡ä¸€ç»„é¡ºåºæ‰§è¡Œçš„æŒ‡ä»¤ï¼ŒBBä¸­ç¬¬ä¸€æ¡æŒ‡ä»¤è¢«æ‰§è¡Œåï¼Œåç»­çš„æŒ‡ä»¤ä¹Ÿä¼šè¢«å…¨éƒ¨æ‰§è¡Œï¼Œæ¯ä¸ªBBä¸­æ‰€æœ‰æŒ‡ä»¤çš„æ‰§è¡Œæ¬¡æ•°æ˜¯ç›¸åŒçš„ï¼Œä¹Ÿå°±æ˜¯è¯´ä¸€ä¸ªBBå¿…é¡»æ»¡è¶³ä»¥ä¸‹ç‰¹å¾ï¼š
- â€‹ åªæœ‰ä¸€ä¸ªå…¥å£ç‚¹ï¼ŒBBä¸­çš„æŒ‡ä»¤ä¸æ˜¯ä»»ä½•**è·³è½¬æŒ‡ä»¤**çš„ç›®æ ‡ã€‚
- åªæœ‰ä¸€ä¸ªé€€å‡ºç‚¹ï¼Œåªæœ‰æœ€åä¸€æ¡æŒ‡ä»¤ä½¿æ‰§è¡Œæµç¨‹è½¬ç§»åˆ°å¦ä¸€ä¸ªBB

**è¾¹ï¼ˆedgeï¼‰**
AFLçš„æŠ€æœ¯ç™½çš®ä¹¦ä¸­æåˆ°fuzzeré€šè¿‡æ’æ¡©ä»£ç æ•è·è¾¹ï¼ˆedgeï¼‰è¦†ç›–ç‡ã€‚é‚£ä¹ˆä»€ä¹ˆæ˜¯edgeå‘¢ï¼Ÿæˆ‘ä»¬å¯ä»¥å°†ç¨‹åºçœ‹æˆä¸€ä¸ªæ§åˆ¶æµå›¾ï¼ˆCFGï¼‰ï¼Œå›¾çš„æ¯ä¸ªèŠ‚ç‚¹è¡¨ç¤ºä¸€ä¸ªåŸºæœ¬å—ï¼Œè€Œedgeå°±è¢«ç”¨æ¥è¡¨ç¤ºåœ¨åŸºæœ¬å—ä¹‹é—´çš„è½¬è·³ã€‚çŸ¥é“äº†æ¯ä¸ªåŸºæœ¬å—å’Œè·³è½¬çš„æ‰§è¡Œæ¬¡æ•°ï¼Œå°±å¯ä»¥çŸ¥é“ç¨‹åºä¸­çš„æ¯ä¸ªè¯­å¥å’Œåˆ†æ”¯çš„æ‰§è¡Œæ¬¡æ•°ï¼Œä»è€Œè·å¾—æ¯”è®°å½•BBæ›´ç»†ç²’åº¦çš„è¦†ç›–ç‡ä¿¡æ¯ã€‚

**å…ƒç»„ï¼ˆtupleï¼‰**
å…·ä½“åˆ°AFLçš„å®ç°ä¸­ï¼Œä½¿ç”¨äºŒå…ƒç»„(branch_src, branch_dst)æ¥è®°å½•**å½“å‰åŸºæœ¬å—**Â +Â **å‰ä¸€åŸºæœ¬å—**Â çš„ä¿¡æ¯ï¼Œä»è€Œè·å–ç›®æ ‡çš„æ‰§è¡Œæµç¨‹å’Œä»£ç è¦†ç›–æƒ…å†µï¼Œä¼ªä»£ç å¦‚ä¸‹ï¼š

``` 
cur_location = <COMPILE_TIME_RANDOM>;//ç”¨ä¸€ä¸ªéšæœºæ•°æ ‡è®°å½“å‰åŸºæœ¬å—
shared_mem[cur_location ^ prev_location]++;//å°†å½“å‰å—å’Œå‰ä¸€å—å¼‚æˆ–ä¿å­˜åˆ°shared_mem[]
prev_location = cur_location >> 1;//cur_locationå³ç§»1ä½åŒºåˆ†ä»å½“å‰å—åˆ°å½“å‰å—çš„è½¬è·³
```

å®é™…æ’å…¥çš„æ±‡ç¼–ä»£ç ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œé¦–å…ˆä¿å­˜å„ç§å¯„å­˜å™¨çš„å€¼å¹¶è®¾ç½®ecx/rcxï¼Œç„¶åè°ƒç”¨`__afl_maybe_log`ï¼Œè¿™ä¸ªæ–¹æ³•çš„å†…å®¹ç›¸å½“å¤æ‚ï¼Œè¿™é‡Œå°±ä¸å±•å¼€è®²äº†ï¼Œä½†å…¶ä¸»è¦åŠŸèƒ½å°±å’Œä¸Šé¢çš„ä¼ªä»£ç ç›¸ä¼¼ï¼Œç”¨äºè®°å½•è¦†ç›–ç‡ï¼Œæ”¾å…¥ä¸€å—å…±äº«å†…å­˜ä¸­ã€‚

#### Performance-related Metrics

#### Time-related Metrics


### Validity of Fuzzer Evaluation Metrics
> George Klees, Andrew Ruef, Benji Cooper, Shiyi Wei, and Michael Hicks. 2018. Evaluating Fuzz Testing. In Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security (CCS '18). Association for Computing Machinery, New York, NY, USA, 2123â€“2138. https://doi.org/10.1145/3243734.3243804

Fuzz testing is a promising technology that has been used to uncover many important bugs and security vulnerabilities. This promise has prompted a growing number of researchers to develop new fuzz testing algorithms. The evidence that such algorithms work is primarily experimental, so it is important that it comes from a well-founded experimental methodology.

...

In this paper we propose some clear guidelines to which future papersâ€™ evaluations should adhere. In particular, researchers should perform multiple trials and use statistical tests (Section 4); they should evaluate different seeds (Section 5), and should consider longer (â‰¥24 hour vs. 5 hour) timeouts (Section 6); and they should evaluate bug-finding performance using ground truth rather than heuristics such as â€œunique crashesâ€ (Section 7). Finally, we argue for the establishment and adoption of a good fuzzing benchmark, and sketch what it might look like. The practice of hand selecting a few particular targets, and varying them from paper to paper, is problematic (Section 8). A well-designed and agreed-upon benchmark would address this problem. We also identify other problems that our results suggest are worth studying, including the establishment of better de-duplication heuristics (a topic of recent interest [42, 51]), and the use of algorithmic ideas from related areas, such as SAT solving.

...

![](../../../../../../../../Assets/Pics/Screenshot%202025-03-05%20at%2019.15.01.png)

In this paper, we surveyed 32 recent papers and analyzed their experimental methodologies. We found that no paper completely follows the methodology we have outlined above. Moreover, results of experiments we carried out using AFLFast [6] (as A) and AFL [1] (as B) illustrate why not following this methodology can lead to misleading or weakened conclusions. We found that 
- Most papers failed to perform multiple runs, and those that did failed to account for varying performance by using a statistical test. This is a problem because our experiments showed that run-to-run performance can vary substantially.
- Many papers measured fuzzer performance not by counting distinct bugs, but instead by counting â€œunique crashesâ€ using heuristics such as AFLâ€™s coverage measure and stack hashes. This is a problem because experiments we carried out showed that the heuristics can dramatically over-count the number of bugs, and indeed may suppress bugs by wrongly grouping crashing inputs. This means that apparent improvements may be modest or illusory. Many papers made some consideration of root causes, but often as a â€œcase studyâ€ rather than a performance assessment.
- Many papers used short timeouts, without justification. Our experiments showed that longer timeouts may be needed to paint a complete picture of an algorithmâ€™s performance.
- Many papers did not carefully consider the impact of seed choices on algorithmic improvements. Our experiments showed that performance can vary substantially depending on what seeds are used. In particular, two different non-empty inputs need not produce similar performance, and the empty seed can work better than one might expect.
- Papers varied widely on their choice of target programs. A growing number are using synthetic suites CGC and/or LAVA-M, which have the advantage that they are defined independently of a given algorithm, and bugs found by fuzzing them can be reliably counted (no crash de-duplication strategy is needed). Other papers often picked small, disjoint sets of programs, making it difficult to compare results across papers. Our experiments showed AFLFast performs well on the targets it was originally assessed against, but performed no better than AFL on two targets used by other papers.

Ultimately, our experiments roughly matched the positive results of the original AFLFast paper [6], but by expanding the scope of the evaluation to different seeds, longer timeouts, and different target programs, evidence of AFLFastâ€™s superiority, at least for the versions we tested, was weakened. The fact that heuristic crash deduplication strategies are of questionable value further weakens our confidence in claims of improvement. We believe the same could be said of many prior papersâ€”all suffer from problems in their evaluation to some degree. As such, a key conclusion of this paper is that the fuzzing community needs to start carrying out more rigorous experiments in order to draw more reliable conclusions.

Specifically, we recommend that fuzz testing evaluations should have the following elements:
- multiple trials with statistical tests to distinguish distributions;
- a range of benchmark target programs with known bugs (e.g.,LAVA-M, CGC, or old programs with bug fixes);
- measurement of performance in terms of known bugs, rather than heuristics based on AFL coverage profiles or stack hashes; block or edge coverage can be used as a secondary measure;
- a consideration of various (well documented) seed choices including empty seed;
- timeouts of at least 24 hours, or else justification for less, with performance plotted over time.

We see (at least) three important lines of future work. First, we believe there is a pressing need for well-designed, well-assessed benchmark suite, as described at the end of the last section. Second, and related, it would be worthwhile to carry out a larger study of the value of crash de-duplication methods on the results of realistic fuzzing runs, and potentially develop new methods that work better, for assisting with triage and assessing fuzzing when ground truth is not known. Recent work shows promise [42, 51]. Finally, it would be interesting to explore enhancements to the fuzzing algorithm inspired by the observation that no single fuzzing run found all true bugs in cxxfilt; ideas from other search algorithms, like SAT solving â€œrebootsâ€ [46], might be brought to bear.


### Fuzzing Test Benchmarks
> ğŸ”— https://itea.org/journals/volume-45-4/review-of-fuzz-testing-to-find-system-vulnerabilities/

**Magma Benchmark.**Â Hazimeh, et al. [25] (2020) summarize the Magma benchmark for fuzzing methods as follows:

> _High scalability and low running costs have made fuzz testing the de facto standard for discovering software bugs. Fuzzing techniques are constantly being improved in a race to build the ultimate bug-finding tool. However, while fuzzing excels at finding bugs in the wild, evaluating and comparing fuzzer performance is challenging due to the lack of metrics and benchmarks. For example, crash countâ€”perhaps the most commonly used performance metricâ€”is inaccurate due to imperfections in deduplication techniques. â€¦ We tackle these problems by developing Magma, a ground-truth fuzzing benchmark that enables uniform fuzzer evaluation and comparison. By introducing real bugs into real software, Magma allows for the realistic evaluation of fuzzers against a broad set of targets. By instrumenting these bugs, Magma also enables the collection of bug-centric performance metrics independent of the fuzzer. â€¦ Based on the number of bugs reached, triggered, and detected, we draw conclusions about the fuzzersâ€™ exploration and detection capabilities â€¦ highlighting the importance of ground truth in performing more accurate and meaningful evaluations._

**Unifuzz Test Bench.**Â Li, et al. [26] (2020) summarize the Unifuzz test bench effort as follows:

> _A flurry of fuzzing tools (fuzzers) have been proposed in the literature, aiming at detecting software vulnerabilities effectively and efficiently. To date, it is however still challenging to compare fuzzers due to the inconsistency of the benchmarks, performance metrics, and/or environments for evaluation, which buries the useful insights and thus impedes the discovery of promising fuzzing primitives. In this paper, we design and develop UNIFUZZ, an open-source and metrics-driven platform for assessing fuzzers in a comprehensive and quantitative manner. Specifically, UNIFUZZ to date has incorporated 35 usable fuzzers, a benchmark of 20 real-world programs, and six categories of performance metrics._

**Fuzzbench.**Â Metzman, et al. [27] (2021) summarize the reasoning behind the development of Fuzzbench as follows:

> _In 2020 alone, over 120 papers were published on the topic of improving, developing, and evaluating fuzzers and fuzzing techniques. Yet, proper evaluation of fuzzing techniques remains elusive. The community has struggled to converge on methodology and standard tools for fuzzer evaluation. To address this problem, we introduce FuzzBench as an opensource turnkey platform and free service for evaluating fuzzers. It aims to be easy to use, fast, reliable, and provides reproducible experiments. Since its release in March 2020, FuzzBench has been widely used both in industry and academia, carrying out more than 150 experiments for external users._

**IOT Fuzzbench.**Â Cheng, et al. [28] (2023) summarize the reasoning behind the development of a fuzzing engine test bench specific to cyber-physical systems:

> _High scalability and low operating cost make black-box protocol fuzzing a vital tool for discovering vulnerabilities in the firmware of IoT smart devices. â€¦ In this paper, we design and implement IoTFuzzBench, a scalable, modular, metric-driven automation framework for evaluating black-box protocol fuzzers for IoT smart devices â€¦ We deployed IoTFuzzBench and evaluated 7 popular black-box protocol fuzzers on all benchmark firmware images and benchmark vulnerabilities. The experimental results show that IoTFuzzBench can not only provide fast, reliable, and reproducible experiments, but also effectively evaluate the ability of each fuzzer to find vulnerabilities and the differential performance on different performance metrics._



## Ref
[æ¨¡ç³Šæµ‹è¯•ç®€ä»‹ | CSDN]: https://blog.csdn.net/kelxLZ/article/details/112067973
[ğŸ‘ FuzzingæŠ€æœ¯æ€»ç»“ï¼ˆBrief Surveys on Fuzz Testingï¼‰ - wcventureçš„æ–‡ç«  - çŸ¥ä¹]: https://zhuanlan.zhihu.com/p/43432370

[Fuzzå¿«é€Ÿä¸Šæ‰‹æŒ‡å—]: https://wjk.moe/2023/Fuzz%E5%BF%AB%E9%80%9F%E4%B8%8A%E6%89%8B%E6%8C%87%E5%8D%97/
ä¸å¾—ä¸è¯´ï¼Œå­¦ä¹ ä½¿ç”¨LibAFLçš„è¿‡ç¨‹ï¼Œå°±æ˜¯å­¦ä¹ Fuzzæ¶æ„çš„è¿‡ç¨‹ã€‚åŸºäºLibAFLçš„baby fuzzeræ•™ç¨‹ï¼Œå°±å¯ä»¥äº†è§£åˆ°fuzzerçš„æ¶æ„ã€‚LibAFLæ˜¯åŸºäºRustè¯­è¨€ï¼Œé«˜åº¦å¯è‡ªå®šä¹‰çš„ç»„ä»¶åŒ–fuzzerã€‚ä»–ä»¬çš„å¼€å‘è€…æ­£å°è¯•ç”¨LibAFLå¤åˆ»AFL++ï¼Œlibfuzzerç­‰å¤šä¸ªçŸ¥åfuzzerï¼Œè¿™è¶³ä»¥è¯´æ˜LibAFLçš„å¼ºå¤§ã€‚

Fuzzingçš„å¾ˆé‡è¦çš„ä¸€éƒ¨åˆ†å°±æ˜¯è°ƒè¯•å´©æºƒå’Œä¿®å¤æ¼æ´ã€‚è¿™ä¸ªå’Œfuzzæœ¬èº«ä¸€æ ·é‡è¦ã€‚è¿™ä¸ªæš‚æ—¶æ²¡æ¶‰åŠ

æœ¬æ–‡æ¶µç›–ä»¥ä¸‹å†…å®¹ï¼š
- AFLçš„coverage mapè®¾è®¡
    - PCGUARDæ¨¡å¼
    - cmplog/input to state/redqueen
- Mutators
    - honggfuzzä¸­çš„Mutator
    - AFLä¸­çš„spliceå’Œhavoc
    - MOpt

å­¦ä¹ èµ„æº
- [fuzz101](https://github.com/antonio-morales/Fuzzing101)
- [AFL-TRAINING](https://github.com/mykter/afl-training)ã€‚

[AFLä½¿ç”¨æŒ‡å—]: https://www.cnblogs.com/tomyyyyy/articles/13610206.html
[AFLæ¼æ´æŒ–æ˜æŠ€æœ¯æ¼«è°ˆï¼ˆä¸€ï¼‰ï¼šç”¨AFLå¼€å§‹ä½ çš„ç¬¬ä¸€æ¬¡Fuzzing | freebuf (2018)]: https://www.freebuf.com/system/191543.html
ç¬¬ä¸€ç¯‡æ–‡ç« æ—¨åœ¨è®©è¯»è€…å¯¹AFLçš„ä½¿ç”¨æµç¨‹æœ‰ä¸ªåŸºæœ¬çš„è®¤è¯†ï¼Œæ–‡ä¸­å°†è®¨è®ºå¦‚ä¸‹ä¸€äº›åŸºæœ¬é—®é¢˜ï¼š
> AFLçš„åŸºæœ¬åŸç†å’Œå·¥ä½œæµç¨‹ï¼›
> å¦‚ä½•é€‰æ‹©Fuzzingçš„â½¬æ ‡?
> å¦‚ä½•è·å¾—åˆå§‹è¯­æ–™åº“?
> å¦‚ä½•ä½¿ç”¨AFLæ„å»ºç¨‹åºï¼Ÿ
> AFLçš„å„ç§æ‰§è¡Œæ–¹å¼ï¼›
> AFLçŠ¶æ€çª—å£ä¸­å„éƒ¨åˆ†ä»£è¡¨äº†ä»€ä¹ˆæ„ä¹‰ï¼Ÿ

[ğŸ‘ Fuzzç»“æœåˆ†æå’Œä»£ç è¦†ç›–ç‡ (2018)]: https://www.cnblogs.com/tomyyyyy/articles/13608791.html
- ä»£ç è¦†ç›–ç‡æ˜¯æ¨¡ç³Šæµ‹è¯•ä¸­ä¸€ä¸ªæå…¶é‡è¦çš„æ¦‚å¿µï¼Œ**ä½¿ç”¨ä»£ç è¦†ç›–ç‡å¯ä»¥è¯„ä¼°å’Œæ”¹è¿›æµ‹è¯•è¿‡ç¨‹ï¼Œæ‰§è¡Œåˆ°çš„ä»£ç è¶Šå¤šï¼Œæ‰¾åˆ°bugçš„å¯èƒ½æ€§å°±è¶Šå¤§**ï¼Œæ¯•ç«Ÿï¼Œåœ¨è¦†ç›–çš„ä»£ç ä¸­å¹¶ä¸èƒ½100%å‘ç°bugï¼Œåœ¨æœªè¦†ç›–çš„ä»£ç ä¸­å´æ˜¯100%æ‰¾ä¸åˆ°ä»»ä½•bugçš„ï¼Œæ‰€ä»¥æœ¬èŠ‚ä¸­å°±å°†è¯¦ç»†ä»‹ç»ä»£ç è¦†ç›–ç‡çš„ç›¸å…³æ¦‚å¿µã€‚
- **é˜¿å°”æ³•å®éªŒåœ¨ä¸Šä¸€ç¯‡[æ–‡ç« ](https://www.freebuf.com/system/191543.html)ä¸­å‘å¤§å®¶ä»‹ç»äº†ä½¿ç”¨AFLå¼€å§‹æ¨¡ç³Šæµ‹è¯•å‰è¦åšçš„ä¸€äº›å‡†å¤‡å·¥ä½œï¼Œä»¥åŠAFLçš„å‡ ç§å·¥ä½œæ–¹å¼ï¼Œä½†æ˜¯å¹¶æ²¡æœ‰æåˆ°ä½•æ—¶ç»“æŸæµ‹è¯•è¿‡ç¨‹ï¼Œä»¥åŠæµ‹è¯•å®Œæˆååˆéœ€è¦åšäº›ä»€ä¹ˆã€‚æœ¬æ–‡ä¸­å°±ç»§ç»­ä»‹ç»è¿™äº›å†…å®¹ï¼Œå¹¶å¼€å§‹é€æ­¥ä»‹ç»ä¸€äº›AFLç›¸å…³åŸç†ï¼Œä»¥ä¸‹æ˜¯æœ¬æ–‡ä¸­ä¸»è¦è®¨è®ºçš„é—®é¢˜ï¼š**
	- â€‹ 1.ä½•æ—¶ç»“æŸFuzzingå·¥ä½œ
	- â€‹ 2.afl-fuzzç”Ÿæˆäº†å“ªäº›æ–‡ä»¶
	- â€‹ 3.å¦‚ä½•å¯¹äº§ç”Ÿçš„crashè¿›è¡ŒéªŒè¯å’Œåˆ†ç±»
	- â€‹ 4.ç”¨ä»€ä¹ˆæ¥è¯„ä¼°Fuzzingçš„ç»“æœ
	- â€‹ 5.ä»£ç è¦†ç›–ç‡åŠç›¸å…³æ¦‚å¿µ
	- â€‹ 6.AFLæ˜¯å¦‚ä½•è®°å½•ä»£ç è¦†ç›–ç‡çš„

[ç¬¦å·æ‰§è¡Œ (Symbolic Execution) ä¸çº¦æŸæ±‚è§£ (Constraint Solving) - Flowletçš„æ–‡ç«  - çŸ¥ä¹]: https://zhuanlan.zhihu.com/p/675592367
[é™æ€ä»£ç åˆ†æä¹‹çº¦æŸæ±‚è§£ç®€ä»‹]: https://bbs.huaweicloud.com/blogs/229334

[æ¢ç©¶ï¼šè½¯ä»¶å·¥ç¨‹ä¸­çš„test oracleåˆ°åº•æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿ | CSDN]: https://dalewushuang.blog.csdn.net/article/details/83893881?fromshare=blogdetail&sharetype=blogdetail&sharerId=83893881&sharerefer=PC&sharesource=weixin_43336330&sharefrom=from_link
- Xie T. Augmenting Automatically Generated Unit-Test Suites with Regression Oracle Checking[J]. 2006, 4067:380-403.
	- A test case consists of two parts: a test input to exercise the program under test and a test oracle to check the correctness of the test execution. A test oracle is often in the form of executable assertions such as in the JUnit testing framework. Manually generated test cases are valuable in exposing program faults in the current program version or regression faults in future program versions.
- Tu D, Chen R, Du Z, et al. A Method of Log File Analysis for Test Oracle[C]// International Conference on Scalable Computing and Communications; Eighth International Conference on Embedded Computing, 2009. Scalcom-Embeddedcom. IEEE, 2009:351-354.
- What is a test oracle, and what is it used for? https://stackoverflow.com/questions/23522166/what-is-a-test-oracle-and-what-is-it-used-for
	- ![](../../../../../../../../Assets/Pics/Pasted%20image%2020250407144452.png)
- Â A Course in Black Box Software Testing-ExamplesÂ of Test Oracles.Â  http://www.testingeducation.org/k04/OracleExamples.htm
	- An oracle is a mechanism for determining whether the program has passed or failed a test.
	- A complete oracle would have three capabilities and would carry them out perfectly:
		- A generator, to provide predicted or expected results for each test.
		- A comparator, to compare predicted and obtained results.
		- An evaluator, to determine whether the comparison results are sufficiently close to be a pass.

[Instrumentation ä¸ Profiling | CSDN]: https://blog.csdn.net/fenng/article/details/81362183?fromshare=blogdetail&sharetype=blogdetail&sharerId=81362183&sharerefer=PC&sharesource=weixin_43336330&sharefrom=from_link

çœ‹åˆ°æœ‰åé¦ˆè¯´åˆ°ã€ŠOracleæ€§èƒ½è¯Šæ–­è‰ºæœ¯ã€‹ä¸­å¯¹äº Instrumentation è¿™ä¸ªè¯çš„ç¿»è¯‘é—®é¢˜ã€‚è¯´å®è¯ï¼Œå¯¹è¿™ä¸ªè¯çš„å¤„ç†å½“åˆæŒºè®©æˆ‘å¤´ç–¼ï¼Œè¿™æ˜¯ä¸ªå¯ä»¥æ„ä¼šä½†å¾ˆéš¾ç”¨ä¸€ä¸ªä¸­æ–‡è¯æ±‡å¯¹åº”çš„æœ¯è¯­ï¼Œä¸€äº›ç¿»è¯‘è¯å…¸æˆ–æ˜¯å·²æœ‰çš„ç¿»è¯‘ä½œå“å¯¹è¿™ä¸ªè¯çš„å¤„ç†ä¹Ÿæ˜¯äº”èŠ±å…«é—¨ã€‚åœ¨å›¾çµè‘—è¯‘ä¿±ä¹éƒ¨é‡Œé¢æé—®å¾—åˆ°å¾ˆå¤šå›ç­”ï¼ˆè¿™é‡Œè¦è‡´è°¢ï¼ï¼‰ã€‚æƒè¡¡å†ä¸‰ï¼Œæœ€åæ ¹æ®æ•´ä¸ªç« èŠ‚çš„é‡ç‚¹ä»¥åŠä¸Šä¸‹æ–‡é€‰æ‹©ç”¨ â€œæ€§èƒ½æµ‹é‡â€ã€‚

æˆ‘ä¸å–œæ¬¢ç”¨æœ‰äº›äººè¯´çš„æµ‹è¯•é¢†åŸŸå†…æ‰€ç”¨çš„æœ¯è¯­â€æ’æ¡©â€ï¼Œå®åœ¨æ˜¯æœ‰ç‚¹è¯¡å¼‚ã€‚å½“ç„¶ï¼Œå¦‚æœè¿™ä¸ªè¯ä¸ç¿»è¯‘çš„è¯ï¼Œæˆ–è®¸æ›´å¥½ã€‚

å¦ä¸€ä¸ªæ¯”è¾ƒéš¾ä»¥å¤„ç†çš„å°±æ˜¯ â€œProfilingâ€ ï¼Œæ ¹æ®ç»´åŸºç™¾ç§‘çš„è§£é‡Š ï¼Œè¿™ä¸ªè¯æŒ‡â€åŠ¨æ€ç¨‹åºåˆ†æçš„ä¸€ç§å½¢å¼â€¦æ ¹æ®ç¨‹åºæ‰§è¡Œæ”¶é›†åˆ°çš„ä¿¡æ¯è°ƒæŸ¥ç¨‹åºçš„è¿è¡Œè¡Œä¸ºï¼Œé€šå¸¸ç”¨æ¥æŸ¥æ‰¾ç¨‹åºä¸­çš„ç“¶é¢ˆâ€ã€‚æœ€åæˆ‘ç”¨äº†â€å‰–æâ€ã€‚(Updated: ä¸­æ–‡æ˜¯ â€œæ€§èƒ½åˆ†æâ€œã€‚ä¸è¿‡æˆ‘è§‰å¾—ä¼¼ä¹æœ‰ç‚¹å®¹æ˜“æ··æ·†ã€‚)

è¿™ä¸¤ä¸ªè¯å¾ˆæœ‰è¶£ï¼Œä»»ä½•ä¸€ä¸ªç¨‹åºæˆ–è€…è½¯ä»¶é¡¹ç›®æ„å»ºçš„åˆæœŸï¼Œå¦‚æœæ²¡æœ‰è€ƒè™‘ Instrumentation ï¼Œåœ¨ç¨‹åºæˆ–é¡¹ç›®äº¤ä»˜åï¼Œåˆä¸èƒ½åš Profiling ï¼Œé‚£ä¹ˆè¿™ä¸ªç¨‹åºæˆ–è€…é¡¹ç›®è‚¯å®šä¼šæ˜¯ç¾éš¾ã€‚æ‰€ä»¥ï¼Œèƒ½å¯¹ DBA ç€é‡å¼ºè°ƒä¸€ä¸‹è¿™ä¸€ç‚¹æˆ–è®¸è¦æ¯”çœ‹æ›´å¤šçš„åŒè´¨åŒ–å†…å®¹æ›´æœ‰ä»·å€¼ã€‚
