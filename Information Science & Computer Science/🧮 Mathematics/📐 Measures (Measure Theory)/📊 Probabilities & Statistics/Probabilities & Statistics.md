# Probabilities  & Statistics

[TOC]



## Res
### Related Topics
↗ [Statistical Learning Theory & ML Types](../../../🧠%20Computing%20Methodologies/👽%20Artificial%20Intelligence/🗝️%20AI%20Basics%20&%20Machine%20Learning%20(ML)/📊%20Statistical%20Learning%20Theory%20&%20ML%20Types/Statistical%20Learning%20Theory%20&%20ML%20Types.md)
↗ [Data-Oriented & Human-Centered Technologies](../../../Data-Oriented%20&%20Human-Centered%20Technologies/Data-Oriented%20&%20Human-Centered%20Technologies.md)
- ↗ [Data Science](../../../Data-Oriented%20&%20Human-Centered%20Technologies/Data%20Science/Data%20Science.md)
- ↗ [Data Mining](../../../Data-Oriented%20&%20Human-Centered%20Technologies/Data%20Science/⛏️%20Data%20Mining/Data%20Mining.md)


### Learning Resources
🏫 [UCB /CS70 Discrete Math and Probability Theory](../../../🗺%20CS%20Overview/💋%20Intro%20to%20Computer%20Science/👩🏼‍🏫%20Courses%20of%20Universities/UC%20Berkeley/CS70%20Discrete%20Math%20and%20Probability%20Theory/CS70%20Discrete%20Math%20and%20Probability%20Theory.md)
🏫 [UCB /CS126 Probability Theory](../../../🗺%20CS%20Overview/💋%20Intro%20to%20Computer%20Science/👩🏼‍🏫%20Courses%20of%20Universities/UC%20Berkeley/CS126%20Probability%20Theory/CS126%20Probability%20Theory.md)

📖 何书元《概率论与数理统计》
- 第一章 古典概型和概率空间
	- 1.1 试验与事件. . . . . . . . . . . . . . . . . . . . . . . . . . . . 3
	- 1.2 古典概型与几何概型. . . . . . . . . . . . . . . . . . . . . . . 7
		- 1.2.1 古典概型. . . . . . . . . . . . . . . . . . . . . . . . . 7
		- 1.2.2 几何概型. . . . . . . . . . . . . . . . . . . . . . . . . 14
	- 1.3 概率的公理化和加法公式. . . . . . . . . . . . . . . . . . . . 15
		- 1.3.1 概率的公理化. . . . . . . . . . . . . . . . . . . . . . . 15
		- 1.3.2 概率的加法公式. . . . . . . . . . . . . . . . . . . . . . 17
		- 1.3.3 概率的连续性. . . . . . . . . . . . . . . . . . . . . . . 18
	- 1.4 条件概率和乘法公式. . . . . . . . . . . . . . . . . . . . . . . 18
	- 1.5 事件的独立性. . . . . . . . . . . . . . . . . . . . . . . . . . . 21
	- 1.6 全概率公式与 Bayes 公式. . . . . . . . . . . . . . . . . . . . 24
		- 1.6.1 全概率公式. . . . . . . . . . . . . . . . . . . . . . . . 24
		- 1.6.2 Bayes 公式. . . . . . . . . . . . . . . . . . . . . . . . 28
	- 1.7 概率与频率. . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
- 第二章 随机变量和概率分布
	- 2.1 随机变量. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
	- 2.2 离散型随机变量. . . . . . . . . . . . . . . . . . . . . . . . . . 35
	- 2.3 连续型随机变量. . . . . . . . . . . . . . . . . . . . . . . . . . 43
	- 2.4  概率分布函数. . . . . . . . . . . . . . . . . . . . . . . . . . . 51
		- 2.4.1 概率分布函数. . . . . . . . . . . . . . . . . . . . . . . 51
		- 2.4.2 常见分布的分布函数. . . . . . . . . . . . . . . . . . . 54
	- 2.5 随机变量函数的分布. . . . . . . . . . . . . . . . . . . . . . . 56
- 第三章 随机向量及其分布
	- 3.1 随机向量及其联合分布. . . . . . . . . . . . . . . . . . . . . . 63
	- 3.2 离散型随机向量及其分布. . . . . . . . . . . . . . . . . . . . 65
	- 3.3 连续型随机向量及其分布. . . . . . . . . . . . . . . . . . . . 68
	- 3.4 随机向量函数的分布. . . . . . . . . . . . . . . . . . . . . . . 75
	- 3.5 极大极小值的分布. . . . . . . . . . . . . . . . . . . . . . . . 81
	- 3.6 条件分布和条件密度. . . . . . . . . . . . . . . . . . . . . . . 84
- 第四章 数学期望和方差
	- 4.1 数学期望. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91
		- 4.1.1 数学期望概念. . . . . . . . . . . . . . . . . . . . . . . 91
		- 4.1.2 常见分布数学期望. . . . . . . . . . . . . . . . . . . . 96
	- 4.2 数学期望的性质. . . . . . . . . . . . . . . . . . . . . . . . . . 99
		- 4.2.1 随机向量函数的数学期望. . . . . . . . . . . . . . . . 99
		- 4.2.2 数学期望的性质. . . . . . . . . . . . . . . . . . . . . . 102
	- 4.3 随机变量的方差. . . . . . . . . . . . . . . . . . . . . . . . . . 106
	- 4.4 协方差和相关系数. . . . . . . . . . . . . . . . . . . . . . . . 115
- 第五章 多元正态分布和极限定理
	- 5.1 多元正态分布. . . . . . . . . . . . . . . . . . . . . . . . . . . 119
	- 5.2 大数律. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 123
	- 5.3 中心极限定理. . . . . . . . . . . . . . . . . . . . . . . . . . . 126
- 第六章 描述性统计
	- 6.1 总体和参数. . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
	- 6.2 抽样调查方法. . . . . . . . . . . . . . . . . . . . . . . . . . . 133
	- 6.3 用样本估计总体分布. . . . . . . . . . . . . . . . . . . . . . . 141
	- 6.4 众数和中位数. . . . . . . . . . . . . . . . . . . . . . . . . . . 148
	- 6.5 随机对照试验. . . . . . . . . . . . . . . . . . . . . . . . . . . 152
- 第七章 参数估计
	- 7.1 点估计和矩估计. . . . . . . . . . . . . . . . . . . . . . . . . . 159
	- 7.2 最大似然估计. . . . . . . . . . . . . . . . . . . . . . . . . . . 166
		- 7.2.1 离散型随机变量的情况. . . . . . . . . . . . . . . . . . 166
		- 7.2.2 连续型随机变量的情况. . . . . . . . . . . . . . . . . . 168
	- 7.3 抽样分布及其上 α 分位数. . . . . . . . . . . . . . . . . . . . 173
		- 7.3.1 抽样分布. . . . . . . . . . . . . . . . . . . . . . . . . 174
		- 7.3.2 抽样分布的上 α 分位数. . . . . . . . . . . . . . . . . 179
	- 7.4 正态总体的区间估计. . . . . . . . . . . . . . . . . . . . . . . 182
		- 7.4.1 已知 σ 时, µ 的置信区间. . . . . . . . . . . . . . . . . 183
		- 7.4.2 未知 σ 时 µ 的置信区间. . . . . . . . . . . . . . . . . 185
		- 7.4.3 方差 $σ_2$ 的置信区间. . . . . . . . . . . . . . . . . . . 187
		- 7.4.4 均值差 $µ_1−µ_2$ 的置信区间. . . . . . . . . . . . . . . 189
		- 7.4.5 方差比 $σ^2_1 /σ^2_2$ 的置信区间. . . . . . . . . . . . . . . . 191
		- 7.4.6 单侧置信区间. . . . . . . . . . . . . . . . . . . . . . . 191
	- 7.5 非正态总体和比例 p 的置信区间. . . . . . . . . . . . . . . . 192
		- 7.5.1 正态逼近法. . . . . . . . . . . . . . . . . . . . . . . . 192
		- 7.5.2 比例 p 的置信区间. . . . . . . . . . . . . . . . . . . . 194
- 第八章 假设检验
	- 8.1 假设检验的概念. . . . . . . . . . . . . . . . . . . . . . . . . . 197
	- 8.2 正态均值的假设检验. . . . . . . . . . . . . . . . . . . . . . . 201
		- 8.2.1 已知 σ 时, µ 的正态检验法. . . . . . . . . . . . . . . 201
		- 8.2.2 p 值检验法. . . . . . . . . . . . . . . . . . . . . . . . 203
		- 8.2.3 未知 σ 时, 均值 µ 的 t 检验法. . . . . . . . . . . . . . 204
		- 8.2.4 未知 σ 时, µ 的单边检验法. . . . . . . . . . . . . . . 205
		- 8.2.5 正态近似法. . . . . . . . . . . . . . . . . . . . . . . . 208
	- 8.3 样本量的选择. . . . . . . . . . . . . . . . . . . . . . . . . . . 209
	- 8.4 均值比较的检验. . . . . . . . . . . . . . . . . . . . . . . . . . 210
		- 8.4.1 已知 $σ^2_1 \text{, } σ^2_2$ 时, $µ_1 \text{, } µ_2$ 的检验. . . . . . . . . . . . . . 211
		- 8.4.2 未知 $σ^2_1 \text{, } σ^2_2$, 但已知 $σ^2_1 = σ^2_2$ 时, $µ_1 - µ_2$ 的检验. . . . 213
		- 8.4.3 成对数据的假设检验. . . . . . . . . . . . . . . . . . . 214
		- 8.4.4 未知 $σ^2_1 \text{, } σ^2_2$ 时, $µ_1 \text{, } µ_2$ 的检验. . . . . . . . . . . . . . 216
	- 8.5 方差的假设检验. . . . . . . . . . . . . . . . . . . . . . . . . . 217
	- 8.6 比例的假设检验. . . . . . . . . . . . . . . . . . . . . . . . . . 219
		- 8.6.1 小样本情况下的假设检验. . . . . . . . . . . . . . . . 219
		- 8.6.2 大样本情况下单个比例的假设检验. . . . . . . . . . . 221
		- 8.6.3 大样本情况下两个总体比例的比较. . . . . . . . . . . 224
	- 8.7 总体分布的假设检验. . . . . . . . . . . . . . . . . . . . . . . 227
- 第九章 线性回归分析 233
	- 9.1 数据的相关性. . . . . . . . . . . . . . . . . . . . . . . . . . . 233
		- 9.1.1 样本相关系数. . . . . . . . . . . . . . . . . . . . . . . 234
		- 9.1.2 相关性检验. . . . . . . . . . . . . . . . . . . . . . . . 236
	- 9.2 回归直线. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 238
	- 9.3 一元线性回归. . . . . . . . . . . . . . . . . . . . . . . . . . . 242
		- 9.3.1 最大似然估计和最小二乘估计. . . . . . . . . . . . . . 243
		- 9.3.2 平方和分解公式. . . . . . . . . . . . . . . . . . . . . . 247
		- 9.3.3 斜率 b 的检验. . . . . . . . . . . . . . . . . . . . . . . 248
		- 9.3.4 预测的置信区间. . . . . . . . . . . . . . . . . . . . . . 249
	- 9.4 多元线性回归. . . . . . . . . . . . . . . . . . . . . . . . . . . 251
		- 9.4.1 最小二乘估计. . . . . . . . . . . . . . . . . . . . . . . 252
		- 9.4.2 回归显著性检验. . . . . . . . . . . . . . . . . . . . . . 253
		- 9.4.3 单个系数的显著性检验. . . . . . . . . . . . . . . . . . 254
		- 9.4.4 残差诊断. . . . . . . . . . . . . . . . . . . . . . . . . 255

🎬《概率论与数理统计》教学视频全集（宋浩 https://www.bilibili.com/video/BV1ot411y7mU?p=9&share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d

🎬【比刷剧还爽!】一生推！！【麻省理工公开课】听说你概率论挂了？ MIT 概率论 (中英双语字幕)完整版全25讲，概率论应该这样学 https://www.bilibili.com/video/BV1MV4y1W73J?share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d

📖 概率论与数理统计, 陈希孺

📖 统计学习方法, 李航

📖 INTRODUCTION TO PROBABILITY AND STATISTICS FOR ENGINEERS AND SCIENTISTS 
📖 A FIRST COURSE IN PROBABILITY
📖 Introduction to Probability Model (PM)
📖 Stochastic Process (SP)
Sheldon M. Ross

📖 贝叶斯反演

📖 H. Pishro-Nik, "Introduction to probability, statistics, and random processes", available at [https://www.probabilitycourse.com](https://www.probabilitycourse.com/), Kappa Research LLC, 2014.

👍 https://www.math.wm.edu/~leemis/chart/UDR/UDR.html
![](../../../../Assets/Pics/Screenshot%202025-10-05%20at%2023.37.13.png)

👍 https://stanford.edu/~shervine/teaching/cme-106/
CME 106 ― Introduction to Probability and Statistics for Engineers  
My twin brother [Afshine](https://twitter.com/afshinea) and [I](https://twitter.com/shervinea) ([Afshine Amidi](https://twitter.com/afshinea) and [Shervine Amidi](https://twitter.com/shervinea)) created this set of cheatsheets when I was a TA for Stanford's CME 106 class in Winter 2018. They can (hopefully!) be useful to all future students taking this course as well as to anyone else interested in learning the fundamentals of Probabilities and Statistics.
- [Probability cheatsheet](https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-probability)
- [Statistics cheatsheet](https://stanford.edu/~shervine/teaching/cme-106/cheatsheet-statistics)

👍 https://www.wzchen.com/probability-cheatsheet
https://github.com/wzchen/probability_cheatsheet
- This cheatsheet is a 10-page reference in probability that covers a semester's worth of introductory probability.
- The cheatsheet is based off of Harvard's introductory probability course, Stat 110. It is co-authored by former Stat 110 Teaching Fellow William Chen and Stat 110 Professor Joe Blitzstein.



## Intro
### Probability 🆚 Statistics?
> 🤖 Gemini 2.5

Probability and statistics are two different, though related, fields that work in opposite directions: ==probability uses existing models and rules to predict future events, while statistics uses observed data to infer the underlying rules or models==. In essence, probability moves from model to data, and statistics moves from data to model.

> 🔗 https://zh.wikipedia.org/zh-hans/Portal:%E6%A6%82%E7%8E%87%E4%B8%8E%E7%BB%9F%E8%AE%A1

概率论是集中研究概率及随机现象的数学分支，主要研究对象为随机事件、随机变量以及随机过程。对于随机事件是不可能准确预测其结果的，然而对于一系列的独立随机事件——例如掷骰子、扔硬币、抽扑克牌以及轮盘等，会呈现出一定的、可以被用于研究及预测的规律，两个用来描述这些规律的最具代表性的数学结论分别是大数定律和中心极限定理。

作为统计学的数学基础，概率论对诸多涉及大量数据定量分析的人类活动极为重要，概率论的方法同样适用于其他方面，例如是对只知道系统部分状态的复杂系统的描述——统计力学，而二十世纪物理学的重大发现是以量子力学所描述的原子尺度上物理现象的概率本质。

统计学是对数据的收集、分析、解释、展示、整理进行研究的学科，广泛地应用在各门学科，从自然科学、社会科学到人文学科，甚至被用来工商业及政府的情报决策之上。其中用来描述、摘要数据情况的统计方法称为描述统计学；而对观测中随机性和不确定性，可以通过对观测数据进行数学建模所得的规律进行解释，然后利用这些规律对所研究的过程或总体进行推断，这样的统计方法称为推论统计学。

> 🔗 https://www3.cs.stonybrook.edu/~skiena/jaialai/excerpts/node12.html#

Probability and statistics are related areas of mathematics which concern themselves with analyzing the relative frequency of events. Still, there are fundamental differences in the way they see the world:
- _Probability_ deals with predicting the likelihood of future events, while _statistics_ involves the analysis of the frequency of past events.   
- _Probability_ is primarily a theoretical branch of mathematics, which studies the consequences of mathematical definitions. _Statistics_ is primarily an applied branch of mathematics, which tries to make sense of observations in the real world.

Both subjects are important, relevant, and useful. But they are different, and understanding the distinction is crucial in properly interpreting the relevance of mathematical evidence. Many a gambler has gone to a cold and lonely grave for failing to make the proper distinction between probability and statistics.

This distinction will perhaps become clearer if we trace the thought process of a mathematician encountering her first craps game:
- If this mathematician were a probabilist, she would see the dice and think ``Six-sided dice? Presumably each face of the dice is equally likely to land face up. Now _assuming_ that each face comes up with probability 1/6, I can figure out what my chances of crapping out are.''
- If instead a statistician wandered by, she would see the dice and think ``Those dice may look OK, but how do I _know_ that they are not loaded? I'll watch a while, and keep track of how often each number comes up. Then I can decide if my observations are consistent with the assumption of equal-probability faces. Once I'm confident enough that the dice are fair, I'll call a probabilist to tell me how to play.''

In summary, probability theory enables us to find the consequences of a given ideal world, while statistical theory enables us to to measure the extent to which our world is ideal.

Modern probability theory emerged from the dice tables of France in 1654. Chevalier de Méré, a French nobleman, wondered whether the player or the house had the advantage in a variation of the following betting game.[6.1](https://www3.cs.stonybrook.edu/~skiena/jaialai/excerpts/footnode.html#foot389) In the basic version, the player rolls four dice, and wins provided none of them are a six. The house collects on the even money bet if at least one six appears.  

De Méré brought this problem to attention of the French mathematicians Blaise Pascal and Pierre de Fermat, most famous as the source of Fermat's Last Theorem. Together, these men worked out the basics of probability theory, along the way establishing that the house wins the basic version with probability ![$p = 1 - (5/6)^4 \approx 0.517$](https://www3.cs.stonybrook.edu/~skiena/jaialai/excerpts/img12.gif), where the probability _p_ = 0.5 would denote a fair game where the house wins exactly half the time.    The jai-alai world of our Monte Carlo simulation assumes that we decide the outcome of a point between two teams by flipping a suitably biased coin. If this world were reality, our simulation will compute the correct probability of each possible betting outcome. But all players are not created equal, of course. By doing a statistical study of the outcome of all the matches involving a particular player, we can determine an appropriate amount to bias the coin.

But such computations only make sense if our simulated jai-alai world is a model consistent with the real world. John von Neuman once said that ``the valuation of a poker hand can be sheer mathematics.'' We have to reduce our evaluation of a pelotari to sheer mathematics.



## Ref