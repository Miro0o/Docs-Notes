# Natural Language Processing (NLP)

[TOC]



## Res
### Related Topics
â†— [Linguistics](../../../../Other%20Networks%20of%20Knowledge/Arts%20&%20Cultures/ğŸ“ƒ%20Language%20&%20Literature/Linguistics/Linguistics.md)
â†— [Mathematical Logic Basics (Formal Logic)](../../../ğŸ§®%20Mathematics/ğŸ¤¼â€â™€ï¸%20Mathematical%20Logic/ğŸ“%20Mathematical%20Logic%20Basics%20(Formal%20Logic)/Mathematical%20Logic%20Basics%20(Formal%20Logic).md)
â†— [Formal Syntax, Language Grammar, and BNF](../../../ğŸ§®%20Mathematics/ğŸ¤¼â€â™€ï¸%20Mathematical%20Logic/ğŸ“%20Mathematical%20Logic%20Basics%20(Formal%20Logic)/Formal%20Syntax,%20Language%20Grammar,%20and%20BNF.md)
â†— [Automata Theory and (Formal) Language Theory](../../../ğŸ§®%20Mathematics/ğŸ¤¼â€â™€ï¸%20Mathematical%20Logic/ğŸ˜¶â€ğŸŒ«ï¸%20Theory%20of%20Computation/ğŸ%20Automata%20Theory%20and%20(Formal)%20Language%20Theory/Automata%20Theory%20and%20(Formal)%20Language%20Theory.md)

â†— [LLM (Large Language Model)](ğŸ¦‘%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md)


### Other Resources



## Intro
### Language Models
> ğŸ”— [What is a language model?](https://stanford-cs324.github.io/winter2022/lectures/introduction/#what-is-a-language-model)

The classic definition of a language model (LM) is aÂ **probability distribution over sequences of tokens**. Suppose we have aÂ **vocabulary**Â î‰‚Â of a set of tokens. A language modelÂ pÂ assigns each sequence of tokensÂ x1,â€¦,xLâˆˆî‰‚Â a probability (a number between 0 and 1):

p(x1,â€¦,xL).

The probability intuitively tells us how â€œgoodâ€ a sequence of tokens is. For example, if the vocabulary isÂ î‰‚={ğ–ºğ—ğ–¾,ğ–»ğ–ºğ—…ğ—…,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ—ğ—ğ–¾}, the language model might assign ([demo](http://crfm-models.stanford.edu/static/index.html?prompt=%24%7Bprompt%7D&settings=echo_prompt%3A%20true%0Amax_tokens%3A%200&environments=prompt%3A%20%5Bthe%20mouse%20ate%20the%20cheese%2C%20the%20cheese%20ate%20the%20mouse%2C%20mouse%20the%20the%20cheese%20ate%5D)):

p(ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾)=0.02,

p(ğ—ğ—ğ–¾,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾,ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾)=0.01,

p(ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ—ğ—ğ–¾,ğ—ğ—ğ–¾,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾,ğ–ºğ—ğ–¾)=0.0001.

Mathematically, a language model is a very simple and beautiful object. But the simplicity is deceiving: the ability to assign (meaningful) probabilities to all sequences requires extraordinary (butÂ _implicit_) linguistic abilities and world knowledge.

For example, the LM should assignÂ ğ—†ğ—ˆğ—ğ—Œğ–¾ ğ—ğ—ğ–¾ ğ—ğ—ğ–¾ ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾ ğ–ºğ—ğ–¾Â a very low probability implicitly because itâ€™s ungrammatical (**syntactic knowledge**). The LM should assignÂ ğ—ğ—ğ–¾ ğ—†ğ—ˆğ—ğ—Œğ–¾ ğ–ºğ—ğ–¾ ğ—ğ—ğ–¾ ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾Â higher probability thanÂ ğ—ğ—ğ–¾ ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾ ğ–ºğ—ğ–¾ ğ—ğ—ğ–¾ ğ—†ğ—ˆğ—ğ—Œğ–¾Â implicitly because ofÂ **world knowledge**: both sentences are the same syntactically, but they differ in semantic plausibility.

**Generation**. As defined, a language modelÂ pÂ takes a sequence and returns a probability to assess its goodness. We can also generate a sequence given a language model. The purest way to do this is to sample a sequenceÂ x1:LÂ from the language modelÂ pÂ with probability equal toÂ p(x1:L), denoted:

x1:Lâˆ¼p.

How to do this computationally efficiently depends on the form of the language modelÂ p. In practice, we do not generally sample directly from a language model both because of limitations of real language models and because we sometimes wish to obtain not an â€œaverageâ€ sequence but something closer to the â€œbestâ€ sequence.


**Autoregressive language models**

A common way to write the joint distributionÂ p(x1:L)Â of a sequenceÂ x1:LÂ is using theÂ **chain rule of probability**:

p(x1:L)=p(x1)p(x2âˆ£x1)p(x3âˆ£x1,x2)â‹¯p(xLâˆ£x1:Lâˆ’1)=âˆi=1Lp(xiâˆ£x1:iâˆ’1).

For example ([demo](http://crfm-models.stanford.edu/static/index.html?prompt=the%20mouse%20ate%20the%20cheese&settings=echo_prompt%3A%20true%0Amax_tokens%3A%200%0Atop_k_per_token%3A%2010&environments=)):

p(ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾)=p(ğ—ğ—ğ–¾)p(ğ—†ğ—ˆğ—ğ—Œğ–¾âˆ£ğ—ğ—ğ–¾)p(ğ–ºğ—ğ–¾âˆ£ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾)p(ğ—ğ—ğ–¾âˆ£ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ–ºğ—ğ–¾)p(ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾âˆ£ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾).

In particular,Â p(xiâˆ£x1:iâˆ’1)Â is aÂ **conditional probability distribution**Â of the next tokenÂ xiÂ given the previous tokensÂ x1:iâˆ’1.

Of course, any joint probability distribution can be written this way mathematically, but anÂ **autoregressive language model**Â is one where each conditional distributionÂ p(xiâˆ£x1:iâˆ’1)Â can be computed efficiently (e.g., using a feedforward neural network).

**Generation**. Now to generate an entire sequenceÂ x1:LÂ from an autoregressive language modelÂ p, we sample one token at a time given the tokens generated so far:

forÂ i=1,â€¦,L:xiâˆ¼p(xiâˆ£x1:iâˆ’1)1/T,

whereÂ Tâ‰¥0Â is aÂ **temperature**Â parameter that controls how much randomness we want from the language model:

- T=0: deterministically choose the most probable tokenÂ xiÂ at each positionÂ i
- T=1: sample â€œnormallyâ€ from the pure language model
- T=âˆ: sample from a uniform distribution over the entire vocabularyÂ î‰‚

However, if we just raise the probabilities to the powerÂ 1/T, the probability distribution may not sum to 1. We can fix this by re-normalizing the distribution. We call the normalized versionÂ pT(xiâˆ£x1:iâˆ’1)âˆp(xiâˆ£x1:iâˆ’1)1/TÂ theÂ **annealed**Â conditional probability distribution. For example:

p(ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾)=0.4,p(ğ—†ğ—ˆğ—ğ—Œğ–¾)=0.6

pT=0.5(ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾)=0.31,pT=0.5(ğ—†ğ—ˆğ—ğ—Œğ–¾)=0.69

pT=0.2(ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾)=0.12,pT=0.2(ğ—†ğ—ˆğ—ğ—Œğ–¾)=0.88

pT=0(ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾)=0,pT=0(ğ—†ğ—ˆğ—ğ—Œğ–¾)=1

_Aside_: Annealing is a reference to metallurgy, where hot materials are cooled gradually, and shows up in sampling and optimization algorithms such as simulated annealing.

_Technical note_: sampling iteratively with a temperatureÂ TÂ parameter applied to each conditional distributionÂ p(xiâˆ£x1:iâˆ’1)1/TÂ is not equivalent (except whenÂ T=1) to sampling from the annealed distribution over lengthÂ LÂ sequences.

**Conditional generation**. More generally, we can perform conditional generation by specifying some prefix sequenceÂ x1:iÂ (called aÂ **prompt**) and sampling the restÂ xi+1:LÂ (called theÂ **completion**). For example, generating withÂ T=0Â produces ([demo](http://crfm-models.stanford.edu/static/index.html?prompt=the%20mouse%20ate&settings=temperature%3A%200%0Amax_tokens%3A%202%0Atop_k_per_token%3A%2010%0Anum_completions%3A%2010&environments=)):

ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ–ºğ—ğ–¾î„½î„¾î…î…‹î…‹î…‹î…‹promptâ‡T=0ğ—ğ—ğ–¾,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾î„½î„¾î…î…‹î…‹completion.

If we change the temperature toÂ T=1, we can get more variety ([demo](http://crfm-models.stanford.edu/static/index.html?prompt=the%20mouse%20ate&settings=temperature%3A%201%0Amax_tokens%3A%202%0Atop_k_per_token%3A%2010%0Anum_completions%3A%2010&environments=)), for example,Â ğ—‚ğ—ğ—Œ ğ—ğ—ˆğ—ğ—Œğ–¾Â andÂ ğ—†ğ—’ ğ—ğ—ˆğ—†ğ–¾ğ—ğ—ˆğ—‹ğ—„.

As weâ€™ll see shortly, conditional generation unlocks the ability for language models to solve a variety of tasks by simply changing the prompt.

**Summary**
- A language model is a probability distributionÂ pÂ over sequencesÂ x1:L.
- Intuitively, a good language model should have linguistic capabilities and world knowledge.
- An autoregressive language model allows for efficient generation of a completionÂ xi+1:LÂ given a promptÂ x1:i.
- The temperature can be used to control the amount of variability in generation.


### From Language Model to Task Model
> â†— [Post-Training & Fine Tuning](ğŸ¦‘%20LLM%20(Large%20Language%20Model)/LLM%20Training,%20Utilization,%20and%20Evaluation/Post-Training%20&%20Fine%20Tuning/Post-Training%20&%20Fine%20Tuning.md)
> â†— [LLM Utilization & Prompt Engineering](ğŸ¦‘%20LLM%20(Large%20Language%20Model)/LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/LLM%20Utilization%20&%20Prompt%20Engineering.md)

> ğŸ”— https://stanford-cs324.github.io/winter2022/lectures/capabilities/

Recall that aÂ **language model**Â pÂ is a distribution over sequences of tokensÂ x1:LÂ and thus can be used to score sequences:
p(ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾).

It can also be used to perform conditional generation of a completion given a prompt:
ğ—ğ—ğ–¾ ğ—†ğ—ˆğ—ğ—Œğ–¾ ğ–ºğ—ğ–¾â‡ğ—ğ—ğ–¾ ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾.

AÂ **task**Â is a mapping from inputs to outputs. For example, for question answering, we might have:
> Input: What school did burne hogarth establish?  
> Output: School of Visual Arts

We use the termÂ **adaptation**Â to refer to the process of taking a language model and turning it into a task model, given:
- a natural languageÂ **description**Â of the task, and
- a set ofÂ **training instances**Â (input-output pairs).

There are two primary ways to perform adaptation:
1. **Training**Â (standard supervised learning): train a new model that maps inputs to outputs, either by
    1. creating a new model that uses the language model as features (probing), or
    2. starting with the language model and updating it based on the training instances (fine-tuning), or
    3. something in between (lightweight fine-tuning).
2. **Prompting**Â (in-context learning): Construct a prompt (a string based on the description and training instances) or a set of prompts, feed those into a language model to obtain completions.
    1. Zero-shot learning: number of training examples is 0
    2. One-shot learning: number of training examples is 1
    3. Few-shot learning: number of training examples is few

Which adaptation procedure should we go with?
- **Training can be challenging due to overfitting**Â (just imagine fine-tuning a 175 billion parameter model based on 5 examples). How to do this effectively will be the topic of the adaptation lecture.
- For now, we will be content withÂ **adaptation of GPT-3 using prompting**. Note that the limitation of prompting is that we can only leverage a only small number of training instances (as many as can fit into a prompt). This is due to a limitation of Transformers, where the prompt and the completion must fit into 2048 tokens.

The GPT-3 paper evaluated GPT-3 on a large set of tasks. We will consider a subset of these, and for each task, discuss the following:
1. **Definition**: What is the task and its motivation?
2. **Adaptation**: How do we reduce the task to language modeling (via prompting)?
3. **Results**: What are the quantitative numbers compared to task-specific state-of-the-art models?

**Size and number of examples matters**. By default, the results will based on
- the full GPT-3 model (davinci), which has 175 billion parameters
- using in-context learning with as many training instances as you can stuff into the prompt.

Along the way, we will do ablations to see if model size and number of in-context training instances matters. Spoiler: it does and more is better.

The tasks are grouped as follows:
1. [Language modeling](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#language-modeling)
2. [Question answering](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#question-answering)
3. [Translation](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#translation)
4. [Arithmetic](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#arithmetic)
5. [News article generation](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#news-article-generation)
6. [Novel tasks](https://stanford-cs324.github.io/winter2022/lectures/capabilities/#novel-tasks)


### A Brief History of The Technical Evolution Of Language Models
#### Information Theory, Entropy of English, N-Gram Models
â†— [Information Theory](../../../ğŸ§®%20Mathematics/ğŸ§%20Information%20Theory/Information%20Theory.md)

> ğŸ”— https://stanford-cs324.github.io/winter2022/lectures/introduction/#a-brief-history

**Information theory**. Language models date back to Claude Shannon, who founded information theory in 1948 with his seminal paper,Â [A Mathematical Theory of Communication](https://dl.acm.org/doi/pdf/10.1145/584091.584093). In this paper, he introduced theÂ **entropy**Â of a distribution as

H(p)=âˆ‘xp(x)log1p(x).

The entropy measures the expected number of bitsÂ **any algorithm**Â needs to encode (compress) a sampleÂ xâˆ¼pÂ into a bitstring:

ğ—ğ—ğ–¾ ğ—†ğ—ˆğ—ğ—Œğ–¾ ğ–ºğ—ğ–¾ ğ—ğ—ğ–¾ ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾â‡’0001110101.

- The lower the entropy, the more â€œstructuredâ€ the sequence is, and the shorter the code length.
- Intuitively,Â log1p(x)Â is the length of the code used to represent an elementÂ xÂ that occurs with probabilityÂ p(x).
- IfÂ p(x)=18, we should allocateÂ log2(8)=3Â bits (equivalently,Â log(8)=2.08Â nats).

_Aside_: actually achieving the Shannon limit is non-trivial (e.g., LDPC codes) and is the topic of coding theory.


**Entropy of English**. Shannon was particularly interested in measuring the entropy of English, represented as a sequence of letters. This means we imagine that there is a â€œtrueâ€ distributionÂ pÂ out there (the existence of this is questionable, but itâ€™s still a useful mathematical abstraction) that can spout out samples of English textÂ xâˆ¼p.

Shannon also definedÂ **cross entropy**:

H(p,q)=âˆ‘xp(x)log1q(x),

which measures the expected number of bits (nats) needed to encode a sampleÂ xâˆ¼pÂ using the compression scheme given by the modelÂ qÂ (representingÂ xÂ with a code of lengthÂ 1q(x)).

**Estimating entropy via language modeling**. A crucial property is that the cross entropyÂ H(p,q)Â upper bounds the entropyÂ H(p),

H(p,q)â‰¥H(p),

which means that we can estimateÂ H(p,q)Â by constructing a (language) modelÂ qÂ with only samples from the true data distributionÂ p, whereasÂ H(p)Â is generally inaccessible ifÂ pÂ is English.

So we can get better estimates of the entropyÂ H(p)Â by constructing better modelsÂ q, as measured byÂ H(p,q).


**Shannon game (human language model)**. Shannon first used n-gram models asÂ qÂ in 1948, but in his 1951 paperÂ [Prediction and Entropy of Printed English](https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6773263), he introduced a clever scheme (known as the Shannon game) whereÂ qÂ was provided by a human:

ğ—ğ—ğ–¾ ğ—†ğ—ˆğ—ğ—Œğ–¾ ğ–ºğ—ğ–¾ ğ—†ğ—’ ğ—ğ—ˆ_

Humans arenâ€™t good at providing calibrated probabilities of arbitrary text, so in the Shannon game, the human language model would repeatedly try to guess the next letter, and one would record the number of guesses.
##### N-gram models for downstream applications
Language models became first used in practical applications that required generation of text:
- speech recognition in the 1970s (input: acoustic signal, output: text), and
- machine translation in the 1990s (input: text in a source language, output: text in a target language).


**Noisy channel model**. The dominant paradigm for solving these tasks then was theÂ **noisy channel model**. Taking speech recognition as an example:
- We posit that there is some text sampled from some distributionÂ p.
- This text becomes realized to speech (acoustic signals).
- Then given the speech, we wish to recover the (most likely) text. This can be done via Bayes rule:

p(textâˆ£speech)âˆp(text)âŸlanguage modelp(speechâˆ£text)î„½î„¾î…î…‹î…‹î…‹î…‹î…‹î…‹acoustic model.

Speech recognition and machine translation systems used n-gram language models over words (first introduced by Shannon, but for characters).

**N-gram models**. In anÂ **n-gram model**, the prediction of a tokenÂ xiÂ only depends on the lastÂ nâˆ’1Â charactersÂ xiâˆ’(nâˆ’1):iâˆ’1Â rather than the full history:

p(xiâˆ£x1:iâˆ’1)=p(xiâˆ£xiâˆ’(nâˆ’1):iâˆ’1).

For example, a trigram (n=3) model would define:

p(ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾âˆ£ğ—ğ—ğ–¾,ğ—†ğ—ˆğ—ğ—Œğ–¾,ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾)=p(ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾âˆ£ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾).

These probabilities are computed based on the number of times various n-grams (e.g.,Â ğ–ºğ—ğ–¾ ğ—ğ—ğ–¾ ğ—†ğ—ˆğ—ğ—Œğ–¾Â andÂ ğ–ºğ—ğ–¾ ğ—ğ—ğ–¾ ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾) occur in a large corpus of text, and appropriately smoothed to avoid overfitting (e.g., Kneser-Ney smoothing).

Fitting n-gram models to data is extremelyÂ **computationally cheap**Â and scalable. As a result, n-gram models were trained on massive amount of text. For example,Â [Brants et al. (2007)](https://aclanthology.org/D07-1090.pdf)Â trained a 5-gram model on 2 trillion tokens for machine translation. In comparison, GPT-3 was trained on only 300 billion tokens. However, an n-gram model was fundamentally limited. Imagine the prefix:

ğ–²ğ—ğ–ºğ—‡ğ–¿ğ—ˆğ—‹ğ–½ ğ—ğ–ºğ—Œ ğ–º ğ—‡ğ–¾ğ— ğ–¼ğ—ˆğ—ğ—‹ğ—Œğ–¾ ğ—ˆğ—‡ ğ—…ğ–ºğ—‹ğ—€ğ–¾ ğ—…ğ–ºğ—‡ğ—€ğ—ğ–ºğ—€ğ–¾ ğ—†ğ—ˆğ–½ğ–¾ğ—…ğ—Œ. ğ–¨ğ— ğ—ğ—‚ğ—…ğ—… ğ–»ğ–¾ ğ—ğ–ºğ—ğ—€ğ—ğ— ğ–»ğ—’ ___

IfÂ nÂ is too small, then the model will be incapable of capturing long-range dependencies, and the next word will not be able to depend onÂ ğ–²ğ—ğ–ºğ—‡ğ–¿ğ—ˆğ—‹ğ–½. However, ifÂ nÂ is too big, it will beÂ **statistically infeasible**Â to get good estimates of the probabilities (almost all reasonable long sequences show up 0 times even in â€œhugeâ€ corpora):

count(ğ–²ğ—ğ–ºğ—‡ğ–¿ğ—ˆğ—‹ğ–½,ğ—ğ–ºğ—Œ,ğ–º,ğ—‡ğ–¾ğ—,ğ–¼ğ—ˆğ—ğ—‹ğ—Œğ–¾,ğ—ˆğ—‡,ğ—…ğ–ºğ—‹ğ—€ğ–¾,ğ—…ğ–ºğ—‡ğ—€ğ—ğ–ºğ—€ğ–¾,ğ—†ğ—ˆğ–½ğ–¾ğ—…ğ—Œ)=0.

As a result, language models were limited to tasks such as speech recognition and machine translation where the acoustic signal or source text provided enough information that only capturingÂ **local dependencies**Â (and not being able to capture long-range dependencies) wasnâ€™t a huge problem.
#### Neural Networks
â†— [Deep Learning (Neural Networks)](../ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning/ğŸ“Œ%20Deep%20Learning%20(Neural%20Network)/Deep%20Learning%20(Neural%20Networks).md)
â†— [Neural Network Models](../ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning/ğŸ“Œ%20Deep%20Learning%20(Neural%20Network)/2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/Neural%20Network%20Models.md)

![](../../../../Assets/Pics/Screenshot%202025-09-01%20at%2010.56.49.png)
<small>
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., â€¦ Wen, J.-R. (2025). A Survey of Large Language Models (arXiv:2303.18223). arXiv. <br><a>https://doi.org/10.48550/arXiv.2303.18223</a></small>

> ğŸ”— https://stanford-cs324.github.io/winter2022/lectures/introduction/#neural-language-models

An important step forward for language models was the introduction of neural networks.Â [Bengio et al., 2003](https://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf)Â pioneered neural language models, whereÂ p(xiâˆ£xiâˆ’(nâˆ’1):iâˆ’1)Â is given by a neural network:

p(ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾âˆ£ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾)=some-neural-network(ğ–ºğ—ğ–¾,ğ—ğ—ğ–¾,ğ–¼ğ—ğ–¾ğ–¾ğ—Œğ–¾).

Note that the context length is still bounded byÂ n, but it is nowÂ **statistically feasible**Â to estimate neural language models for much larger values ofÂ n.

Now, the main challenge was that training neural networks was much moreÂ **computationally expensive**. They trained a model on only 14 million words and showed that it outperformed n-gram models trained on the same amount of data. But since n-gram models were more scalable and data was not a bottleneck, n-gram models continued to dominate for at least another decade.

Since 2003, two other key developments in neural language modeling include:
- **Recurrent Neural Networks**Â (RNNs), including Long Short Term Memory (LSTMs), allowed the conditional distribution of a tokenÂ xiÂ to depend on theÂ **entire context**Â x1:iâˆ’1Â (effectivelyÂ n=âˆ), but these were hard to train.
- **Transformers**Â are a more recent architecture (developed for machine translation in 2017) that again returned to having fixed context lengthÂ n, but were muchÂ **easier to train**Â (and exploited the parallelism of GPUs). Also,Â nÂ could be made â€œlarge enoughâ€ for many applications (GPT-3 usedÂ n=2048).
#### Large Language Models
â†— [LLM (Large Language Model) /The Technical Evolution of LLM & Future Directions](ğŸ¦‘%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md#The%20Technical%20Evolution%20of%20LLM%20&%20Future%20Directions)



## Ref
