# LLM (Large Language Model)

[TOC]



## Res
### Related Topics
â†— [Deep Learning (Neural Networks)](../../ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning/ğŸ“Œ%20Deep%20Learning%20(Neural%20Network)/Deep%20Learning%20(Neural%20Networks).md)
- â†— [Transformers](../../ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning/ğŸ“Œ%20Deep%20Learning%20(Neural%20Network)/2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/Transformers/Transformers.md)

LLM & Academics ğŸ§‘â€ğŸ“
- â†— [LLM & Federated Learning](../../../../Academics%20ğŸ“%20(In%20CS)/ğŸ—’ï¸%20My%20Academic%20Projects%20Workspace/LLM%20&%20Federated%20Learning/LLM%20&%20Federated%20Learning.md)
- â†— [LLM & Fuzzing](../../../../Academics%20ğŸ“%20(In%20CS)/ğŸ—’ï¸%20My%20Academic%20Projects%20Workspace/LLM%20&%20Software%20Security%20and%20Analysis/LLM%20&%20Fuzzing.md)
- â†— [LLM & Software Security and Analysis](../../../../Academics%20ğŸ“%20(In%20CS)/ğŸ—’ï¸%20My%20Academic%20Projects%20Workspace/LLM%20&%20Software%20Security%20and%20Analysis/LLM%20&%20Software%20Security%20and%20Analysis.md)
â†— [LLM & Security](../../../../CyberSecurity/ğŸ¤–%20AI%20x%20Security/LLM%20&%20Security/LLM%20&%20Security.md)

â†— [AI(LLM) x SE](../../../../Software%20Engineering/ğŸ¤–%20AI(LLM)%20x%20SE/AI(LLM)%20x%20SE.md)
- â†— [LLM Application Dev](../../../../Software%20Engineering/ğŸ¤–%20AI(LLM)%20x%20SE/LLM%20Application%20Dev/LLM%20Application%20Dev.md)
- â†— [LangChain](../../../../Software%20Engineering/ğŸ¤–%20AI(LLM)%20x%20SE/LLM%20Application%20Dev/LLM%20Application%20Dev%20Frameworks/LangChain/LangChain.md)

â†— [Research Topics in LLM](../../../../Academics%20ğŸ“%20(In%20CS)/Academic%20Research%20Directions%20&%20Areas/Research%20Topics%20in%20LLM.md)
â†— [XAI (AI Explainable & Interpretable)](../../XAI%20(AI%20Explainable%20&%20Interpretable)/XAI%20(AI%20Explainable%20&%20Interpretable).md)


### Learning Resource
#### Texts & Docs
ğŸ“– å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼šä»ç†è®ºåˆ°å®è·µ
https://intro-llm.github.io
å¤§è¯­è¨€æ¨¡å‹ï¼ˆLarge Language Modelsï¼ŒLLMï¼‰æ˜¯ä¸€ç§ç”±åŒ…å«æ•°ç™¾äº¿ä»¥ä¸Šæƒé‡çš„æ·±åº¦ç¥ç»ç½‘ç»œæ„å»ºçš„è¯­è¨€æ¨¡å‹ï¼Œä½¿ç”¨è‡ªç›‘ç£å­¦ä¹ æ–¹æ³•é€šè¿‡å¤§é‡æ— æ ‡è®°æ–‡æœ¬è¿›è¡Œè®­ç»ƒã€‚è‡ª2018å¹´ä»¥æ¥ï¼ŒåŒ…å«Googleã€OpenAIã€Metaã€ç™¾åº¦ã€åä¸ºç­‰å…¬å¸å’Œç ”ç©¶æœºæ„éƒ½çº·çº·å‘å¸ƒäº†åŒ…æ‹¬BERTï¼Œ GPTç­‰åœ¨å†…å¤šç§æ¨¡å‹ï¼Œå¹¶åœ¨å‡ ä¹æ‰€æœ‰è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­éƒ½è¡¨ç°å‡ºè‰²ã€‚2021å¹´å¼€å§‹å¤§æ¨¡å‹å‘ˆç°çˆ†å‘å¼çš„å¢é•¿ï¼Œç‰¹åˆ«æ˜¯2022å¹´11æœˆChatGPTå‘å¸ƒåï¼Œæ›´æ˜¯å¼•èµ·äº†å…¨ä¸–ç•Œçš„å¹¿æ³›å…³æ³¨ã€‚ç”¨æˆ·å¯ä»¥ä½¿ç”¨è‡ªç„¶è¯­è¨€ä¸ç³»ç»Ÿäº¤äº’ï¼Œä»è€Œå®ç°åŒ…æ‹¬é—®ç­”ã€åˆ†ç±»ã€æ‘˜è¦ã€ç¿»è¯‘ã€èŠå¤©ç­‰ä»ç†è§£åˆ°ç”Ÿæˆçš„å„ç§ä»»åŠ¡ã€‚å¤§å‹è¯­è¨€æ¨¡å‹å±•ç°å‡ºäº†å¼ºå¤§çš„å¯¹ä¸–ç•ŒçŸ¥è¯†æŒæ¡å’Œå¯¹è¯­è¨€çš„ç†è§£ã€‚æœ¬ä¹¦å°†ä»‹ç»å¤§è¯­è¨€æ¨¡å‹çš„åŸºç¡€ç†è®ºåŒ…æ‹¬è¯­è¨€æ¨¡å‹ã€åˆ†å¸ƒå¼æ¨¡å‹è®­ç»ƒä»¥åŠå¼ºåŒ–å­¦ä¹ ï¼Œå¹¶ä»¥Deepspeed-Chatæ¡†æ¶ä¸ºä¾‹ä»‹ç»å®ç°å¤§è¯­è¨€æ¨¡å‹å’Œç±»ChatGPTç³»ç»Ÿçš„å®è·µã€‚

---
ğŸ”¥ ğŸ‘ ğŸ“„ https://github.com/RUCAIBox/LLMSurvey ï¼ˆå¤§è¯­è¨€æ¨¡å‹ç»¼è¿° | ä¸­å›½äººæ°‘å¤§å­¦é«˜ç“´äººå·¥æ™ºèƒ½å­¦é™¢ï¼‰
A collection of papers and resources related to Large Language Models.
The organization of papers refers to our surveyÂ [**"A Survey of Large Language Models"**](https://arxiv.org/abs/2303.18223).
To facilitate the reading of our (English-verison) survey, we also translate aÂ [**Chinese version**](https://github.com/RUCAIBox/LLMSurvey/blob/main/assets/LLM_Survey_Chinese.pdf)Â for this survey. We will continue to update the Chinese version.

---
ğŸ”¥ ğŸ“„ https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE
Papers must know to understand the world of deep learning & AIGC

---
ğŸ”¥ ğŸªœ https://github.com/Hannibal046/Awesome-LLM/tree/main
Large Language Models(LLM) have taken theÂ ~~NLP community~~Â ~~AI community~~Â **the Whole World**Â by storm. Here is a curated list of papers about large language models, especially relating to ChatGPT. It also contains frameworks for LLM training, tools to deploy LLM, courses and tutorials about LLM and all publicly available LLM checkpoints and APIs.

![|500](../../../../../Assets/Pics/Pasted%20image%2020240512212009.png)

- [Awesome-LLM](https://github.com/Hannibal046/Awesome-LLM/tree/main#awesome-llm-)
    - [Milestone Papers](https://github.com/Hannibal046/Awesome-LLM/tree/main#milestone-papers)
    - [Other Papers](https://github.com/Hannibal046/Awesome-LLM/tree/main#other-papers)
    - [LLM Leaderboard](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-leaderboard)
    - [Open LLM](https://github.com/Hannibal046/Awesome-LLM/tree/main#open-llm)
    - [LLM Data](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-data)
    - [LLM Evaluation](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-evaluation)
    - [LLM Training Framework](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-training-frameworks)
    - [LLM Deployment](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-deployment)
    - [LLM Applications](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-applications)
    - [LLM Tutorials and Courses](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-tutorials-and-courses)
    - [LLM Books](https://github.com/Hannibal046/Awesome-LLM/tree/main#llm-books)
    - [Great thoughts about LLM](https://github.com/Hannibal046/Awesome-LLM/tree/main#great-thoughts-about-llm)
    - [Miscellaneous](https://github.com/Hannibal046/Awesome-LLM/tree/main#miscellaneous)

**Great thoughts about LLM**
> ğŸ”— https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#great-thoughts-about-llm

- [Why did all of the public reproduction of GPT-3 fail?](https://jingfengyang.github.io/gpt)
- [A Stage Review of Instruction Tuning](https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2)
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)
- [Why you should work on AI AGENTS!](https://www.youtube.com/watch?v=fqVLjtvWgq8)
- [Google "We Have No Moat, And Neither Does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
- [AI competition statement](https://petergabriel.com/news/ai-competition-statement/)
- [Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
- [Noam Chomsky: The False Promise of ChatGPT](https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html)
- [Is ChatGPT 175 Billion Parameters? Technical Analysis](https://orenleung.super.site/is-chatgpt-175-billion-parameters-technical-analysis)
- [The Next Generation Of Large Language Models](https://www.notion.so/Awesome-LLM-40c8aa3f2b444ecc82b79ae8bbd2696b)
- [Large Language Model Training in 2023](https://research.aimultiple.com/large-language-model-training/)
- [How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)
- [Open Pretrained Transformers](https://www.youtube.com/watch?v=p9IxoSkvZ-M&t=4s)
- [Scaling, emergence, and reasoning in large language models](https://docs.google.com/presentation/d/1EUV7W7X_w0BDrscDhPg7lMGzJCkeaPkGCJ3bN8dluXc/edit?pli=1&resourcekey=0-7Nz5A7y8JozyVrnDtcEKJA#slide=id.g16197112905_0_0)

**Miscellaneous**
> ğŸ”— https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#miscellaneous

- [Arize-Phoenix](https://phoenix.arize.com/)Â - Open-source tool for ML observability that runs in your notebook environment. Monitor and fine tune LLM, CV and Tabular Models.
- [Emergent Mind](https://www.emergentmind.com/)Â - The latest AI news, curated & explained by GPT-4.
- [ShareGPT](https://sharegpt.com/)Â - Share your wildest ChatGPT conversations with one click.
- [Major LLMs + Data Availability](https://docs.google.com/spreadsheets/d/1bmpDdLZxvTCleLGVPgzoMTQ0iDP2-7v7QziPrzPdHyM/edit#gid=0)
- [500+ Best AI Tools](https://vaulted-polonium-23c.notion.site/500-Best-AI-Tools-e954b36bf688404ababf74a13f98d126)
- [Cohere Summarize Beta](https://txt.cohere.ai/summarize-beta/)Â - Introducing Cohere Summarize Beta: A New Endpoint for Text Summarization
- [chatgpt-wrapper](https://github.com/mmabrouk/chatgpt-wrapper)Â - ChatGPT Wrapper is an open-source unofficial Python API and CLI that lets you interact with ChatGPT.
- [Open-evals](https://github.com/open-evals/evals)Â - A framework extend openai'sÂ [Evals](https://github.com/openai/evals)Â for different language model.
- [Cursor](https://www.cursor.so/)Â - Write, edit, and chat about your code with a powerful AI.
- [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT)Â - an experimental open-source application showcasing the capabilities of the GPT-4 language model.
- [OpenAGI](https://github.com/agiresearch/OpenAGI)Â - When LLM Meets Domain Experts.
- [EasyEdit](https://github.com/zjunlp/EasyEdit)Â - An easy-to-use framework to edit large language models.
- [chatgpt-shroud](https://github.com/guyShilo/chatgpt-shroud)Â - A Chrome extension for OpenAI's ChatGPT, enhancing user privacy by enabling easy hiding and unhiding of chat history. Ideal for privacy during screen shares.

---
https://github.com/Shubhamsaboo/awesome-llm-apps
A curated collection ofÂ **Awesome LLM apps built with RAG, AI Agents, Multi-agent Teams, MCP, Voice Agents, and more.**Â This repository features LLM apps that use models from OpenAI, Anthropic, Google, and open-source models like DeepSeek, Qwen or Llama that you can run locally on your computer.

---
ğŸ¤” https://transformer-circuits.pub/2025/attribution-graphs/biology.html
**On the Biology of a Large Language Model | Anthropic**
We investigate the internal mechanisms used by Claude 3.5 Haiku â€” Anthropic's lightweight production model â€” in a variety of contexts, using our circuit tracing methodology.
#### Tutorials & Books
https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-tutorials-and-courses
**LLM Tutorials and Courses**
- [Andrej Karpathy Series](https://www.youtube.com/@AndrejKarpathy)Â - My favorite!
- [Umar Jamil Series](https://www.youtube.com/@umarjamilai)Â - high quality and educational videos you don't want to miss.
- [Alexander Rush Series](https://rush-nlp.com/projects/)Â - high quality and educational materials you don't want to miss.
- [llm-course](https://github.com/mlabonne/llm-course)Â - Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.
- [UWaterloo CS 886](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/)Â - Recent Advances on Foundation Models.
- [CS25-Transformers United](https://web.stanford.edu/class/cs25/)
- [ChatGPT Prompt Engineering](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
- [Princeton: Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)
- [CS324 - Large Language Models](https://stanford-cs324.github.io/winter2022/)
- [State of GPT](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)
- [A Visual Guide to Mamba and State Space Models](https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state?utm_source=multiple-personal-recommendations-email&utm_medium=email&open=false)
- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [minbpe](https://www.youtube.com/watch?v=zduSFxRajkE&t=1157s)Â - Minimal, clean code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.
- [femtoGPT](https://github.com/keyvank/femtoGPT)Â - Pure Rust implementation of a minimal Generative Pretrained Transformer.
- [Neurips2022-Foundational Robustness of Foundation Models](https://nips.cc/virtual/2022/tutorial/55796)
- [ICML2022-Welcome to the "Big Model" Era: Techniques and Systems to Train and Serve Bigger Models](https://icml.cc/virtual/2022/tutorial/18440)
- [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
- [LLMâ€‘RLâ€‘Visualized (EN)](https://github.com/changyeyu/LLM-RL-Visualized/blob/master/src/README_EN.md)Â |Â [LLMâ€‘RLâ€‘Visualized (ä¸­æ–‡)](https://github.com/changyeyu/LLM-RL-Visualized)Â - 100+ LLM / RL Algorithm MapsğŸ“š.

https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-books
**LLM Books**
- [Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs](https://amzn.to/3GUlRng)Â - it comes with aÂ [GitHub repository](https://github.com/benman1/generative_ai_with_langchain)Â that showcases a lot of the functionality
- [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch)Â - A guide to building your own working LLM.
- [BUILD GPT: HOW AI WORKS](https://www.amazon.com/dp/9152799727?ref_=cm_sw_r_cp_ud_dp_W3ZHCD6QWM3DPPC0ARTT_1)Â - explains how to code a Generative Pre-trained Transformer, or GPT, from scratch.
- [Hands-On Large Language Models: Language Understanding and Generation](https://www.llm-book.com/)Â - Explore the world of Large Language Models with over 275 custom made figures in this illustrated guide!
- [The Chinese Book for Large Language Models](http://aibox.ruc.edu.cn/zws/index.htm)Â - An Introductory LLM Textbook Based onÂ [_A Survey of Large Language Models_](https://arxiv.org/abs/2303.18223).

https://diffusion.csail.mit.edu/
Introduction to Flow Matching and Diffusion Models
MIT Computer Science Class 6.S184: Generative AI with Stochastic Differential Equations
- Diffusion and flow-based models have become the state of the art for generative AI across a wide range of data modalities, including images, videos, shapes, molecules, music, and more! This course aims to build up the mathematical framework underlying these models from first principles. At the end of the class, students will have built a toy image diffusion model from scratch, and along the way, will have gained hands-on experience with the mathematical toolbox of stochastic differential equations that is useful in many other fields. This course is ideal for students who want to develop a principled understanding of the theory and practice of generative AI.
#### Videos
https://youtu.be/1il-s4mgNdI?si=DxlD_98ITLZsnCIw
What does it mean for computers to understand language? | LM1
vcubingx

https://youtu.be/kCc8FmEb1nY?si=Dhj1moY2pHkyiCiT
Let's build GPT: from scratch, in code, spelled out.
Andrej Karpathy

https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=AUDMGwyz7-yL33Xd
Neural networks | 3Blue1Brown
- [But what is a neural network? | Deep learning chapter 1](https://youtu.be/aircAruvnKk?si=RiyEviyfGbC8YwS0)
- [Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://youtu.be/IHZwWFHWa-w?si=DqZgN_65JZfHX-81)
- [Backpropagation, intuitively | Deep Learning Chapter 3](https://youtu.be/Ilg3gGewQ5U?si=yYl6Vi6Sb-NxWbh5)
- [Backpropagation calculus | Deep Learning Chapter 4](https://youtu.be/tIeHLnjs5U8?si=w84SrOkyDnMwKSk7)
- [Large Language Models explained briefly](https://youtu.be/LPZh9BOjkQs?si=7CRyWTVnx3BIGQGy)
- [Transformers, the tech behind LLMs | Deep Learning Chapter 5](https://youtu.be/wjZofJX0v4M?si=cLC36CWJiJPKQJgT)
	- ã€ã€å®˜æ–¹åŒè¯­ã€‘GPTæ˜¯ä»€ä¹ˆï¼Ÿç›´è§‚è§£é‡ŠTransformer | æ·±åº¦å­¦ä¹ ç¬¬5ç« -å“”å“©å“”å“©ã€‘ https://b23.tv/rcO76mO
- [Attention in transformers, step-by-step | Deep Learning Chapter 6](https://youtu.be/eMlx5fFNoYc?si=UqpVj1vDxOtWAnlc)
	- ã€ã€å®˜æ–¹åŒè¯­ã€‘ç›´è§‚è§£é‡Šæ³¨æ„åŠ›æœºåˆ¶ï¼ŒTransformerçš„æ ¸å¿ƒ | ã€æ·±åº¦å­¦ä¹ ç¬¬6ç« ã€‘-å“”å“©å“”å“©ã€‘ https://b23.tv/f0udg4P
- [How might LLMs store facts | Deep Learning Chapter 7](https://youtu.be/9-Jl0dxWQs8?si=jJPuNPfLV6AtWNJa)

Lex Fridman

Machine Learning Street Talk

StatQuest with Josh Starmer

Jeremy Howard

Serrano.Academy

Hamel Husain

Jason Liu

Dave Ebbelaar
#### Blogs & Communities
https://www.alignmentforum.org/


### Papers & Researches
#### LLM Survey Papers
Minaee, S., Mikolov, T., Nikzad, N., Chenaghlu, M., Socher, R., Amatriain, X., & Gao, J. (2025). _Large Language Models: A Survey_ (arXiv:2402.06196). arXiv. [https://doi.org/10.48550/arXiv.2402.06196](https://doi.org/10.48550/arXiv.2402.06196)

ğŸ  https://github.com/RUCAIBox/LLMSurvey
A collection of papers and resources related to Large Language Models.
The organization of papers refers to our surveyÂ [**"A Survey of Large Language Models"**](https://arxiv.org/abs/2303.18223).Â 
- Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., â€¦ Wen, J.-R. (2025). _A Survey of Large Language Models_ (arXiv:2303.18223). arXiv. [https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)
#### Other Papers
> ğŸ”— https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#other-papers (2025.01)

If you're interested in the field of LLM, you may find the above list of milestone papers helpful to explore its history and state-of-the-art. However, each direction of LLM offers a unique set of insights and contributions, which are essential to understanding the field as a whole. For a detailed list of papers in various subfields, please refer to the following link:
- [Awesome-LLM-hallucination](https://github.com/LuckyyySTA/Awesome-LLM-hallucination)Â - LLM hallucination paper list.
- [awesome-hallucination-detection](https://github.com/EdinburghNLP/awesome-hallucination-detection)Â - List of papers on hallucination detection in LLMs.
- [LLMsPracticalGuide](https://github.com/Mooler0410/LLMsPracticalGuide)Â - A curated list of practical guide resources of LLMs
- [Awesome ChatGPT Prompts](https://github.com/f/awesome-chatgpt-prompts)Â - A collection of prompt examples to be used with the ChatGPT model.
- [awesome-chatgpt-prompts-zh](https://github.com/PlexPt/awesome-chatgpt-prompts-zh)Â - A Chinese collection of prompt examples to be used with the ChatGPT model.
- [Awesome ChatGPT](https://github.com/humanloop/awesome-chatgpt)Â - Curated list of resources for ChatGPT and GPT-3 from OpenAI.
- [Chain-of-Thoughts Papers](https://github.com/Timothyxxx/Chain-of-ThoughtsPapers)Â - A trend starts from "Chain of Thought Prompting Elicits Reasoning in Large Language Models.
- [Awesome Deliberative Prompting](https://github.com/logikon-ai/awesome-deliberative-prompting)Â - How to ask LLMs to produce reliable reasoning and make reason-responsive decisions.
- [Instruction-Tuning-Papers](https://github.com/SinclairCoder/Instruction-Tuning-Papers)Â - A trend starts fromÂ `Natrural-Instruction`Â (ACL 2022),Â `FLAN`Â (ICLR 2022) andÂ `T0`Â (ICLR 2022).
- [LLM Reading List](https://github.com/crazyofapple/Reading_groups/)Â - A paper & resource list of large language models.
- [Reasoning using Language Models](https://github.com/atfortes/LM-Reasoning-Papers)Â - Collection of papers and resources on Reasoning using Language Models.
- [Chain-of-Thought Hub](https://github.com/FranxYao/chain-of-thought-hub)Â - Measuring LLMs' Reasoning Performance
- [Awesome GPT](https://github.com/formulahendry/awesome-gpt)Â - A curated list of awesome projects and resources related to GPT, ChatGPT, OpenAI, LLM, and more.
- [Awesome GPT-3](https://github.com/elyase/awesome-gpt3)Â - a collection of demos and articles about theÂ [OpenAI GPT-3 API](https://openai.com/blog/openai-api/).
- [Awesome LLM Human Preference Datasets](https://github.com/PolisAI/awesome-llm-human-preference-datasets)Â - a collection of human preference datasets for LLM instruction tuning, RLHF and evaluation.
- [RWKV-howto](https://github.com/Hannibal046/RWKV-howto)Â - possibly useful materials and tutorial for learning RWKV.
- [ModelEditingPapers](https://github.com/zjunlp/ModelEditingPapers)Â - A paper & resource list on model editing for large language models.
- [Awesome LLM Security](https://github.com/corca-ai/awesome-llm-security)Â - A curation of awesome tools, documents and projects about LLM Security.
- [Awesome-Align-LLM-Human](https://github.com/GaryYufei/AlignLLMHumanSurvey)Â - A collection of papers and resources about aligning large language models (LLMs) with human.
- [Awesome-Code-LLM](https://github.com/huybery/Awesome-Code-LLM)Â - An awesome and curated list of best code-LLM for research.
- [Awesome-LLM-Compression](https://github.com/HuangOwen/Awesome-LLM-Compression)Â - Awesome LLM compression research papers and tools.
- [Awesome-LLM-Systems](https://github.com/AmberLJC/LLMSys-PaperList)Â - Awesome LLM systems research papers.
- [awesome-llm-webapps](https://github.com/snowfort-ai/awesome-llm-webapps)Â - A collection of open source, actively maintained web apps for LLM applications.
- [awesome-japanese-llm](https://github.com/llm-jp/awesome-japanese-llm)Â - æ—¥æœ¬èªLLMã¾ã¨ã‚ - Overview of Japanese LLMs.
- [Awesome-LLM-Healthcare](https://github.com/mingze-yuan/Awesome-LLM-Healthcare)Â - The paper list of the review on LLMs in medicine.
- [Awesome-LLM-Inference](https://github.com/DefTruth/Awesome-LLM-Inference)Â - A curated list of Awesome LLM Inference Paper with codes.
- [Awesome-LLM-3D](https://github.com/ActiveVisionLab/Awesome-LLM-3D)Â - A curated list of Multi-modal Large Language Model in 3D world, including 3D understanding, reasoning, generation, and embodied agents.
- [LLMDatahub](https://github.com/Zjh-819/LLMDataHub)Â - a curated collection of datasets specifically designed for chatbot training, including links, size, language, usage, and a brief description of each dataset
- [Awesome-Chinese-LLM](https://github.com/HqWu-HITCS/Awesome-Chinese-LLM)Â - æ•´ç†å¼€æºçš„ä¸­æ–‡å¤§è¯­è¨€æ¨¡å‹ï¼Œä»¥è§„æ¨¡è¾ƒå°ã€å¯ç§æœ‰åŒ–éƒ¨ç½²ã€è®­ç»ƒæˆæœ¬è¾ƒä½çš„æ¨¡å‹ä¸ºä¸»ï¼ŒåŒ…æ‹¬åº•åº§æ¨¡å‹ï¼Œå‚ç›´é¢†åŸŸå¾®è°ƒåŠåº”ç”¨ï¼Œæ•°æ®é›†ä¸æ•™ç¨‹ç­‰ã€‚
- [LLM4Opt](https://github.com/FeiLiu36/LLM4Opt)Â - Applying Large language models (LLMs) for diverse optimization tasks (Opt) is an emerging research area. This is a collection of references and papers of LLM4Opt.
- [awesome-language-model-analysis](https://github.com/Furyton/awesome-language-model-analysis)Â - This paper list focuses on the theoretical or empirical analysis of language models, e.g., the learning dynamics, expressive capacity, interpretability, generalization, and other interesting topics.


### Other Resources
ğŸ¬ https://youtu.be/OFS90-FX6pg?si=hlsJj4DUWzGrZ_V-
The Origin of ChatGPT | Art of the Problem
I follow the 35 year journey that led to the explosion of Large Language Models. From Jordan's pioneering work in 1986 to today's GPT-4, this documentary traces how AI learned to talk. Featuring insights from AI pioneers including Chomsky, Hofstadter, Hinton, and LeCun, exploring the revolutionary concepts that made ChatGPT possible: transformer architecture, attention mechanism, next-token prediction, and emergent capabilities. Next video following open ai's o1 model My script, references & visualizations here: https://docs.google.com/document/d/1s7FNPoKPW9y3EhvzNgexJaEG2pP4Fx_rmI4askoKZPA/edit?usp=sharing

ğŸ¬ (1hr Talk) Intro to Large Language Models | Andrej Karpathy
https://youtu.be/zjkBMFhNj_g?si=G546Rtz9r9hc233z



## Intro: LLM Principles & Utilization
[Large Language Models explained briefly | 3Blue1Brown](https://youtu.be/LPZh9BOjkQs?si=7CRyWTVnx3BIGQGy)

ğŸ“ https://cameronrwolfe.substack.com/p/understanding-and-using-supervised
- [Transformer Architecture](https://cameronrwolfe.substack.com/i/136366740/the-transformer-from-top-to-bottom): Nearly all modern language modelsâ€”_and many other deep learning models_â€”are based upon this architecture.
- [Decoder-only Transformers](https://twitter.com/cwolferesearch/status/1640446111348555776?s=20)Â : This is the specific variant of the transformer architecture that is used by most generative LLMs.
- [Brief History of LLMs](https://twitter.com/cwolferesearch/status/1639378997627826176?s=20): LLMs have gone through several phases from the creation ofÂ [GPT](https://cameronrwolfe.substack.com/i/85568430/improving-language-understanding-by-generative-pre-training-gpt)Â  to the release of ChatGPT.Â 
- [Next token prediction](https://cameronrwolfe.substack.com/i/136638774/understanding-next-token-prediction): thisÂ [self-supervised](https://cameronrwolfe.substack.com/i/76273144/self-supervised-learning)Â training objective underlies nearly all LLM functionality and is used by SFT!
- [Language Model Pretraining](https://cameronrwolfe.substack.com/i/136638774/language-model-pretraining): language models are pretrained over a massive, unlabeled textual corpus.Â 
- [Language Model Inference](https://cameronrwolfe.substack.com/i/136638774/autoregressive-inference-process): language models can be used to generate coherent sequences of text via autoregressive next token prediction.

â†— [Natural Language Processing (NLP) /Intro](../Natural%20Language%20Processing%20(NLP).md#Intro)


### LLM Backgrounds
#### Scaling Laws

#### Emergent Abilities


### â­ LLM Modeling
â†— [LLM Models Guide & Leaderboard](ğŸªœ%20LLM%20Models%20Guide%20&%20Leaderboard/LLM%20Models%20Guide%20&%20Leaderboard.md)

![](../../../../../Assets/Pics/Screenshot%202025-09-04%20at%2020.14.39.png)
<small><a>https://poloclub.github.io/transformer-explainer/</a></small>
#### Tokenization & Embedding
> ğŸ”— https://stanford-cs324.github.io/winter2022/lectures/modeling/#model-architecture


#### LLM Model Architectures
â†— [Transformers](../../ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning/ğŸ“Œ%20Deep%20Learning%20(Neural%20Network)/2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/Transformers/Transformers.md)
- Tokenization
- Attention
- Probability

â†— [RWKV (Receptance Weighted Key Value)](../../ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning/ğŸ“Œ%20Deep%20Learning%20(Neural%20Network)/2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/RNN%20(Recurrent%20Neural%20Network)/RWKV%20(Receptance%20Weighted%20Key%20Value).md)
â†— [Mamba](../../ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning/ğŸ“Œ%20Deep%20Learning%20(Neural%20Network)/2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/SSM%20(State-Space%20Model)/Mamba.md)


### LLM Training, Utilization, and Evaluation
â†— [LLM Training, Utilization, and Evaluation](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Training,%20Utilization,%20and%20Evaluation.md)
- â†— [Pre-Training](LLM%20Training,%20Utilization,%20and%20Evaluation/Pre-Training/Pre-Training.md) (In-Weight Learning)
- â†— [LLM Adaptation & Alignment Tuning](LLM%20Training,%20Utilization,%20and%20Evaluation/Post-Training%20&%20Fine%20Tuning/LLM%20Adaptation%20&%20Alignment%20Tuning/LLM%20Adaptation%20&%20Alignment%20Tuning.md)
- â†— [LLM Utilization & Prompt Engineering](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/LLM%20Utilization%20&%20Prompt%20Engineering.md) (In-Context Learning)
	- â†— [CoT (Chain-of-Thought)](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/CoT%20(Chain-of-Thought).md)
	- â†— [RAG (Retrieval Augmented Generation)](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/RAG%20(Retrieval%20Augmented%20Generation).md)
	- â†— [Context Engineering & ICL (In-Context Learning)](LLM%20Training,%20Utilization,%20and%20Evaluation/LLM%20Utilization%20&%20Prompt%20Engineering/Context%20Engineering%20&%20ICL%20(In-Context%20Learning).md)


### LLM Infrastructure & Deployment
â†— [LLM Infrastructure & Deployment](LLM%20Infrastructure%20&%20Deployment/LLM%20Infrastructure%20&%20Deployment.md)
â†— [AI (Data) Infrastructure & Techniques Stack](../../ğŸ—ï¸%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack.md)


### LLM Applications & LLM-Driven Automation
â†— [LLM Applications & LLM-Driven Automation](ğŸš®%20LLM%20Applications%20&%20LLM-Driven%20Automation/LLM%20Applications%20&%20LLM-Driven%20Automation.md)
#### Agentic LLM and LLM OS
â†— [LLM Agents & Agentical LLM](ğŸš®%20LLM%20Applications%20&%20LLM-Driven%20Automation/ğŸ«£%20LLM%20Agents%20&%20Agentical%20LLM/LLM%20Agents%20&%20Agentical%20LLM.md)
â†— [LLM OS](ğŸš®%20LLM%20Applications%20&%20LLM-Driven%20Automation/ğŸ«£%20LLM%20Agents%20&%20Agentical%20LLM/LLM%20OS.md)
#### Artificial General Intelligence?
â†— [AGI (Artificial General Intelligence) & AIGC (AI-Generated Content)](../../AGI%20(Artificial%20General%20Intelligence)%20&%20AIGC%20(AI-Generated%20Content)/AGI%20(Artificial%20General%20Intelligence)%20&%20AIGC%20(AI-Generated%20Content).md)



## The Technical Evolution of LLM & Future Directions
> â†— [LLM Models Guide & Leaderboard](ğŸªœ%20LLM%20Models%20Guide%20&%20Leaderboard/LLM%20Models%20Guide%20&%20Leaderboard.md)

![](../../../../../Assets/Pics/Screenshot%202025-09-15%20at%2010.55.30.png)
<small>Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., â€¦ Wen, J.-R. (2025). _A Survey of Large Language Models_ (arXiv:2303.18223). arXiv. <br> <a>https://doi.org/10.48550/arXiv.2303.18223</a></small>


### â­ LLM Milestone Papers
> https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#milestone-papers (2025.01)

|  Date   |       keywords       |      Institute       | Paper                                                                                                                                                                                                              |
| :-----: | :------------------: | :------------------: | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| 2017-06 |     Transformers     |        Google        | [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf)                                                                                                                                                  |
| 2018-06 |       GPT 1.0        |        OpenAI        | [Improving Language Understanding by Generative Pre-Training](https://www.cs.ubc.ca/~amuham01/LING530/papers/radford2018improving.pdf)                                                                             |
| 2018-10 |         BERT         |        Google        | [BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding](https://aclanthology.org/N19-1423.pdf)                                                                                          |
| 2019-02 |       GPT 2.0        |        OpenAI        | [Language Models are Unsupervised Multitask Learners](https://d4mucfpksywv.cloudfront.net/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)                                          |
| 2019-09 |     Megatron-LM      |        NVIDIA        | [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf)                                                                                      |
| 2019-10 |          T5          |        Google        | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://jmlr.org/papers/v21/20-074.html)                                                                                       |
| 2019-10 |         ZeRO         |      Microsoft       | [ZeRO: Memory Optimizations Toward Training Trillion Parameter Models](https://arxiv.org/pdf/1910.02054.pdf)                                                                                                       |
| 2020-01 |     Scaling Law      |        OpenAI        | [Scaling Laws for Neural Language Models](https://arxiv.org/pdf/2001.08361.pdf)                                                                                                                                    |
| 2020-05 |       GPT 3.0        |        OpenAI        | [Language models are few-shot learners](https://papers.nips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf)                                                                                         |
| 2021-01 | Switch Transformers  |        Google        | [Switch Transformers: Scaling to Trillion Parameter Models with Simple and Efficient Sparsity](https://arxiv.org/pdf/2101.03961.pdf)                                                                               |
| 2021-08 |        Codex         |        OpenAI        | [Evaluating Large Language Models Trained on Code](https://arxiv.org/pdf/2107.03374.pdf)                                                                                                                           |
| 2021-08 |  Foundation Models   |       Stanford       | [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)                                                                                                                        |
| 2021-09 |         FLAN         |        Google        | [Finetuned Language Models are Zero-Shot Learners](https://openreview.net/forum?id=gEZrGCozdqR)                                                                                                                    |
| 2021-10 |          T0          |  HuggingFace et al.  | [Multitask Prompted Training Enables Zero-Shot Task Generalization](https://arxiv.org/abs/2110.08207)                                                                                                              |
| 2021-12 |         GLaM         |        Google        | [GLaM: Efficient Scaling of Language Models with Mixture-of-Experts](https://arxiv.org/pdf/2112.06905.pdf)                                                                                                         |
| 2021-12 |        WebGPT        |        OpenAI        | [WebGPT: Browser-assisted question-answering with human feedback](https://www.semanticscholar.org/paper/WebGPT%3A-Browser-assisted-question-answering-with-Nakano-Hilton/2f3efe44083af91cef562c1a3451eee2f8601d22) |
| 2021-12 |        Retro         |       DeepMind       | [Improving language models by retrieving from trillions of tokens](https://www.deepmind.com/publications/improving-language-models-by-retrieving-from-trillions-of-tokens)                                         |
| 2021-12 |        Gopher        |       DeepMind       | [Scaling Language Models: Methods, Analysis & Insights from Training Gopher](https://arxiv.org/pdf/2112.11446.pdf)                                                                                                 |
| 2022-01 |         COT          |        Google        | [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models](https://arxiv.org/pdf/2201.11903.pdf)                                                                                                      |
| 2022-01 |        LaMDA         |        Google        | [LaMDA: Language Models for Dialog Applications](https://arxiv.org/pdf/2201.08239.pdf)                                                                                                                             |
| 2022-01 |       Minerva        |        Google        | [Solving Quantitative Reasoning Problems with Language Models](https://arxiv.org/abs/2206.14858)                                                                                                                   |
| 2022-01 | Megatron-Turing NLG  |   Microsoft&NVIDIA   | [Using Deep and Megatron to Train Megatron-Turing NLG 530B, A Large-Scale Generative Language Model](https://arxiv.org/pdf/2201.11990.pdf)                                                                         |
| 2022-03 |     InstructGPT      |        OpenAI        | [Training language models to follow instructions with human feedback](https://arxiv.org/pdf/2203.02155.pdf)                                                                                                        |
| 2022-04 |         PaLM         |        Google        | [PaLM: Scaling Language Modeling with Pathways](https://arxiv.org/pdf/2204.02311.pdf)                                                                                                                              |
| 2022-04 |      Chinchilla      |       DeepMind       | [An empirical analysis of compute-optimal large language model training](https://arxiv.org/abs/2408.00724)                                                                                                         |
| 2022-05 |         OPT          |         Meta         | [OPT: Open Pre-trained Transformer Language Models](https://arxiv.org/pdf/2205.01068.pdf)                                                                                                                          |
| 2022-05 |         UL2          |        Google        | [Unifying Language Learning Paradigms](https://arxiv.org/abs/2205.05131v1)                                                                                                                                         |
| 2022-06 |  Emergent Abilities  |        Google        | [Emergent Abilities of Large Language Models](https://openreview.net/pdf?id=yzkSU5zdwD)                                                                                                                            |
| 2022-06 |      BIG-bench       |        Google        | [Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models](https://github.com/google/BIG-bench)                                                                                |
| 2022-06 |        METALM        |      Microsoft       | [Language Models are General-Purpose Interfaces](https://arxiv.org/pdf/2206.06336.pdf)                                                                                                                             |
| 2022-09 |       Sparrow        |       DeepMind       | [Improving alignment of dialogue agents via targeted human judgements](https://arxiv.org/pdf/2209.14375.pdf)                                                                                                       |
| 2022-10 |     Flan-T5/PaLM     |        Google        | [Scaling Instruction-Finetuned Language Models](https://arxiv.org/pdf/2210.11416.pdf)                                                                                                                              |
| 2022-10 |       GLM-130B       |       Tsinghua       | [GLM-130B: An Open Bilingual Pre-trained Model](https://arxiv.org/pdf/2210.02414.pdf)                                                                                                                              |
| 2022-11 |         HELM         |       Stanford       | [Holistic Evaluation of Language Models](https://arxiv.org/pdf/2211.09110.pdf)                                                                                                                                     |
| 2022-11 |        BLOOM         |      BigScience      | [BLOOM: A 176B-Parameter Open-Access Multilingual Language Model](https://arxiv.org/pdf/2211.05100.pdf)                                                                                                            |
| 2022-11 |      Galactica       |         Meta         | [Galactica: A Large Language Model for Science](https://arxiv.org/pdf/2211.09085.pdf)                                                                                                                              |
| 2022-12 |       OPT-IML        |         Meta         | [OPT-IML: Scaling Language Model Instruction Meta Learning through the Lens of Generalization](https://arxiv.org/pdf/2212.12017)                                                                                   |
| 2023-01 | Flan 2022 Collection |        Google        | [The Flan Collection: Designing Data and Methods for Effective Instruction Tuning](https://arxiv.org/pdf/2301.13688.pdf)                                                                                           |
| 2023-02 |        LLaMA         |         Meta         | [LLaMA: Open and Efficient Foundation Language Models](https://research.facebook.com/publications/llama-open-and-efficient-foundation-language-models/)                                                            |
| 2023-02 |       Kosmos-1       |      Microsoft       | [Language Is Not All You Need: Aligning Perception with Language Models](https://arxiv.org/abs/2302.14045)                                                                                                         |
| 2023-03 |         LRU          |       DeepMind       | [Resurrecting Recurrent Neural Networks for Long Sequences](https://arxiv.org/abs/2303.06349)                                                                                                                      |
| 2023-03 |        PaLM-E        |        Google        | [PaLM-E: An Embodied Multimodal Language Model](https://palm-e.github.io/)                                                                                                                                         |
| 2023-03 |        GPT 4         |        OpenAI        | [GPT-4 Technical Report](https://openai.com/research/gpt-4)                                                                                                                                                        |
| 2023-04 |        LLaVA         | UWâ€“Madison&Microsoft | [Visual Instruction Tuning](https://arxiv.org/abs/2304.08485)                                                                                                                                                      |
| 2023-04 |        Pythia        |  EleutherAI et al.   | [Pythia: A Suite for Analyzing Large Language Models Across Training and Scaling](https://arxiv.org/abs/2304.01373)                                                                                                |
| 2023-05 |      Dromedary       |      CMU et al.      | [Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision](https://arxiv.org/abs/2305.03047)                                                                                 |
| 2023-05 |        PaLM 2        |        Google        | [PaLM 2 Technical Report](https://ai.google/static/documents/palm2techreport.pdf)                                                                                                                                  |
| 2023-05 |         RWKV         |       Bo Peng        | [RWKV: Reinventing RNNs for the Transformer Era](https://arxiv.org/abs/2305.13048)                                                                                                                                 |
| 2023-05 |         DPO          |       Stanford       | [Direct Preference Optimization: Your Language Model is Secretly a Reward Model](https://arxiv.org/pdf/2305.18290.pdf)                                                                                             |
| 2023-05 |         ToT          |   Google&Princeton   | [Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/pdf/2305.10601.pdf)                                                                                                    |
| 2023-07 |        LLaMA2        |         Meta         | [Llama 2: Open Foundation and Fine-Tuned Chat Models](https://arxiv.org/pdf/2307.09288.pdf)                                                                                                                        |
| 2023-10 |      Mistral 7B      |       Mistral        | [Mistral 7B](https://arxiv.org/pdf/2310.06825.pdf)                                                                                                                                                                 |
| 2023-12 |        Mamba         |    CMU&Princeton     | [Mamba: Linear-Time Sequence Modeling with Selective State Spaces](https://arxiv.org/pdf/2312.00752)                                                                                                               |
| 2024-01 |     DeepSeek-v2      |       DeepSeek       | [DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model](https://arxiv.org/abs/2405.04434)                                                                                             |
| 2024-05 |        Mamba2        |    CMU&Princeton     | [Transformers are SSMs: Generalized Models and Efficient Algorithms Through Structured State Space Duality](https://arxiv.org/abs/2405.21060)                                                                      |
| 2024-05 |        Llama3        |         Meta         | [The Llama 3 Herd of Models](https://arxiv.org/abs/2407.21783)                                                                                                                                                     |
| 2024-12 |       Qwen2.5        |       Alibaba        | [Qwen2.5 Technical Report](https://arxiv.org/abs/2412.15115)                                                                                                                                                       |
| 2024-12 |     DeepSeek-V3      |       DeepSeek       | [DeepSeek-V3 Technical Report](https://arxiv.org/abs/2412.19437v1)                                                                                                                                                 |
| 2025-01 |     DeepSeek-R1      |       DeepSeek       | [DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2501.12948)                                                                                             |
|         |                      |                      |                                                                                                                                                                                                                    |


### Technical Evolution of Specific LLM Model Series
#### GPT-series Model
â†— [OpenAI ChatGPT](ğŸªœ%20LLM%20Models%20Guide%20&%20Leaderboard/OpenAI%20ChatGPT.md)

> Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., â€¦ Wen, J.-R. (2025). _A Survey of Large Language Models_ (arXiv:2303.18223). arXiv. [https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)
#### Gemini-series Model
â†— [Google Gemini](ğŸªœ%20LLM%20Models%20Guide%20&%20Leaderboard/Google%20Gemini.md)
#### LLaMA-series Model
â†— [Meta LLama](ğŸªœ%20LLM%20Models%20Guide%20&%20Leaderboard/Meta%20LLama.md)
#### Qwen-series (é€šä¹‰åƒé—®) Model
â†— [Alibaba Qwen](ğŸªœ%20LLM%20Models%20Guide%20&%20Leaderboard/Alibaba%20Qwen.md)
#### DeepSeek-series Model
â†— [DeepSeek](ğŸªœ%20LLM%20Models%20Guide%20&%20Leaderboard/DeepSeek.md)



## Ref
[ä»€ä¹ˆæ˜¯LLMå¤§è¯­è¨€æ¨¡å‹ï¼ŸLarge Language Modelï¼Œä»é‡å˜åˆ°è´¨å˜ - è‰¾å‡¡AFançš„æ–‡ç«  - çŸ¥ä¹]: https://zhuanlan.zhihu.com/p/622518771

[ğŸ‘ å¤§è¯­è¨€æ¨¡å‹è°ƒç ”æ±‡æ€» - guolipaçš„æ–‡ç«  - çŸ¥ä¹]: https://zhuanlan.zhihu.com/p/614766286

[ğŸ‘ å¤§è¯­è¨€æ¨¡å‹ç»¼è¿° | ä¸­å›½äººæ°‘å¤§å­¦é«˜ç“´äººå·¥æ™ºèƒ½å­¦é™¢]: http://ai.ruc.edu.cn/research/science/20230605100.html

[åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„æ¼æ´æ£€æµ‹æ–¹æ³•ç»¼è¿°]: https://m.fx361.com/news/2022/1215/16831048.html

[ç¬¬äºŒç¯‡ åŸºäºè‡ªç„¶è¯­è¨€å¤„ç†çš„æ¼æ´æ£€æµ‹æ–¹æ³•ç»¼è¿°]: https://blog.csdn.net/qq_55202378/article/details/127583425
[76é¡µç»¼è¿°+300ä½™ç¯‡å‚è€ƒæ–‡çŒ®ï¼Œå¤©å¤§å›¢é˜Ÿå…¨é¢ä»‹ç»å¤§è¯­è¨€æ¨¡å‹å¯¹é½æŠ€æœ¯]: https://cloud.tencent.com/developer/article/2336345

[ä»Promptæ³¨å…¥åˆ°å‘½ä»¤æ‰§è¡Œï¼šæ¢ç©¶LLMå¤§å‹è¯­è¨€æ¨¡å‹ä¸­ OpenAIçš„é£é™©ç‚¹]: https://www.secpulse.com/archives/199158.html

Prompt Injection æ˜¯ä¸€ç§æ”»å‡»æŠ€æœ¯ï¼Œé»‘å®¢æˆ–æ¶æ„æ”»å‡»è€…æ“çºµ AI æ¨¡å‹çš„è¾“å…¥å€¼ï¼Œä»¥è¯±å¯¼æ¨¡å‹è¿”å›éé¢„æœŸçš„ç»“æœã€‚è¿™é‡Œæåˆ°çš„å±äºæ˜¯SSTIæœåŠ¡ç«¯æ¨¡æ¿æ³¨å…¥ã€‚

è¿™å…è®¸æ”»å‡»è€…åˆ©ç”¨æ¨¡å‹çš„å®‰å…¨æ€§æ¥æ³„éœ²ç”¨æˆ·æ•°æ®æˆ–æ‰­æ›²æ¨¡å‹çš„è®­ç»ƒç»“æœã€‚åœ¨æŸäº›æ¨¡å‹ä¸­ï¼Œå¾ˆå¤šæƒ…å†µä¸‹è¾“å…¥æç¤ºçš„æ•°æ®ä¼šç›´æ¥æš´éœ²æˆ–å¯¹è¾“å‡ºæœ‰å¾ˆå¤§å½±å“ã€‚

[æ·±å…¥å‰–æå¤§æ¨¡å‹å®‰å…¨é—®é¢˜ï¼šLangchainæ¡†æ¶çš„éšè—é£é™© | è…¾è®¯æŠ€æœ¯å·¥ç¨‹]: https://www.secrss.com/articles/59635

[ğŸ‘ AAAI2024 | åˆ†äº«10ç¯‡ä¼˜ç§€è®ºæ–‡ï¼Œæ¶‰åŠå›¾ç¥ç»ç½‘ç»œã€å¤§æ¨¡å‹ä¼˜åŒ–ã€è¡¨æ ¼åˆ†æç­‰çƒ­é—¨è¯é¢˜]: https://mp.weixin.qq.com/s/F7X8N_wUyZQNhDtIfHm17Q

[ğŸ¤” ä»LLMä¸­å®Œå…¨æ¶ˆé™¤çŸ©é˜µä¹˜æ³•ï¼Œæ•ˆæœå‡ºå¥‡å¾—å¥½ï¼Œ10äº¿å‚æ•°è·‘åœ¨FPGAä¸Šæ¥è¿‘å¤§è„‘åŠŸè€—]: https://mp.weixin.qq.com/s/3YSega29u8hnc8CcCrNwqQ

[80 ã€çœ‹å¤§ä½¬YouTubeèƒœè¯»å››å¹´æœ¬ç§‘ - cheezit03 | å°çº¢ä¹¦ - ä½ çš„ç”Ÿæ´»å…´è¶£ç¤¾åŒºã€‘ ğŸ˜† eMXAZs7NTrAMe8a ğŸ˜† ]: https://www.xiaohongshu.com/discovery/item/67cda8e7000000002903d0f9?source=webshare&xhsshare=pc_web&xsec_token=AB-gNabAhHDfNbngCiSe41bKwI6pTghIBsMcRPTSSb1Qo=&xsec_source=pc_share

[çŸ¥è¯†å’Œå¤§è¯­è¨€æ¨¡å‹çš„é›†æˆè¶‹åŠ¿ï¼šç»¼è¿° - é»„æµ´çš„æ–‡ç«  - çŸ¥ä¹]: https://zhuanlan.zhihu.com/p/668825246

[Tracing the thoughts of a large language model | Anthropic]: https://youtu.be/Bj9BD2D3DzA?si=fcpeY9wYY5IQDd2q

QuantumBlack AI by McKinsey:Â ["The next innovation revolution - powered by AI"](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-next-innovation-revolution-powered-by-ai)
Gruber & Tal:Â [The Market Opportunity Navigator](https://wheretoplay.co/the-navigator/)Â ,Â [PDF worksheet](https://drive.google.com/file/d/1eBUlii5EfLJM0Fc9JRk3Iy7DR6K0eykS/view?usp=share_link)

[(1hr Talk) Intro to Large Language Models | Andrej Karpathy]: https://youtu.be/zjkBMFhNj_g?si=G546Rtz9r9hc233z