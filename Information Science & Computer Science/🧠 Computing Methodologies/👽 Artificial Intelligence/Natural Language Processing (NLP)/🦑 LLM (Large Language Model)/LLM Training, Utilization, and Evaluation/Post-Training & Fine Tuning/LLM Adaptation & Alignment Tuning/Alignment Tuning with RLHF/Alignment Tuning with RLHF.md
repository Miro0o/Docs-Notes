# Alignment Tuning with RLHF

[TOC]



## Res
### Related Topics
â†— [RLHF (Reinforcement learning from Human Feedback)](../../../../../../ğŸ—ï¸%20AI%20Basics%20&%20Machine%20Learning/ğŸ“Š%20Statistical%20Learning%20Theory/ğŸ—¿%20Types%20of%20Classic%20ML%20Tasks%20&%20Statistical%20Machine%20Learning%20Methods/Reinforcement%20Learning/RLHF%20(Reinforcement%20learning%20from%20Human%20Feedback).md)
â†— [SFT (Supervised Fine Tuning)](../SFT%20(Supervised%20Fine%20Tuning)/SFT%20(Supervised%20Fine%20Tuning).md)



## Intro
> ğŸ”— https://aws.amazon.com/what-is/reinforcement-learning-from-human-feedback/

Reinforcement learning from human feedback (RLHF) is a machine learning (ML) technique that uses human feedback to optimize ML models to self-learn more efficiently. Reinforcement learning (RL) techniques train software to make decisions that maximize rewards, making their outcomes more accurate. RLHF incorporates human feedback in the rewards function, so the ML model can perform tasks more aligned with human goals, wants, and needs. RLHF is used throughout generative artificial intelligence (generative AI) applications, including in large language models (LLM).

![](../../../../../../../../../Assets/Pics/Pasted%20image%2020240520132208.png)

> Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., â€¦ Wen, J.-R. (2025). _A Survey of Large Language Models_ (arXiv:2303.18223). arXiv. [https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)

To align LLMs with human values, reinforcement learning from human feedback (RLHF) [79, 365] has been proposed to fine-tune LLMs with the collected human feedback data, which is useful to improve the alignment criteria (e.g., helpfulness, honesty, and harmlessness). RLHF employs reinforcement learning (RL) algorithms (e.g., Proximal Policy Optimization (PPO) [128]) to adapt LLMs to human feedback by learning a reward model. Such an approach incorporates humans in the training loop for developing well-aligned LLMs, as exemplified by InstructGPT [66].  

The RLHF system mainly comprises three key components: a pre-trained LM to be aligned, a reward model learning from human feedback, and a RL algorithm training the LM. Specifically, the pre-trained LM is typically a generative model that is initialized with existing pretrained LM parameters. For example, OpenAI uses 175B GPT-3 for its first popular RLHF model, InstructGPT [66],  and DeepMind uses the 280 billion parameter model Gopher [64] for its GopherCite model [368]. Further, the reward model (RM) provides (learned) guidance signals that reflect human preferences for the text generated by the LM, usually in the form of a scalar value. The reward model can take on two forms: a fine-tuned LM or a LM trained de novo using human preference data. Existing work typically employs reward models having a parameter scale different from that of the aligned LM [66, 368]. For example, OpenAI uses 6B GPT-3 and DeepMind uses 7B Gopher as the reward model, respectively. Finally, to optimize the pre-trained LM using the signal from the reward model, a specific RL algorithm is designed for large-scale model tuning. Specifically, **Proximal Policy Optimization (PPO)** [128] is a widely used RL algorithm for alignment in existing work [66, 116, 368].

![|600](../../../../../../../../../Assets/Pics/Screenshot%202025-09-15%20at%2013.40.34.png)


### Relations Between RLHF and SFT in LLM Alignment Tuning



## Ref
[ğŸ‘ Understanding and Using Supervised Fine-Tuning (SFT) for Language Models]: https://cameronrwolfe.substack.com/p/understanding-and-using-supervised

![](../../../../../../../../../Assets/Pics/Pasted%20image%2020240602204501.png)

![](../../../../../../../../../Assets/Pics/Pasted%20image%2020240602204515.png)

[ğŸ‘ Fine-Tuning, PEFT, Prompt Engineering, and RAG]: https://deci.ai/blog/fine-tuning-peft-prompt-engineering-and-rag-which-one-is-right-for-you/



[ğŸ‘ Illustrating Reinforcement Learning from Human Feedback (RLHF) | Hugging Face]: https://huggingface.co/blog/rlhf

![](../../../../../../../../../Assets/Pics/Pasted%20image%2020240520132243.png)

![](../../../../../../../../../Assets/Pics/Pasted%20image%2020240520132257.png)

[ğŸ‘ 2023å¹´ç¥ç§˜è€Œéš¾ä»¥ç†è§£çš„å¤§æ¨¡å‹å¼ºåŒ–å­¦ä¹ æŠ€æœ¯ï¼šRLHF PPOï¼ŒDPOï¼Œä»¥åŠInstructGPTï¼ŒDeepSpeed-Chatï¼Œ LLama2ï¼ŒBaichuan2çš„RLHF - æ˜¯å¿µçš„æ–‡ç«  - çŸ¥ä¹]: https://zhuanlan.zhihu.com/p/662753985

![](../../../../../../../../../Assets/Pics/Pasted%20image%2020240605224024.png)

