# Low-Rank Adaptation (LoRA)

[TOC]



## Res
### Related Topics



## Intro



## QLoRA
🏠 
🚧 https://github.com/artidoro/qlora?tab=readme-ov-file
📄 https://arxiv.org/abs/2305.14314 (paper)
🤗 https://huggingface.co/timdettmers (Adapter Weights)
🚗 https://huggingface.co/timdettmers (demo)


"QLoRA: Efficient Finetuning of Quantized LLMs", an effort to democratize access to LLM research.

QLoRA uses [bitsandbytes](https://github.com/TimDettmers/bitsandbytes) for quantization and is integrated with Hugging Face's [PEFT](https://github.com/huggingface/peft) and [transformers](https://github.com/huggingface/transformers/) libraries. QLoRA was developed by members of the [University of Washington's UW NLP group](https://twitter.com/uwnlp?s=20).



## Ref
