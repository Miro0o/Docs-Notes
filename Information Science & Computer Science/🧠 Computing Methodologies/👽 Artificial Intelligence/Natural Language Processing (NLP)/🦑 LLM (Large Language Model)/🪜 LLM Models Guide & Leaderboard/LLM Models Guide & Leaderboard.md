# LLM Models Guide & Leaderboard

[TOC]



## Res
### Related Topics
↗ [Transformers](../../../🗝️%20AI%20Basics%20&%20Machine%20Learning/📌%20Deep%20Learning%20(Neural%20Network)/2️⃣%20Neural%20Network%20Models%20🗿/Transformers/Transformers.md)


### Leaderboards
https://lmarena.ai/
Chatbot Arena LLM Leaderboard: Community-driven Evaluation for Best LLM and AI chatbots
- Chatbot Arena is an open platform for crowdsourced AI benchmarking, hosted by researchers at UC Berkeley [SkyLab](https://sky.cs.berkeley.edu/) and [LMArena](https://blog.lmarena.ai/about/). We open-source the [FastChat](https://github.com/lm-sys/FastChat) project at GitHub and release open datasets. We always welcome contributions from the community. If you're interested in collaboration, we'd love to hear from you!

https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard#/
Open LLM Leaderboard

https://artificialanalysis.ai/leaderboards/models
Comparison and ranking the performance of over 30 AI models (LLMs) across key metrics including quality, price, performance and speed (output speed - tokens per second & latency - TTFT), context window & others. For more details including relating to our methodology, see our [FAQs.](https://artificialanalysis.ai/faq)

https://tatsu-lab.github.io/alpaca_eval/

https://www.vellum.ai/llm-leaderboard

https://livebench.ai/#/

https://llm-stats.com/

https://scale.com/leaderboard

https://gorilla.cs.berkeley.edu/leaderboard.html
BFCL: From Tool Use to Agentic Evaluation of Large Language Models
The Berkeley Function Calling Leaderboard (BFCL) V4 evaluates the LLM's ability to call functions (aka tools) accurately. This leaderboard consists of real-world data and will be updated periodically. For more information on the evaluation dataset and methodology, please refer to our blogs: [BFCL-v1](https://gorilla.cs.berkeley.edu/blogs/8_berkeley_function_calling_leaderboard.html) introducing AST as an evaluation metric, [BFCL-v2](https://gorilla.cs.berkeley.edu/blogs/12_bfcl_v2_live.html) introducing enterprise and OSS-contributed functions, [BFCL-v3](https://gorilla.cs.berkeley.edu/blogs/13_bfcl_v3_multi_turn.html) introducing multi-turn interactions, and [BFCL-v4](https://gorilla.cs.berkeley.edu/blogs/15_bfcl_v4_web_search.html) introducing holistic agentic evaluation. Checkout [code and data](https://github.com/ShishirPatil/gorilla/tree/main/berkeley-function-call-leaderboard).

https://aider.chat/docs/leaderboards/



## A list of popular Large Language Models
> 🔗 https://github.com/Hannibal046/Awesome-LLM/tree/main (2024.06)

- [Gemma](https://blog.google/technology/developers/gemma-open-models/) - Gemma is built for responsible AI development from the same research and technology used to create Gemini models.
- [Mistral](https://mistral.ai/) - Mistral-7B-v0.1 is a small, yet powerful model adaptable to many use-cases including code and 8k sequence length. Apache 2.0 licence.
- [Mixtral 8x7B](https://mistral.ai/news/mixtral-of-experts/) - a high-quality sparse mixture of experts model (SMoE) with open weights.
- [LLaMA](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/) & [LLaMA-2](https://ai.meta.com/llama/) - A foundational large language model. [LLaMA.cpp](https://github.com/ggerganov/llama.cpp) [Lit-LLaMA](https://github.com/Lightning-AI/lit-llama)
    - [Alpaca](https://crfm.stanford.edu/2023/03/13/alpaca.html) - A model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. [Alpaca.cpp](https://github.com/antimatter15/alpaca.cpp) [Alpaca-LoRA](https://github.com/tloen/alpaca-lora)
    - [Flan-Alpaca](https://github.com/declare-lab/flan-alpaca) - Instruction Tuning from Humans and Machines.
    - [Baize](https://github.com/project-baize/baize-chatbot) - Baize is an open-source chat model trained with [LoRA](https://github.com/microsoft/LoRA). It uses 100k dialogs generated by letting ChatGPT chat with itself.
    - [Cabrita](https://github.com/22-hours/cabrita) - A portuguese finetuned instruction LLaMA.
    - [Vicuna](https://lmsys.org/blog/2023-03-30-vicuna/) - An Open-Source Chatbot Impressing GPT-4 with 90% ChatGPT Quality.
    - [Llama-X](https://github.com/AetherCortex/Llama-X) - Open Academic Research on Improving LLaMA to SOTA LLM.
    - [Chinese-Vicuna](https://github.com/Facico/Chinese-Vicuna) - A Chinese Instruction-following LLaMA-based Model.
    - [GPTQ-for-LLaMA](https://github.com/qwopqwop200/GPTQ-for-LLaMa) - 4 bits quantization of [LLaMA](https://arxiv.org/abs/2302.13971) using [GPTQ](https://arxiv.org/abs/2210.17323).
    - [GPT4All](https://github.com/nomic-ai/gpt4all) - Demo, data, and code to train open-source assistant-style large language model based on GPT-J and LLaMa.
    - [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/) - A Dialogue Model for Academic Research
    - [BELLE](https://github.com/LianjiaTech/BELLE) - Be Everyone's Large Language model Engine
    - [StackLLaMA](https://huggingface.co/blog/stackllama) - A hands-on guide to train LLaMA with RLHF.
    - [RedPajama](https://github.com/togethercomputer/RedPajama-Data) - An Open Source Recipe to Reproduce LLaMA training dataset.
    - [Chimera](https://github.com/FreedomIntelligence/LLMZoo) - Latin Phoenix.
    - [WizardLM|WizardCoder](https://github.com/nlpxucan/WizardLM) - Family of instruction-following LLMs powered by Evol-Instruct: WizardLM, WizardCoder.
    - [CaMA](https://github.com/zjunlp/CaMA) - a Chinese-English Bilingual LLaMA Model.
    - [Orca](https://aka.ms/orca-lm) - Microsoft's finetuned LLaMA model that reportedly matches GPT3.5, finetuned against 5M of data, ChatGPT, and GPT4
    - [BayLing](https://github.com/ictnlp/BayLing) - an English/Chinese LLM equipped with advanced language alignment, showing superior capability in English/Chinese generation, instruction following and multi-turn interaction.
    - [UltraLM](https://github.com/thunlp/UltraChat) - Large-scale, Informative, and Diverse Multi-round Chat Models.
    - [Guanaco](https://github.com/artidoro/qlora) - QLoRA tuned LLaMA
    - [ChiMed-GPT](https://github.com/synlp/ChiMed-GPT) - A Chinese medical large language model.
    - [RAFT](https://aka.ms/raft-blog) - RAFT: A new way to teach LLMs to be better at RAG ([paper](https://arxiv.org/abs/2403.10131)).
    - [Gorilla LLM](https://github.com/ShishirPatil/gorilla) - Gorilla: Large Language Model Connected with Massive APIs
    - [LLaVa](https://github.com/haotian-liu/LLaVA) - LLaVA: Large Language and Vision Assistant, an end-to-end trained large multimodal model that connects a vision encoder and LLM for general-purpose visual and language understanding.
- [BLOOM](https://huggingface.co/bigscience/bloom) - BigScience Large Open-science Open-access Multilingual Language Model [BLOOM-LoRA](https://github.com/linhduongtuan/BLOOM-LORA)
    - [BLOOMZ&mT0](https://huggingface.co/bigscience/bloomz) - a family of models capable of following human instructions in dozens of languages zero-shot.
    - [Phoenix](https://github.com/FreedomIntelligence/LLMZoo)
- [Deepseek](https://github.com/deepseek-ai/)
    - [Coder](https://github.com/deepseek-ai/DeepSeek-Coder) - Let the Code Write Itself.
    - [LLM](https://github.com/deepseek-ai/DeepSeek-LLM) - Let there be answers.
    - 知名私募巨头幻方量化旗下的人工智能公司深度求索（DeepSeek）自主研发的大语言模型开发的智能助手。包括 [7B-base](https://modelscope.cn/models/deepseek-ai/deepseek-llm-7b-base/summary), [67B-base](https://modelscope.cn/models/deepseek-ai/deepseek-llm-67b-base/summary),
- [Yi](https://github.com/01-ai/Yi) - A series of large language models trained from scratch by developers @01-ai.
- [T5](https://arxiv.org/abs/1910.10683) - Text-to-Text Transfer Transformer
    - [T0](https://arxiv.org/abs/2110.08207) - Multitask Prompted Training Enables Zero-Shot Task Generalization
- [OPT](https://arxiv.org/abs/2205.01068) - Open Pre-trained Transformer Language Models.
- [UL2](https://arxiv.org/abs/2205.05131v1) - a unified framework for pretraining models that are universally effective across datasets and setups.
- [GLM](https://github.com/THUDM/GLM)- GLM is a General Language Model pretrained with an autoregressive blank-filling objective and can be finetuned on various natural language understanding and generation tasks.
    - [ChatGLM-6B](https://github.com/THUDM/ChatGLM-6B) - ChatGLM-6B 是一个开源的、支持中英双语的对话语言模型，基于 [General Language Model (GLM)](https://github.com/THUDM/GLM) 架构，具有 62 亿参数.
    - [ChatGLM2-6B](https://github.com/THUDM/ChatGLM2-6B) - An Open Bilingual Chat LLM | 开源双语对话语言模型
    - [ChatGLM3-6B](https://github.com/THUDM/ChatGLM3) - An Open Bilingual Chat LLMs | 开源双语对话语言模型 ; Including [ChatGLM3-6B-32k](https://huggingface.co/THUDM/chatglm3-6b-32k), [ChatGLM3-6B-128k](https://huggingface.co/THUDM/chatglm3-6b-128k).
- [RWKV](https://github.com/BlinkDL/RWKV-LM) - Parallelizable RNN with Transformer-level LLM Performance.
    - [ChatRWKV](https://github.com/BlinkDL/ChatRWKV) - ChatRWKV is like ChatGPT but powered by my RWKV (100% RNN) language model.
    - [Trending Demo](https://huggingface.co/spaces/BlinkDL/RWKV-Gradio-2) - RWKV-5 trained on 100+ world languages (70% English, 15% multilang, 15% code).
- [StableLM](https://stability.ai/blog/stability-ai-launches-the-first-of-its-stablelm-suite-of-language-models) - Stability AI Language Models.
- [YaLM](https://medium.com/yandex/yandex-publishes-yalm-100b-its-the-largest-gpt-like-neural-network-in-open-source-d1df53d0e9a6) - a GPT-like neural network for generating and processing text. It can be used freely by developers and researchers from all over the world.
- [GPT-Neo](https://github.com/EleutherAI/gpt-neo) - An implementation of model & data parallel [GPT3](https://arxiv.org/abs/2005.14165)-like models using the [mesh-tensorflow](https://github.com/tensorflow/mesh) library.
- [GPT-J](https://github.com/kingoflolz/mesh-transformer-jax/#gpt-j-6b) - A 6 billion parameter, autoregressive text generation model trained on [The Pile](https://pile.eleuther.ai/).
    - [Dolly](https://www.databricks.com/blog/2023/03/24/hello-dolly-democratizing-magic-chatgpt-open-models.html) - a cheap-to-build LLM that exhibits a surprising degree of the instruction following capabilities exhibited by ChatGPT.
- [Pythia](https://github.com/EleutherAI/pythia) - Interpreting Autoregressive Transformers Across Time and Scale
    - [Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm) - the first open source, instruction-following LLM, fine-tuned on a human-generated instruction dataset licensed for research and commercial use.
- [OpenFlamingo](https://github.com/mlfoundations/open_flamingo) - an open-source reproduction of DeepMind's Flamingo model.
- [Cerebras-GPT](https://www.cerebras.net/blog/cerebras-gpt-a-family-of-open-compute-efficient-large-language-models/) - A Family of Open, Compute-efficient, Large Language Models.
- [GALACTICA](https://github.com/paperswithcode/galai/blob/main/docs/model_card.md) - The GALACTICA models are trained on a large-scale scientific corpus.
    - [GALPACA](https://huggingface.co/GeorgiaTechResearchInstitute/galpaca-30b) - GALACTICA 30B fine-tuned on the Alpaca dataset.
- [Palmyra](https://huggingface.co/Writer/palmyra-base) - Palmyra Base was primarily pre-trained with English text.
- [Camel](https://huggingface.co/Writer/camel-5b-hf) - a state-of-the-art instruction-following large language model designed to deliver exceptional performance and versatility.
- [h2oGPT](https://github.com/h2oai/h2ogpt)
- [PanGu-α](https://openi.org.cn/pangu/) - PanGu-α is a 200B parameter autoregressive pretrained Chinese language model develped by Huawei Noah's Ark Lab, MindSpore Team and Peng Cheng Laboratory.
- [MOSS](https://github.com/OpenLMLab/MOSS) - MOSS是一个支持中英双语和多种插件的开源对话语言模型.
- [Open-Assistant](https://github.com/LAION-AI/Open-Assistant) - a project meant to give everyone access to a great chat based large language model.
    - [HuggingChat](https://huggingface.co/chat/) - Powered by Open Assistant's latest model – the best open source chat model right now and @huggingface Inference API.
- [StarCoder](https://huggingface.co/blog/starcoder) - Hugging Face LLM for Code
- [MPT-7B](https://www.mosaicml.com/blog/mpt-7b) - Open LLM for commercial use by MosaicML
- [Falcon](https://falconllm.tii.ae/) - Falcon LLM is a foundational large language model (LLM) with 40 billion parameters trained on one trillion tokens. TII has now released Falcon LLM – a 40B model.
- [XGen](https://github.com/salesforce/xgen) - Salesforce open-source LLMs with 8k sequence length.
- [Baichuan](https://github.com/baichuan-inc) - A series of large language models developed by Baichuan Intelligent Technology.
- [Aquila](https://github.com/FlagAI-Open/FlagAI/tree/master/examples/Aquila) - 悟道·天鹰语言大模型是首个具备中英双语知识、支持商用许可协议、国内数据合规需求的开源语言大模型。
- [phi-1](https://arxiv.org/abs/2306.11644) - a new large language model for code, with significantly smaller size than competing models.
- [phi-1.5](https://arxiv.org/abs/2309.05463) - a 1.3 billion parameter model trained on a dataset of 30 billion tokens, which achieves common sense reasoning benchmark results comparable to models ten times its size that were trained on datasets more than ten times larger.
- [phi-2](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) - a 2.7 billion-parameter language model that demonstrates outstanding reasoning and language understanding capabilities, showcasing state-of-the-art performance among base language models with less than 13 billion parameters.
- [InternLM / 书生·浦语](https://github.com/InternLM/InternLM) - Official release of InternLM2 7B and 20B base and chat models. 200K context support. [Homepage](https://internlm.intern-ai.org.cn/) | [ModelScope](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-7b/summary)
- [BlueLM-7B](https://github.com/vivo-ai-lab/BlueLM) - BlueLM(蓝心大模型): Open large language models developed by vivo AI Lab. [Homepage](https://developers.vivo.com/product/ai/bluelm) | [ModelScope](https://modelscope.cn/models/vivo-ai/BlueLM-7B-Base/summary) [MoE-16B-base](https://modelscope.cn/models/deepseek-ai/deepseek-moe-16b-base), 等. | [Chat with DeepSeek (Beta)](https://chat.deepseek.com/sign_in)
- [Qwen series](https://huggingface.co/Qwen) - The large language model series proposed by Alibaba Cloud. ｜ 阿里云研发的通义千问大模型系列. 包括 [7B](https://huggingface.co/Qwen/Qwen-7B), [72B](https://huggingface.co/Qwen/Qwen-72B), 及各种量化和Chat版本. [Chat Demo](https://huggingface.co/spaces/Qwen/Qwen-72B-Chat-Demo)
- [XVERSE series](https://github.com/xverse-ai) - Multilingual large language model developed by XVERSE Technology Inc | 由深圳元象科技自主研发的支持多语言的大语言模型. 包括[7B](https://github.com/xverse-ai/XVERSE-7B), [13B](https://github.com/xverse-ai/XVERSE-13B), [65B](https://github.com/xverse-ai/XVERSE-65B)等.
- [Skywork series](https://github.com/SkyworkAI/Skywork) - A series of large models developed by the Kunlun Group · Skywork team | 昆仑万维集团·天工团队开发的一系列大型模型.
- [Command-R series](https://huggingface.co/CohereForAI) - Two multilingual large language models intended for retrieval augmented generation (RAG) and conversational use, at [35](https://huggingface.co/CohereForAI/c4ai-command-r-v01) and [104](https://huggingface.co/CohereForAI/c4ai-command-r-plus) billion parameters. 128k context support.
- [Jamba](https://huggingface.co/ai21labs/Jamba-v0.1) - A Hybrid Transformer-Mamba MoE model, with 52B params, first production grade mamba based LLM, 256K context support.



## A list of open LLM models
> 🔗 https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#open-llm (2025.09)

DeepSeek
- [DeepSeek-Math-7B](https://huggingface.co/collections/deepseek-ai/deepseek-math-65f2962739da11599e441681)
- [DeepSeek-Coder-1.3|6.7|7|33B](https://huggingface.co/collections/deepseek-ai/deepseek-coder-65f295d7d8a0a29fe39b4ec4)
- [DeepSeek-VL-1.3|7B](https://huggingface.co/collections/deepseek-ai/deepseek-vl-65f295948133d9cf92b706d3)
- [DeepSeek-MoE-16B](https://huggingface.co/collections/deepseek-ai/deepseek-moe-65f29679f5cf26fe063686bf)
- [DeepSeek-v2-236B-MoE](https://arxiv.org/abs/2405.04434)
- [DeepSeek-Coder-v2-16|236B-MOE](https://github.com/deepseek-ai/DeepSeek-Coder-V2)
- [DeepSeek-V2.5](https://huggingface.co/deepseek-ai/DeepSeek-V2.5)
- [DeepSeek-V3](https://github.com/deepseek-ai/DeepSeek-V3)
- [DeepSeek-R1](https://github.com/deepseek-ai/DeepSeek-R1)

Alibaba
- [Qwen-1.8B|7B|14B|72B](https://huggingface.co/collections/Qwen/qwen-65c0e50c3f1ab89cb8704144)
- [Qwen1.5-0.5B|1.8B|4B|7B|14B|32B|72B|110B|MoE-A2.7B](https://qwenlm.github.io/blog/qwen1.5/)
- [Qwen2-0.5B|1.5B|7B|57B-A14B-MoE|72B](https://qwenlm.github.io/blog/qwen2)
- [Qwen2.5-0.5B|1.5B|3B|7B|14B|32B|72B](https://qwenlm.github.io/blog/qwen2.5/)
- [CodeQwen1.5-7B](https://qwenlm.github.io/blog/codeqwen1.5/)
- [Qwen2.5-Coder-1.5B|7B|32B](https://qwenlm.github.io/blog/qwen2.5-coder/)
- [Qwen2-Math-1.5B|7B|72B](https://qwenlm.github.io/blog/qwen2-math/)
- [Qwen2.5-Math-1.5B|7B|72B](https://qwenlm.github.io/blog/qwen2.5-math/)
- [Qwen-VL-7B](https://huggingface.co/Qwen/Qwen-VL)
- [Qwen2-VL-2B|7B|72B](https://qwenlm.github.io/blog/qwen2-vl/)
- [Qwen2-Audio-7B](https://qwenlm.github.io/blog/qwen2-audio/)
- [Qwen2.5-VL-3|7|72B](https://qwenlm.github.io/blog/qwen2.5-vl/)
- [Qwen2.5-1M-7|14B](https://qwenlm.github.io/blog/qwen2.5-1m/)

Meta
- [Llama 3.2-1|3|11|90B](https://llama.meta.com/)
- [Llama 3.1-8|70|405B](https://llama.meta.com/)
- [Llama 3-8|70B](https://llama.meta.com/llama3/)
- [Llama 2-7|13|70B](https://llama.meta.com/llama2/)
- [Llama 1-7|13|33|65B](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/)
- [OPT-1.3|6.7|13|30|66B](https://arxiv.org/abs/2205.01068)

Mistral AI
- [Codestral-7|22B](https://mistral.ai/news/codestral/)
- [Mistral-7B](https://mistral.ai/news/announcing-mistral-7b/)
- [Mixtral-8x7B](https://mistral.ai/news/mixtral-of-experts/)
- [Mixtral-8x22B](https://mistral.ai/news/mixtral-8x22b/)

Google
- [Gemma2-9|27B](https://blog.google/technology/developers/google-gemma-2/)
- [Gemma-2|7B](https://blog.google/technology/developers/gemma-open-models/)
- [RecurrentGemma-2B](https://github.com/google-deepmind/recurrentgemma)
- [T5](https://arxiv.org/abs/1910.10683)

Apple
- [OpenELM-1.1|3B](https://huggingface.co/apple/OpenELM)

Microsoft
- [Phi1-1.3B](https://huggingface.co/microsoft/phi-1)
- [Phi2-2.7B](https://huggingface.co/microsoft/phi-2)
- [Phi3-3.8|7|14B](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)

AllenAI
- [OLMo-7B](https://huggingface.co/collections/allenai/olmo-suite-65aeaae8fe5b6b2122b46778)

xAI
- [Grok-1-314B-MoE](https://x.ai/blog/grok-os)

Cohere
- [Command R-35B](https://huggingface.co/CohereForAI/c4ai-command-r-v01)

01-ai
- [Yi-34B](https://huggingface.co/collections/01-ai/yi-2023-11-663f3f19119ff712e176720f)
- [Yi1.5-6|9|34B](https://huggingface.co/collections/01-ai/yi-15-2024-05-663f3ecab5f815a3eaca7ca8)
- [Yi-VL-6B|34B](https://huggingface.co/collections/01-ai/yi-vl-663f557228538eae745769f3)

Baichuan
- [Baichuan-7|13B](https://huggingface.co/baichuan-inc)
- [Baichuan2-7|13B](https://huggingface.co/baichuan-inc)

Nvidia
- [Nemotron-4-340B](https://huggingface.co/nvidia/Nemotron-4-340B-Instruct)

BLOOM
- [BLOOMZ&mT0](https://huggingface.co/bigscience/bloomz)

Zhipu AI
- [GLM-2|6|10|13|70B](https://huggingface.co/THUDM)
- [CogVLM2-19B](https://huggingface.co/collections/THUDM/cogvlm2-6645f36a29948b67dc4eef75)

OpenBMB
- [MiniCPM-2B](https://huggingface.co/collections/openbmb/minicpm-2b-65d48bf958302b9fd25b698f)
- [OmniLLM-12B](https://huggingface.co/openbmb/OmniLMM-12B)
- [VisCPM-10B](https://huggingface.co/openbmb/VisCPM-Chat)
- [CPM-Bee-1|2|5|10B](https://huggingface.co/collections/openbmb/cpm-bee-65d491cc84fc93350d789361)

RWKV Foundation
- [RWKV-v4|5|6](https://huggingface.co/RWKV)minicpm-2b-65d48bf958302b9fd25b698f)

ElutherAI
- [Pythia-1|1.4|2.8|6.9|12B](https://github.com/EleutherAI/pythia)

Stability AI
- [StableLM-3B](https://huggingface.co/stabilityai/stablelm-3b-4e1t)
- [StableLM-v2-1.6B](https://huggingface.co/stabilityai/stablelm-2-1_6b)
- [StableLM-v2-12B](https://huggingface.co/stabilityai/stablelm-2-12b)
- [StableCode-3B](https://huggingface.co/collections/stabilityai/stable-code-64f9dfb4ebc8a1be0a3f7650)

BigCode
- [StarCoder-1|3|7B](https://huggingface.co/collections/bigcode/%E2%AD%90-starcoder-64f9bd5740eb5daaeb81dbec)
- [StarCoder2-3|7|15B](https://huggingface.co/collections/bigcode/starcoder2-65de6da6e87db3383572be1a)

DataBricks
- [MPT-7B](https://www.databricks.com/blog/mpt-7b)
- [DBRX-132B-MoE](https://www.databricks.com/blog/introducing-dbrx-new-state-art-open-llm)

Shanghai AI Laboratory
- [InternLM2-1.8|7|20B](https://huggingface.co/collections/internlm/internlm2-65b0ce04970888799707893c)
- [InternLM-Math-7B|20B](https://huggingface.co/collections/internlm/internlm2-math-65b0ce88bf7d3327d0a5ad9f)
- [InternLM-XComposer2-1.8|7B](https://huggingface.co/collections/internlm/internlm-xcomposer2-65b3706bf5d76208998e7477)
- [InternVL-2|6|14|26](https://huggingface.co/collections/OpenGVLab/internvl-65b92d6be81c86166ca0dde4)

Moonshot AI
- [Moonlight-A3B](https://huggingface.co/collections/moonshotai/moonlight-a3b-67f67b029cecfdce34f4dc23)
- [Kimi-VL-A3B](https://huggingface.co/collections/moonshotai/kimi-vl-a3b-67f67b6ac91d3b03d382dd85)
- [Kimi-K2](https://huggingface.co/collections/moonshotai/kimi-k2-6871243b990f2af5ba60617d)



## LLM Data

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-data)

> Reference: [LLMDataHub](https://github.com/Zjh-819/LLMDataHub)

- [IBM data-prep-kit](https://github.com/IBM/data-prep-kit) - Open-Source Toolkit for Efficient Unstructured Data Processing with Pre-built Modules and Local to Cluster Scalability.
- [Datatrove](https://github.com/huggingface/datatrove) - Freeing data processing from scripting madness by providing a set of platform-agnostic customizable pipeline processing blocks.
- [Dingo](https://github.com/DataEval/dingo) - Dingo: A Comprehensive Data Quality Evaluation Tool
- [FastDatasets](https://github.com/ZhuLinsen/FastDatasets) - A powerful tool for creating high-quality training datasets for Large Language Models

## LLM Evaluation:

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-evaluation)

- [lm-evaluation-harness](https://github.com/EleutherAI/lm-evaluation-harness) - A framework for few-shot evaluation of language models.
- [lighteval](https://github.com/huggingface/lighteval) - a lightweight LLM evaluation suite that Hugging Face has been using internally.
- [simple-evals](https://github.com/openai/simple-evals) - Eval tools by OpenAI.

other evaluation frameworks

- [](https://github.com/allenai/OLMo-Eval)
- [](https://github.com/Psycoy/MixEval)
- [](https://github.com/stanford-crfm/helm)
- [](https://github.com/declare-lab/instruct-eval)
- [](https://github.com/Giskard-AI/giskard)
- [](https://www.langchain.com/langsmith)
- [](https://github.com/explodinggradients/ragas)

## LLM Training Frameworks

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-training-frameworks)

- [Meta Lingua](https://github.com/facebookresearch/lingua) - a lean, efficient, and easy-to-hack codebase to research LLMs.
- [Litgpt](https://github.com/Lightning-AI/litgpt) - 20+ high-performance LLMs with recipes to pretrain, finetune and deploy at scale.
- [nanotron](https://github.com/huggingface/nanotron) - Minimalistic large language model 3D-parallelism training.
- [DeepSpeed](https://github.com/microsoft/DeepSpeed) - DeepSpeed is a deep learning optimization library that makes distributed training and inference easy, efficient, and effective.
- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM) - Ongoing research training transformer models at scale.
- [torchtitan](https://github.com/pytorch/torchtitan) - A native PyTorch Library for large model training.

other frameworks

- [](https://github.com/microsoft/Megatron-DeepSpeed)
- [](https://github.com/pytorch/torchtune)
- [](https://github.com/alibaba/ROLL)
- [](https://github.com/volcengine/verl)
- [](https://github.com/NVIDIA/NeMo)
- [](https://github.com/hpcaitech/ColossalAI)
- [](https://github.com/OpenBMB/BMTrain)
- [](https://github.com/tensorflow/mesh)
- [](https://github.com/AI-Hypercomputer/maxtext)
- [](https://github.com/EleutherAI/gpt-neox)
- [](https://github.com/NVIDIA/TransformerEngine)
- [](https://github.com/OpenRLHF/OpenRLHF)
- [](https://huggingface.co/docs/trl/en/index)
- [](https://github.com/unslothai/unsloth)
- [](https://github.com/axolotl-ai-cloud/axolotl)

## LLM Inference

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-inference)

> Reference: [llm-inference-solutions](https://github.com/mani-kantap/llm-inference-solutions)

- [SGLang](https://github.com/sgl-project/sglang) - SGLang is a fast serving framework for large language models and vision language models.
- [vLLM](https://github.com/vllm-project/vllm) - A high-throughput and memory-efficient inference and serving engine for LLMs.
- [llama.cpp](https://github.com/ggerganov/llama.cpp) - LLM inference in C/C++.
- [ollama](https://github.com/ollama/ollama) - Get up and running with Llama 3, Mistral, Gemma, and other large language models.
- [TGI](https://huggingface.co/docs/text-generation-inference/en/index) - a toolkit for deploying and serving Large Language Models (LLMs).
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM) - Nvidia Framework for LLM Inference

other deployment tools

- [](https://github.com/NVIDIA/FasterTransformer)
- [](https://github.com/microsoft/MInference)
- [](https://github.com/turboderp/exllama)
- [](https://github.com/lm-sys/FastChat)
- [](https://github.com/EricLBuehler/mistral.rs)
- [](https://github.com/skypilot-org/skypilot)
- [](https://haystack.deepset.ai/)
- [](https://github.com/bentoml/OpenLLM)[](https://bentoml.com/)
- [](https://github.com/microsoft/DeepSpeed-MII)
- [](https://github.com/huggingface/text-embeddings-inference)
- [](https://github.com/michaelfeil/infinity)
- [](https://github.com/InternLM/lmdeploy)
- [](https://github.com/linkedin/Liger-Kernel)
- [](https://github.com/Lizonghang/prima.cpp)
- [](https://github.com/xamey/deploy-llms-with-ansible)

## LLM Applications

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-applications)

> Reference: [awesome-llm-apps](https://github.com/Shubhamsaboo/awesome-llm-apps)

- [dspy](https://github.com/stanfordnlp/dspy) - DSPy: The framework for programming—not prompting—foundation models.
- [LangChain](https://github.com/hwchase17/langchain) — A popular Python/JavaScript library for chaining sequences of language model prompts.
- [LlamaIndex](https://github.com/jerryjliu/llama_index) — A Python library for augmenting LLM apps with data.

more applications

- [](https://mlflow.org/)
    
- [](https://github.com/Dicklesworthstone/swiss_army_llama)
    
- [](https://github.com/rogeriochaves/litechain)
    
- [](https://github.com/jackmpcollins/magentic)
    
- [](https://github.com/fuergaosi233/wechat-chatgpt)
    
- [](https://github.com/typpo/promptfoo)
    
- [](https://github.com/agenta-ai/agenta)
    
- [](https://github.com/serge-chat/serge)
    
- [](https://github.com/langroid/langroid)
    
- [](https://github.com/embedchain/embedchain)
    
- [](https://github.com/comet-ml/opik)
    
- [](https://github.com/intelligentnode/IntelliServer)
    
- [](https://github.com/chatchat-space/Langchain-Chatchat)
    
- [](https://github.com/leptonai/search_with_lepton)[](https://github.com/leptonai)
    
- [](https://github.com/robocorp/robocorp)
    
- [](https://studio.tune.app/)
    
- [](https://github.com/nilsherzig/LLocalSearch)
    
- [](https://github.com/Portkey-AI/gateway)
    
- [](https://github.com/talkdai/dialog)
    
- [](https://github.com/ngxson/wllama)
    
- [](https://github.com/gpustack/gpustack)
    
- [](https://github.com/alibaba/MNN)
    
- [](https://www.camel-ai.org/)
    
- [](https://github.com/reid41/QA-Pilot)
    
- [](https://github.com/reid41/shell-pilot)
    
- [](https://github.com/Mindinventory/MindSQL)
    
- [](https://github.com/langfuse/langfuse)
    
- [](https://github.com/SylphAI-Inc/AdalFlow)
    
- [](https://github.com/microsoft/guidance)
    
- [](https://github.com/evidentlyai/evidently)
    
- [](https://docs.chainlit.io/overview)
    
- [](https://www.guardrailsai.com/docs/)
    
- [](https://github.com/microsoft/semantic-kernel)
    
- [](https://github.com/hegelai/prompttools)
    
- [](https://github.com/normal-computing/outlines)
    
- [](https://github.com/promptslab/Promptify)
    
- [](https://scale.com/spellbook)
    
- [](https://promptperfect.jina.ai/prompts)
    
- [](https://wandb.ai/site/solutions/llmops)
    
- [](https://github.com/openai/evals)
    
- [](https://www.arthur.ai/get-started)
    
- [](https://lmql.ai/)
    
- [](https://github.com/lgrammel/modelfusion)
    
- [](https://openspg.yuque.com/ndx6g9/ps5q6b/vfoi61ks3mqwygvy)
    
- [](https://github.com/llm-ui-kit/llm-ui)
    
- [](https://www.wordware.ai/)
    
- [](https://github.com/WallarooLabs)
    
- [](https://github.com/langgenius/dify)
    
- [](https://github.com/LazyAGI/LazyLLM)
    
- [](https://github.com/memfreeme/memfree)
    
- [](https://github.com/Marker-Inc-Korea/AutoRAG)
    
- [](https://github.com/epsilla-cloud)
    
- [](https://phoenix.arize.com/)
    

- [](https://github.com/longevity-genie/just-chat)
    
- [](https://github.com/splx-ai/agentic-radar)
    
- [](https://github.com/langwatch/langwatch)
    
- [](https://www.tensorzero.com/)
    

## LLM Tutorials and Courses

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-tutorials-and-courses)

- [Andrej Karpathy Series](https://www.youtube.com/@AndrejKarpathy) - My favorite!
- [Umar Jamil Series](https://www.youtube.com/@umarjamilai) - high quality and educational videos you don't want to miss.
- [Alexander Rush Series](https://rush-nlp.com/projects/) - high quality and educational materials you don't want to miss.
- [llm-course](https://github.com/mlabonne/llm-course) - Course to get into Large Language Models (LLMs) with roadmaps and Colab notebooks.
- [UWaterloo CS 886](https://cs.uwaterloo.ca/~wenhuche/teaching/cs886/) - Recent Advances on Foundation Models.
- [CS25-Transformers United](https://web.stanford.edu/class/cs25/)
- [ChatGPT Prompt Engineering](https://www.deeplearning.ai/short-courses/chatgpt-prompt-engineering-for-developers/)
- [Princeton: Understanding Large Language Models](https://www.cs.princeton.edu/courses/archive/fall22/cos597G/)
- [CS324 - Large Language Models](https://stanford-cs324.github.io/winter2022/)
- [State of GPT](https://build.microsoft.com/en-US/sessions/db3f4859-cd30-4445-a0cd-553c3304f8e2)
- [A Visual Guide to Mamba and State Space Models](https://maartengrootendorst.substack.com/p/a-visual-guide-to-mamba-and-state?utm_source=multiple-personal-recommendations-email&utm_medium=email&open=false)
- [Let's build GPT: from scratch, in code, spelled out.](https://www.youtube.com/watch?v=kCc8FmEb1nY)
- [minbpe](https://www.youtube.com/watch?v=zduSFxRajkE&t=1157s) - Minimal, clean code for the Byte Pair Encoding (BPE) algorithm commonly used in LLM tokenization.
- [femtoGPT](https://github.com/keyvank/femtoGPT) - Pure Rust implementation of a minimal Generative Pretrained Transformer.
- [Neurips2022-Foundational Robustness of Foundation Models](https://nips.cc/virtual/2022/tutorial/55796)
- [ICML2022-Welcome to the "Big Model" Era: Techniques and Systems to Train and Serve Bigger Models](https://icml.cc/virtual/2022/tutorial/18440)
- [GPT in 60 Lines of NumPy](https://jaykmody.com/blog/gpt-from-scratch/)
- [LLM‑RL‑Visualized (EN)](https://github.com/changyeyu/LLM-RL-Visualized/blob/master/src/README_EN.md) | [LLM‑RL‑Visualized (中文)](https://github.com/changyeyu/LLM-RL-Visualized) - 100+ LLM / RL Algorithm Maps📚.

## LLM Books

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-books)

- [Generative AI with LangChain: Build large language model (LLM) apps with Python, ChatGPT, and other LLMs](https://amzn.to/3GUlRng) - it comes with a [GitHub repository](https://github.com/benman1/generative_ai_with_langchain) that showcases a lot of the functionality
- [Build a Large Language Model (From Scratch)](https://www.manning.com/books/build-a-large-language-model-from-scratch) - A guide to building your own working LLM.
- [BUILD GPT: HOW AI WORKS](https://www.amazon.com/dp/9152799727?ref_=cm_sw_r_cp_ud_dp_W3ZHCD6QWM3DPPC0ARTT_1) - explains how to code a Generative Pre-trained Transformer, or GPT, from scratch.
- [Hands-On Large Language Models: Language Understanding and Generation](https://www.llm-book.com/) - Explore the world of Large Language Models with over 275 custom made figures in this illustrated guide!
- [The Chinese Book for Large Language Models](http://aibox.ruc.edu.cn/zws/index.htm) - An Introductory LLM Textbook Based on [_A Survey of Large Language Models_](https://arxiv.org/abs/2303.18223).

## Great thoughts about LLM

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#great-thoughts-about-llm)

- [Why did all of the public reproduction of GPT-3 fail?](https://jingfengyang.github.io/gpt)
- [A Stage Review of Instruction Tuning](https://yaofu.notion.site/June-2023-A-Stage-Review-of-Instruction-Tuning-f59dbfc36e2d4e12a33443bd6b2012c2)
- [LLM Powered Autonomous Agents](https://lilianweng.github.io/posts/2023-06-23-agent/)
- [Why you should work on AI AGENTS!](https://www.youtube.com/watch?v=fqVLjtvWgq8)
- [Google "We Have No Moat, And Neither Does OpenAI"](https://www.semianalysis.com/p/google-we-have-no-moat-and-neither)
- [AI competition statement](https://petergabriel.com/news/ai-competition-statement/)
- [Prompt Engineering](https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/)
- [Noam Chomsky: The False Promise of ChatGPT](https://www.nytimes.com/2023/03/08/opinion/noam-chomsky-chatgpt-ai.html)
- [Is ChatGPT 175 Billion Parameters? Technical Analysis](https://orenleung.super.site/is-chatgpt-175-billion-parameters-technical-analysis)
- [The Next Generation Of Large Language Models](https://www.notion.so/Awesome-LLM-40c8aa3f2b444ecc82b79ae8bbd2696b)
- [Large Language Model Training in 2023](https://research.aimultiple.com/large-language-model-training/)
- [How does GPT Obtain its Ability? Tracing Emergent Abilities of Language Models to their Sources](https://yaofu.notion.site/How-does-GPT-Obtain-its-Ability-Tracing-Emergent-Abilities-of-Language-Models-to-their-Sources-b9a57ac0fcf74f30a1ab9e3e36fa1dc1)
- [Open Pretrained Transformers](https://www.youtube.com/watch?v=p9IxoSkvZ-M&t=4s)
- [Scaling, emergence, and reasoning in large language models](https://docs.google.com/presentation/d/1EUV7W7X_w0BDrscDhPg7lMGzJCkeaPkGCJ3bN8dluXc/edit?pli=1&resourcekey=0-7Nz5A7y8JozyVrnDtcEKJA#slide=id.g16197112905_0_0)

## Miscellaneous

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#miscellaneous)

- [Emergent Mind](https://www.emergentmind.com/) - The latest AI news, curated & explained by GPT-4.
- [ShareGPT](https://sharegpt.com/) - Share your wildest ChatGPT conversations with one click.
- [Major LLMs + Data Availability](https://docs.google.com/spreadsheets/d/1bmpDdLZxvTCleLGVPgzoMTQ0iDP2-7v7QziPrzPdHyM/edit#gid=0)
- [500+ Best AI Tools](https://vaulted-polonium-23c.notion.site/500-Best-AI-Tools-e954b36bf688404ababf74a13f98d126)
- [Cohere Summarize Beta](https://txt.cohere.ai/summarize-beta/) - Introducing Cohere Summarize Beta: A New Endpoint for Text Summarization
- [chatgpt-wrapper](https://github.com/mmabrouk/chatgpt-wrapper) - ChatGPT Wrapper is an open-source unofficial Python API and CLI that lets you interact with ChatGPT.
- [Cursor](https://www.cursor.so/) - Write, edit, and chat about your code with a powerful AI.
- [AutoGPT](https://github.com/Significant-Gravitas/Auto-GPT) - an experimental open-source application showcasing the capabilities of the GPT-4 language model.
- [OpenAGI](https://github.com/agiresearch/OpenAGI) - When LLM Meets Domain Experts.
- [EasyEdit](https://github.com/zjunlp/EasyEdit) - An easy-to-use framework to edit large language models.
- [chatgpt-shroud](https://github.com/guyShilo/chatgpt-shroud) - A Chrome extension for OpenAI's ChatGPT, enhancing user privacy by enabling easy hiding and unhiding of chat history. Ideal for privacy during screen shares.
- [AI For Developers](https://aifordevelopers.org/) - List of AI Tools and Agents for Developers

## Contributing

[](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#contributing)

This is an active repository and your contributions are always welcome!

I will keep some pull requests open if I'm not sure if they are awesome for LLM, you could vote for them by adding 👍 to them.

---

If you have any question about this opinionated list, do not hesitate to contact me [chengxin1998@stu.pku.edu.cn](mailto:chengxin1998@stu.pku.edu.cn).

## About

Awesome-LLM: a curated list of Large Language Model

### Resources

 [Readme](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#readme-ov-file)

### License

 [CC0-1.0 license](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#CC0-1.0-1-ov-file)

### Contributing

 [Contributing](https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#contributing-ov-file)

 [Activity](https://github.com/Hannibal046/Awesome-LLM/activity)

### Stars

 [**25.1k** stars](https://github.com/Hannibal046/Awesome-LLM/stargazers)

### Watchers

 [**479** watching](https://github.com/Hannibal046/Awesome-LLM/watchers)

### Forks

 [**2.1k** forks](https://github.com/Hannibal046/Awesome-LLM/forks)

[Report repository](https://github.com/contact/report-content?content_url=https%3A%2F%2Fgithub.com%2FHannibal046%2FAwesome-LLM&report=Hannibal046+%28user%29)

## [Releases](https://github.com/Hannibal046/Awesome-LLM/releases)

No releases published

## [Packages](https://github.com/users/Hannibal046/packages?repo_name=Awesome-LLM)

No packages published  

## [Contributors129](https://github.com/Hannibal046/Awesome-LLM/graphs/contributors)

- [![@Hannibal046](https://avatars.githubusercontent.com/u/38466901?s=64&v=4)](https://github.com/Hannibal046)
- [![@zhimin-z](https://avatars.githubusercontent.com/u/8592144?s=64&v=4)](https://github.com/zhimin-z)
- [![@AaronWhy](https://avatars.githubusercontent.com/u/30172609?s=64&v=4)](https://github.com/AaronWhy)
- [![@Sumedhn97](https://avatars.githubusercontent.com/u/110320693?s=64&v=4)](https://github.com/Sumedhn97)
- [![@patrick-tssn](https://avatars.githubusercontent.com/u/38104831?s=64&v=4)](https://github.com/patrick-tssn)
- [![@SinclairCoder](https://avatars.githubusercontent.com/u/46218454?s=64&v=4)](https://github.com/SinclairCoder)
- [![@sinwang20](https://avatars.githubusercontent.com/u/106879815?s=64&v=4)](https://github.com/sinwang20)
- [![@cedricvidal](https://avatars.githubusercontent.com/u/33618?s=64&v=4)](https://github.com/cedricvidal)
- [![@pchalasani](https://avatars.githubusercontent.com/u/554347?s=64&v=4)](https://github.com/pchalasani)
- [![@romilbhardwaj](https://avatars.githubusercontent.com/u/4416605?s=64&v=4)](https://github.com/romilbhardwaj)
- [![@liyin2015](https://avatars.githubusercontent.com/u/14322677?s=64&v=4)](https://github.com/liyin2015)
- [![@guyShilo](https://avatars.githubusercontent.com/u/52055301?s=64&v=4)](https://github.com/guyShilo)
- [![@michaelfeil](https://avatars.githubusercontent.com/u/63565275?s=64&v=4)](https://github.com/michaelfeil)
- [![@lemanschik](https://avatars.githubusercontent.com/u/117739566?s=64&v=4)](https://github.com/lemanschik)

[+ 115 contributors](https://github.com/Hannibal046/Awesome-LLM/graphs/contributors)

## Footer

[](https://github.com/)© 2025 GitHub, Inc.

### Footer navigation

- [Terms](https://docs.github.com/site-policy/github-terms/github-terms-of-service)
- [Privacy](https://docs.github.com/site-policy/privacy-policies/github-privacy-statement)
- [Security](https://github.com/security)
- [Status](https://www.githubstatus.com/)
- [Community](https://github.community/)
- [Docs](https://docs.github.com/)
- [Contact](https://support.github.com/?tags=dotcom-footer)
- Manage cookies
- Do not share my personal information


## Ref

