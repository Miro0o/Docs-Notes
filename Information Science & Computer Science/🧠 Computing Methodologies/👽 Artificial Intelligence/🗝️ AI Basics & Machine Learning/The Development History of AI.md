# The Development History of AI

[TOC]



## Res
### Related Topics
↗ [History of Computing](../../History%20of%20Computing.md)



## Intro
> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#

The **history of artificial intelligence** (**[AI](https://en.wikipedia.org/wiki/Artificial_intelligence "Artificial intelligence")**) began in [antiquity](https://en.wikipedia.org/wiki/Ancient_history "Ancient history"), with myths, stories, and rumors of artificial beings endowed with intelligence or consciousness by master craftsmen. The study of logic and formal reasoning from antiquity to the present led directly to the invention of the [programmable digital computer](https://en.wikipedia.org/wiki/Computer "Computer") in the 1940s, a machine based on abstract mathematical reasoning. This device and the ideas behind it inspired scientists to begin discussing the possibility of building an [electronic brain](https://en.wikipedia.org/wiki/Electronic_brain "Electronic brain").

The field of AI research was founded at a [workshop](https://en.wikipedia.org/wiki/Dartmouth_workshop "Dartmouth workshop") held on the campus of [Dartmouth College](https://en.wikipedia.org/wiki/Dartmouth_College "Dartmouth College") in 1956. Attendees of the workshop became the leaders of AI research for decades. Many of them predicted that machines as intelligent as humans would exist within a generation. The [U.S. government](https://en.wikipedia.org/wiki/DARPA "DARPA") provided millions of dollars with the hope of making this vision come true.

Eventually, it became obvious that researchers had grossly underestimated the difficulty of this feat. In 1974, criticism from [James Lighthill](https://en.wikipedia.org/wiki/James_Lighthill "James Lighthill") and pressure from the U.S.A. Congress led the U.S. and [British Governments](https://en.wikipedia.org/wiki/British_Government "British Government") to stop funding undirected research into artificial intelligence. Seven years later, a visionary initiative by the [Japanese Government](https://en.wikipedia.org/wiki/Japanese_Government "Japanese Government") and the success of [expert systems](https://en.wikipedia.org/wiki/Expert_system "Expert system") reinvigorated investment in AI, and by the late 1980s, the industry had grown into a billion-dollar enterprise. However, investors' enthusiasm waned in the 1990s, and the field was criticized in the press and avoided by industry (a period known as an "[AI winter](https://en.wikipedia.org/wiki/AI_winter "AI winter")"). Nevertheless, research and funding continued to grow under other names.

In the early 2000s, [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning") was applied to a wide range of problems in academia and industry. The success was due to the availability of powerful computer hardware, the collection of immense data sets, and the application of solid mathematical methods. Soon after, [deep learning](https://en.wikipedia.org/wiki/Deep_learning "Deep learning") proved to be a breakthrough technology, eclipsing all other methods. The [transformer architecture](https://en.wikipedia.org/wiki/Transformer_\(deep_learning_architecture\) "Transformer (deep learning architecture)") debuted in 2017 and was used to produce impressive [generative AI](https://en.wikipedia.org/wiki/Generative_AI "Generative AI") applications, amongst other use cases.

Investment in AI [boomed](https://en.wikipedia.org/wiki/AI_boom "AI boom") in the 2020s. The recent AI boom, initiated by the development of transformer architecture, led to the rapid scaling and public releases of [large language models](https://en.wikipedia.org/wiki/Large_language_models "Large language models") (LLMs) like [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT "ChatGPT"). These models exhibit human-like traits of knowledge, attention, and creativity, and have been integrated into various sectors, fueling exponential investment in AI. However, concerns about the potential risks and [ethical implications of advanced AI](https://en.wikipedia.org/wiki/Ethics_of_artificial_intelligence "Ethics of artificial intelligence") have also emerged, causing debate about the future of AI and its impact on society.


### Review: Modern AI Fields
↗ [Academics 🎓 (In CS)](../../../Academics%20🎓%20(In%20CS)/Academics%20🎓%20(In%20CS).md)
↗ [🌲 Road To CS](../../../🗺%20CS%20Overview/💋%20Intro%20to%20Computer%20Science/🌲%20Road%20To%20CS.md)

https://en.wikipedia.org/wiki/ACM_Computing_Classification_System
https://www.acm.org/publications/class-2012
https://dl.acm.org/ccs
ACM CCS 2012
**Artificial intelligence** ✅
- <a>Natural language processing</a>
	- Information extraction
	- Machine translation
	- Discourse, dialogue and pragmatics
	- Natural language generation
	- Speech recognition
	- Lexical semantics
	- Phonology / morphology
	- Language resources
- <a>Knowledge representation and reasoning</a>
	- Description logics
	- Semantic networks
	- Nonmonotonic, default reasoning and belief revision
	- Probabilistic reasoning
	- Vagueness and fuzzy logic
	- Causal reasoning and diagnostics
	- Temporal reasoning
	- Cognitive robotics
	- Ontology engineering
	- Logic programming and answer set programming
	- Spatial and physical reasoning
	- Reasoning about belief and knowledge
- <a>Planning and scheduling</a>
	- Planning for deterministic actions
	- Planning under uncertainty
	- Multi-agent planning
	- Planning with abstraction and generalization
	- Robotic planning
		- Evolutionary robotics
- <a>Search methodologies</a>
	- Heuristic function construction
	- Discrete space search
	- Continuous space search
	- Randomized search
	- Game tree search
	- Abstraction and micro-operators
	- Search with partial observations
- <a>Control methods</a>
	- Robotic planning
		- Evolutionary robotics
	- Computational control theory
	- Motion path planning
- <a>Philosophical/theoretical foundations of artificial intelligence</a>
	- Cognitive science
	- Theory of mind
- <a>Distributed artificial intelligence</a>
	- Multi-agent systems
	- Intelligent agents
	- Mobile agents
	- Cooperation and coordination
- <a>Computer vision</a>
	- Computer vision tasks
		- Biometrics
		- Scene understanding
		- Activity recognition and understanding
		- Video summarization
		- Visual content-based indexing and retrieval
		- Visual inspection
		- Vision for robotics
		- Scene anomaly detection
	- Image and video acquisition
		- Camera calibration
		- Epipolar geometry
		- Computational photography
		- Hyperspectral imaging
		- Motion capture
		- 3D imaging
		- Active vision
	- Computer vision representations
		- Image representations
		- Shape representations
		- Appearance and texture representations
		- Hierarchical representations
	- Computer vision problems
		- Interest point and salient region detections
		- Image segmentation
		- Video segmentation
		- Shape inference
		- Object detection
		- Object recognition
		- Object identification
		- Tracking
		- Reconstruction
		- Matching
- **Machine learning** ✅
	- <a>Learning paradigms</a>
		- Supervised learning
			- Ranking
			- Learning to rank
			- Supervised learning by classification
			- Supervised learning by regression
			- Structured outputs
			- Cost-sensitive learning
		- Unsupervised learning
			- Cluster analysis
			- Anomaly detection
			- Mixture modeling
			- Topic modeling
			- Source separation
			- Motif discovery
			- Dimensionality reduction and manifold learning
		- Reinforcement learning
			- Sequential decision making
			- Inverse reinforcement learning
			- Apprenticeship learning
			- Multi-agent reinforcement learning
			- Adversarial learning
		- Multi-task learning
			- Transfer learning
			- Lifelong machine learning
			- Learning under covariate shift
	- <a>Learning settings</a>
		- Batch learning
		- Online learning settings
		- Learning from demonstrations
		- Learning from critiques
		- Learning from implicit feedback
		- Active learning settings
		- Semi-supervised learning settings
	- <a>Machine learning approaches</a>
		- Classification and regression trees
		- Kernel methods
			- Support vector machines
			- Gaussian processes
		- Neural networks
		- Logical and relational learning
			- Inductive logic learning
			- Statistical relational learning
		- Learning in probabilistic graphical models
			- Maximum likelihood modeling
			- Maximum entropy modeling
			- Maximum a posteriori modeling
			- Mixture models
			- Latent variable models
			- Bayesian network models
		- Learning linear models
			- Perceptron algorithm
		- Factorization methods
			- Non-negative matrix factorization
			- Factor analysis
			- Principal component analysis
			- Canonical correlation analysis
			- Latent Dirichlet allocation
		- Rule learning
		- Instance-based learning
		- Markov decision processes
		- Partially-observable Markov decision processes
		- Stochastic games
		- Learning latent representations
			- Deep belief networks
		- Bio-inspired approaches
			- Artificial life
			- Evolvable hardware
			- Genetic algorithms
			- Genetic programming
			- Evolutionary robotics
			- Generative and developmental approaches
	- <a>Machine learning algorithms</a>
		- Dynamic programming for Markov decision processes
			- Value iteration
			- Q-learning
			- Policy iteration
			- Temporal difference learning
			- Approximate dynamic programming methods
		- Ensemble methods
			- Boosting
			- Bagging
		- Spectral methods
		- Feature selection
		- Regularization
	- <a>Cross-validation</a>



## Precursors & Foundations
### Mythical, Fictional, and Speculative Precursors
> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Mythical,_fictional,_and_speculative_precursors


### Formal Reasoning 
↗ [Mathematical Logic](../../../🧮%20Mathematics/🤼‍♀️%20Mathematical%20Logic/Mathematical%20Logic.md)
↗ [Logic And Mechanized (Formal) Reasoning](../../../🧮%20Mathematics/🤼‍♀️%20Mathematical%20Logic/Logic%20And%20Mechanized%20(Formal)%20Reasoning.md)

> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Formal_reasoning

Artificial intelligence is based on the assumption that the process of human thought can be mechanized. The study of mechanical—or "formal"—reasoning has a long history. [Chinese](https://en.wikipedia.org/wiki/Logic_in_China "Logic in China"), [Indian](https://en.wikipedia.org/wiki/Indian_Logic "Indian Logic") and [Greek](https://en.wikipedia.org/wiki/History_of_Logic#Logic_in_the_West "History of Logic") philosophers all developed structured methods of formal deduction by the first millennium BCE. Their ideas were developed over the centuries by philosophers such as [Aristotle](https://en.wikipedia.org/wiki/Aristotle "Aristotle") (who gave a formal analysis of the [syllogism](https://en.wikipedia.org/wiki/Syllogism "Syllogism")), [Euclid](https://en.wikipedia.org/wiki/Euclid "Euclid") (whose _[Elements](https://en.wikipedia.org/wiki/Euclid%27s_Elements "Euclid's Elements")_ was a model of formal reasoning), [al-Khwārizmī](https://en.wikipedia.org/wiki/Muhammad_ibn_Musa_al-Khwarizmi "Muhammad ibn Musa al-Khwarizmi") (who developed [algebra](https://en.wikipedia.org/wiki/Algebra "Algebra") and gave his name to the word _[algorithm](https://en.wikipedia.org/wiki/Algorithm "Algorithm")_) and European [scholastic](https://en.wikipedia.org/wiki/Scholasticism "Scholasticism") philosophers such as [William of Ockham](https://en.wikipedia.org/wiki/William_of_Ockham "William of Ockham") and [Duns Scotus](https://en.wikipedia.org/wiki/Duns_Scotus "Duns Scotus").

Spanish philosopher [Ramon Llull](https://en.wikipedia.org/wiki/Ramon_Llull "Ramon Llull") (1232–1315) developed several _logical machines_ devoted to the production of knowledge by logical means; Llull described his machines as mechanical entities that could combine basic and undeniable truths by simple logical operations, produced by the machine by mechanical meanings, in such ways as to produce all the possible knowledge. Llull's work had a great influence on [Gottfried Leibniz](https://en.wikipedia.org/wiki/Gottfried_Leibniz "Gottfried Leibniz"), who redeveloped his ideas.

In the 17th century, [Leibniz](https://en.wikipedia.org/wiki/Gottfried_Leibniz "Gottfried Leibniz"), [Thomas Hobbes](https://en.wikipedia.org/wiki/Thomas_Hobbes "Thomas Hobbes") and [René Descartes](https://en.wikipedia.org/wiki/Ren%C3%A9_Descartes "René Descartes") explored the possibility that all rational thought could be made as systematic as algebra or geometry. [Hobbes](https://en.wikipedia.org/wiki/Hobbes "Hobbes") famously wrote in [_Leviathan_](https://en.wikipedia.org/wiki/Leviathan_\(Hobbes_book\) "Leviathan (Hobbes book)"): "For _reason_ ... is nothing but _reckoning_, that is adding and subtracting". [Leibniz](https://en.wikipedia.org/wiki/Gottfried_Leibniz "Gottfried Leibniz") envisioned a universal language of reasoning, the _[characteristica universalis](https://en.wikipedia.org/wiki/Characteristica_universalis "Characteristica universalis")_, which would reduce argumentation to calculation so that "there would be no more need of disputation between two philosophers than between two accountants. For it would suffice to take their pencils in hand, down to their slates, and to say each other (with a friend as witness, if they liked): _Let us calculate_." These philosophers had begun to articulate the [physical symbol system](https://en.wikipedia.org/wiki/Physical_symbol_system "Physical symbol system") hypothesis that would guide AI research.

The study of [mathematical logic](https://en.wikipedia.org/wiki/Mathematical_logic "Mathematical logic") provided the essential breakthrough that made artificial intelligence seem plausible. The foundations had been set by such works as [Boole](https://en.wikipedia.org/wiki/George_Boole "George Boole")'s _[The Laws of Thought](https://en.wikipedia.org/wiki/The_Laws_of_Thought "The Laws of Thought")_ and [Frege](https://en.wikipedia.org/wiki/Frege "Frege")'s _[Begriffsschrift](https://en.wikipedia.org/wiki/Begriffsschrift "Begriffsschrift")_. Building on [Frege](https://en.wikipedia.org/wiki/Frege "Frege")'s system, [Russell](https://en.wikipedia.org/wiki/Bertrand_Russell "Bertrand Russell") and [Whitehead](https://en.wikipedia.org/wiki/Alfred_North_Whitehead "Alfred North Whitehead") presented a formal treatment of the foundations of mathematics in their masterpiece, the _[Principia Mathematica](https://en.wikipedia.org/wiki/Principia_Mathematica "Principia Mathematica")_ in 1913. Inspired by [Russell](https://en.wikipedia.org/wiki/Bertrand_Russell "Bertrand Russell")'s success, [David Hilbert](https://en.wikipedia.org/wiki/Hilbert%27s_program "Hilbert's program") challenged mathematicians of the 1920s and 30s to answer this fundamental question: "can all of mathematical reasoning be formalized?" His question was answered by [Gödel](https://en.wikipedia.org/wiki/Kurt_G%C3%B6del "Kurt Gödel")'s [incompleteness proof](https://en.wikipedia.org/wiki/G%C3%B6del%27s_incompleteness_theorems "Gödel's incompleteness theorems"), [Turing](https://en.wikipedia.org/wiki/Alan_Turing "Alan Turing")'s [machine](https://en.wikipedia.org/wiki/Turing_machine "Turing machine") and [Church](https://en.wikipedia.org/wiki/Alonzo_Church "Alonzo Church")'s [Lambda calculus](https://en.wikipedia.org/wiki/Lambda_calculus "Lambda calculus").

Their answer was surprising in two ways. First, they proved that there were, in fact, limits to what mathematical logic could accomplish. But second (and more important for AI) their work suggested that, within these limits, _any_ form of mathematical reasoning could be mechanized. The [Church-Turing thesis](https://en.wikipedia.org/wiki/Church-Turing_thesis "Church-Turing thesis") implied that a mechanical device, shuffling symbols as simple as _0_ and _1_, could imitate any conceivable process of mathematical deduction. The key insight was the [Turing machine](https://en.wikipedia.org/wiki/Turing_machine "Turing machine")—a simple theoretical construct that captured the essence of abstract symbol manipulation. This invention would inspire a handful of scientists to begin discussing the possibility of thinking machines.


### Information Technology & Computer Science
↗ [🌲 Road To CS](../../../🗺%20CS%20Overview/💋%20Intro%20to%20Computer%20Science/🌲%20Road%20To%20CS.md)
↗ [History of Information Systems & Security Systems](../../../CyberSecurity/History%20of%20Information%20Systems%20&%20Security%20Systems.md)
↗ [History of Computer Evolution](../../../🔑%20CS%20Core/👷🏾‍♂️%20Computer%20(Host)%20System/Computer%20Architecture/📌%20Computer%20Organization%20&%20Architecture%20Basics/History%20of%20Computer%20Evolution.md)

> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Computer_science

Calculating machines were designed or built in antiquity and throughout history by many people, including [Gottfried Leibniz](https://en.wikipedia.org/wiki/Gottfried_Leibniz#Information_technology "Gottfried Leibniz"), [Joseph Marie Jacquard](https://en.wikipedia.org/wiki/Joseph_Marie_Jacquard "Joseph Marie Jacquard"), [Charles Babbage](https://en.wikipedia.org/wiki/Charles_Babbage "Charles Babbage"), [Percy Ludgate](https://en.wikipedia.org/wiki/Percy_Ludgate "Percy Ludgate"), [Leonardo Torres Quevedo](https://en.wikipedia.org/wiki/Leonardo_Torres_Quevedo "Leonardo Torres Quevedo"), [Vannevar Bush](https://en.wikipedia.org/wiki/Vannevar_Bush "Vannevar Bush"), and others. [Ada Lovelace](https://en.wikipedia.org/wiki/Ada_Lovelace "Ada Lovelace") speculated that Babbage's machine was "a thinking or ... reasoning machine", but warned "It is desirable to guard against the possibility of exaggerated ideas that arise as to the powers" of the machine.

The first modern computers were the massive machines of the [Second World War](https://en.wikipedia.org/wiki/Second_World_War "Second World War") (such as [Konrad Zuse](https://en.wikipedia.org/wiki/Konrad_Zuse "Konrad Zuse")'s [Z3](https://en.wikipedia.org/wiki/Z3_\(computer\) "Z3 (computer)"), [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing "Alan Turing")'s [Heath Robinson](https://en.wikipedia.org/wiki/Heath_Robinson_machine "Heath Robinson machine") and [Colossus](https://en.wikipedia.org/wiki/Colossus_computer "Colossus computer"), [Atanasoff](https://en.wikipedia.org/wiki/John_Vincent_Atanasoff "John Vincent Atanasoff") and [Berry](https://en.wikipedia.org/wiki/Clifford_Berry "Clifford Berry")'s [ABC](https://en.wikipedia.org/wiki/Atanasoff%E2%80%93Berry_computer "Atanasoff–Berry computer"), and [ENIAC](https://en.wikipedia.org/wiki/ENIAC "ENIAC") at the [University of Pennsylvania](https://en.wikipedia.org/wiki/University_of_Pennsylvania "University of Pennsylvania")). [ENIAC](https://en.wikipedia.org/wiki/ENIAC "ENIAC") was based on the theoretical foundation laid by [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing "Alan Turing") and developed by [John von Neumann](https://en.wikipedia.org/wiki/John_von_Neumann "John von Neumann"), and proved to be the most influential.



## 👉 Birth of Artificial Intelligence (1941-1956)
> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Birth_of_artificial_intelligence_(1941%E2%80%931956)

The earliest research into thinking machines was inspired by a confluence of ideas that became prevalent in the late 1930s, 1940s, and early 1950s. Recent research in [neurology](https://en.wikipedia.org/wiki/Neurology "Neurology") had shown that the brain was an electrical network of [neurons](https://en.wikipedia.org/wiki/Neuron "Neuron") that fired in all-or-nothing pulses. [Norbert Wiener](https://en.wikipedia.org/wiki/Norbert_Wiener "Norbert Wiener")'s [cybernetics](https://en.wikipedia.org/wiki/Cybernetic "Cybernetic") described control and stability in electrical networks. [Claude Shannon](https://en.wikipedia.org/wiki/Claude_Shannon "Claude Shannon")'s [information theory](https://en.wikipedia.org/wiki/Information_theory "Information theory") described digital signals (i.e., all-or-nothing signals). [Alan Turing](https://en.wikipedia.org/wiki/Alan_Turing "Alan Turing")'s [theory of computation](https://en.wikipedia.org/wiki/Theory_of_computation "Theory of computation") showed that any form of computation could be described digitally. The close relationship between these ideas suggested that it might be possible to construct an "electronic brain".

In the 1940s and 50s, a handful of scientists from a variety of fields (mathematics, psychology, engineering, economics and political science) explored several research directions that would be vital to later AI research. Alan Turing was among the first people to seriously investigate the theoretical possibility of "machine intelligence". The field of "[artificial intelligence research](https://en.wikipedia.org/wiki/Artificial_intelligence "Artificial intelligence")" was founded as an academic discipline in 1956.


### Turing Test


### Neuroscience and Hebbian theory


### Artificial Neural Networks


### Cybernetic Robots


### Game AI


### Symbolic Reasoning and The Logic Theorist


### Dartmouth Workshop


### Cognitive Revolution



## 👉 Early Successes (1956–1974)
> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Early_successes_(1956%E2%80%931974)

There were many successful programs and new directions in the late 50s and 1960s. Here names the ones most influential:


### Reasoning, Planning and Problem Solving as Search


### Natural Language
An important goal of AI research is to allow computers to communicate in [natural languages](https://en.wikipedia.org/wiki/Natural_language_processing "Natural language processing") like English. An early success was [Daniel Bobrow](https://en.wikipedia.org/wiki/Daniel_Bobrow "Daniel Bobrow")'s program [STUDENT](https://en.wikipedia.org/wiki/STUDENT_\(computer_program\) "STUDENT (computer program)"), which could solve high school algebra word problems.

A [semantic net](https://en.wikipedia.org/wiki/Semantic_net "Semantic net") represents concepts (e.g. "house", "door") as nodes, and relations among concepts as links between the nodes (e.g. "has-a"). The first AI program to use a semantic net was written by Ross Quillian and the most successful (and controversial) version was [Roger Schank](https://en.wikipedia.org/wiki/Roger_Schank "Roger Schank")'s [Conceptual dependency theory](https://en.wikipedia.org/wiki/Conceptual_dependency_theory "Conceptual dependency theory").

![|450](../../../../Assets/Pics/Pasted%20image%2020250831123538.png)
<small>Example of a semantic network</small>

[Joseph Weizenbaum](https://en.wikipedia.org/wiki/Joseph_Weizenbaum "Joseph Weizenbaum")'s [ELIZA](https://en.wikipedia.org/wiki/ELIZA "ELIZA") could carry out conversations that were so realistic that users occasionally were fooled into thinking they were communicating with a human being and not a computer program (see [ELIZA effect](https://en.wikipedia.org/wiki/ELIZA_effect "ELIZA effect")). But in fact, ELIZA simply gave a [canned response](https://en.wikipedia.org/wiki/Canned_response "Canned response") or repeated back what was said to it, rephrasing its response with a few grammar rules. ELIZA was the first [chatbot](https://en.wikipedia.org/wiki/Chatbot "Chatbot").


### Micro-Worlds


### Perceptrons and Early Neural Networks



## 👉 1st AI Winter (1974–1980)



## 👉 1st AI Boom (1980–1987)



## 👉 New Directions In The 1980s
> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#New_directions_in_the_1980s

Although symbolic [knowledge representation](https://en.wikipedia.org/wiki/Knowledge_representation_and_reasoning "Knowledge representation and reasoning") and [logical reasoning](https://en.wikipedia.org/wiki/Logical_reasoning "Logical reasoning") produced useful applications in the 80s and received massive amounts of funding, it was still unable to solve problems in [perception](https://en.wikipedia.org/wiki/Perception "Perception"), [robotics](https://en.wikipedia.org/wiki/Robotics "Robotics"), [learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning") and [common sense](https://en.wikipedia.org/wiki/Common_sense_reasoning "Common sense reasoning"). A small number of scientists and engineers began to doubt that the symbolic approach would ever be sufficient for these tasks and developed other approaches, such as "[connectionism](https://en.wikipedia.org/wiki/Connectionism "Connectionism")", [robotics](https://en.wikipedia.org/wiki/Robotic "Robotic"), ["soft" computing](https://en.wikipedia.org/wiki/Soft_computing "Soft computing") and [reinforcement learning](https://en.wikipedia.org/wiki/Reinforcement_learning "Reinforcement learning"). [Nils Nilsson](https://en.wikipedia.org/wiki/Nils_John_Nilsson "Nils John Nilsson") called these approaches "sub-symbolic".


### Revival of Neural Networks: "Connectionism"

### Robotics and Embodied Reason

### Soft Computing and Probabilistic Reasoning

### Reinforcement Learning



## 👉 2nd AI Winter (1990s)



## 👉 Big Data, Deep Learning, AGI (2005–2017)
> ↗ [Deep Learning (Neural Networks) /The Technical Evolution of Neural Networks](🌊%20Deep%20Learning%20(Neural%20Network)/Deep%20Learning%20(Neural%20Networks).md#The%20Technical%20Evolution%20of%20Neural%20Networks)

> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Big_data,_deep_learning,_AGI_(2005%E2%80%932017)

In the first decades of the 21st century, access to large amounts of data (known as "[big data](https://en.wikipedia.org/wiki/Big_data "Big data")"), [cheaper and faster computers](https://en.wikipedia.org/wiki/Moore%27s_law "Moore's law") and advanced [machine learning](https://en.wikipedia.org/wiki/Machine_learning "Machine learning") techniques were successfully applied to many problems throughout the economy. A turning point was the success of [deep learning](https://en.wikipedia.org/wiki/Deep_learning "Deep learning") around 2012 which improved the performance of machine learning on many tasks, including image and video processing, text analysis, and speech recognition. Investment in AI increased along with its capabilities, and by 2016, the market for AI-related products, hardware, and software reached more than $8 billion, and the _New York Times_ reported that interest in AI had reached a "frenzy".

In 2002, [Ben Goertzel](https://en.wikipedia.org/wiki/Ben_Goertzel "Ben Goertzel") and others became concerned that AI had largely abandoned its original goal of producing versatile, fully intelligent machines, and argued in favor of more direct research into [artificial general intelligence](https://en.wikipedia.org/wiki/Artificial_general_intelligence "Artificial general intelligence") (AGI). By the mid-2010s several companies and institutions had been founded to pursue artificial general intelligence, such as [OpenAI](https://en.wikipedia.org/wiki/OpenAI "OpenAI") and [Google](https://en.wikipedia.org/wiki/Google "Google")'s [DeepMind](https://en.wikipedia.org/wiki/DeepMind "DeepMind"). During the same period, new insights into [superintelligence](https://en.wikipedia.org/wiki/Superintelligence "Superintelligence") raised concerns that AI was an [existential threat](https://en.wikipedia.org/wiki/Existential_risk_from_artificial_general_intelligence "Existential risk from artificial general intelligence"). The risks and unintended consequences of AI technology became an area of serious academic research after 2016.


### Big Data and Big Machines
>🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Big_data_and_big_machines

The success of machine learning in the 2000s depended on the availability of vast amounts of training data and faster computers. Russell and Norvig wrote that the "improvement in performance obtained by increasing the size of the data set by two or three orders of magnitude outweighs any improvement that can be made by tweaking the algorithm." [Geoffrey Hinton](https://en.wikipedia.org/wiki/Geoffrey_Hinton "Geoffrey Hinton") recalled that back in the 90s, the problem was that "our labeled datasets were thousands of times too small. [And] our computers were millions of times too slow." This was no longer true by 2010.

The most useful data in the 2000s came from curated, labeled data sets created specifically for machine learning and AI. In 2007, a group at [UMass Amherst](https://en.wikipedia.org/wiki/University_of_Massachusetts_Amherst "University of Massachusetts Amherst") released [Labeled Faces in the Wild](https://en.wikipedia.org/w/index.php?title=Labeled_Faces_in_the_Wild&action=edit&redlink=1 "Labeled Faces in the Wild (page does not exist)"), an annotated set of images of faces that was widely used to train and test [face recognition](https://en.wikipedia.org/wiki/Face_recognition "Face recognition") systems for the next several decades. [Fei-Fei Li](https://en.wikipedia.org/wiki/Fei-Fei_Li "Fei-Fei Li") developed [ImageNet](https://en.wikipedia.org/wiki/ImageNet "ImageNet"), a database of three million images captioned by volunteers using the [Amazon Mechanical Turk](https://en.wikipedia.org/wiki/Amazon_Mechanical_Turk "Amazon Mechanical Turk"). Released in 2009, it was a useful body of training data and a benchmark for testing for the next generation of image processing systems. Google released [word2vec](https://en.wikipedia.org/wiki/Word2vec "Word2vec") in 2013 as an open source resource. It used large amounts of data text scraped from the internet and [word embedding](https://en.wikipedia.org/wiki/Word_embedding "Word embedding") to create a numeric vector to represent each word. Users were surprised at how well it was able to capture word meanings, for example, ordinary vector addition would give equivalences like China + River = Yangtze or London − England + France = Paris. This database in particular would be essential for the development of [large language models](https://en.wikipedia.org/wiki/Large_language_model "Large language model") in the late 2010s.

The explosive growth of the internet gave machine learning programs access to billions of pages of text and images that could be [scraped](https://en.wikipedia.org/wiki/Data_scraping "Data scraping"). And, for specific problems, large privately held databases contained the relevant data. [McKinsey Global Institute](https://en.wikipedia.org/wiki/McKinsey_Global_Institute "McKinsey Global Institute") reported that "by 2009, nearly all sectors in the US economy had at least an average of 200 terabytes of stored data". This collection of information was known in the 2000s as _[big data](https://en.wikipedia.org/wiki/Big_data "Big data")_.

In a _[Jeopardy!](https://en.wikipedia.org/wiki/Jeopardy! "Jeopardy!")_ exhibition match in February 2011, [IBM](https://en.wikipedia.org/wiki/IBM "IBM")'s [question answering system](https://en.wikipedia.org/wiki/Question_answering_system "Question answering system") [Watson](https://en.wikipedia.org/wiki/IBM_Watson "IBM Watson") defeated the two best _Jeopardy!_ champions, [Brad Rutter](https://en.wikipedia.org/wiki/Brad_Rutter "Brad Rutter") and [Ken Jennings](https://en.wikipedia.org/wiki/Ken_Jennings "Ken Jennings"), by a significant margin. Watson's expertise would have been impossible without the information available on the internet.


### Deep Learning - 2012 AlexNet
> ↗ [Deep Learning (Neural Networks)](🌊%20Deep%20Learning%20(Neural%20Network)/Deep%20Learning%20(Neural%20Networks).md)

> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Deep_learning

In 2012, AlexNet, a deep learning model,[am] developed by Alex Krizhevsky, won the ImageNet Large Scale Visual Recognition Challenge, with significantly fewer errors than the second-place winner.[272][206] Krizhevsky worked with Geoffrey Hinton at the University of Toronto.[an] This was a turning point in machine learning: over the next few years dozens of other approaches to image recognition were abandoned in favor of deep learning.[264]

Deep learning uses a multi-layer perceptron. Although this architecture has been known since the 60s, getting it to work requires powerful hardware and large amounts of training data.[273] Before these became available, improving performance of image processing systems required hand-crafted ad hoc features that were difficult to implement.[273] Deep learning was simpler and more general.[ao]

Deep learning was applied to dozens of problems over the next few years (such as speech recognition, machine translation, medical diagnosis, and game playing). In every case it showed enormous gains in performance.[264] Investment and interest in AI boomed as a result.[264]


### The Alignment Problem
> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#The_alignment_problem


### Artificial General Intelligence Research
> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Artificial_general_intelligence_research



## 👉 From NLP to AGI: Boom of LLM (2017~)
> ↗ [Deep Learning (Neural Networks) /The Technical Evolution of Neural Networks](🌊%20Deep%20Learning%20(Neural%20Network)/Deep%20Learning%20(Neural%20Networks).md#The%20Technical%20Evolution%20of%20Neural%20Networks)
> ↗ [Natural Language Processing (NLP) /A Brief History of The Technical Evolution Of Language Models](../Natural%20Language%20Processing%20(NLP)/Natural%20Language%20Processing%20(NLP).md#A%20Brief%20History%20of%20The%20Technical%20Evolution%20Of%20Language%20Models)
> ↗ [LLM (Large Language Model) / LLM Milestone Papers](../Natural%20Language%20Processing%20(NLP)/🦑%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md#LLM%20Milestone%20Papers)
> ↗ [Transformers](🌊%20Deep%20Learning%20(Neural%20Network)/2️⃣%20Neural%20Network%20Models%20🗿/Transformers/Transformers.md)

> 🔗 https://en.wikipedia.org/wiki/History_of_artificial_intelligence#Large_language_models,_AI_boom_(2017%E2%80%93present)

The AI boom started with the initial development of key architectures and algorithms such as the [transformer architecture](https://en.wikipedia.org/wiki/Transformer_architecture "Transformer architecture") in 2017, leading to the scaling and development of large language models exhibiting human-like traits of knowledge, attention, and creativity. The new AI era began since 2020, with the public release of scaled [large language models](https://en.wikipedia.org/wiki/Large_language_model "Large language model") (LLMs) such as [ChatGPT](https://en.wikipedia.org/wiki/ChatGPT "ChatGPT").

> Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2025). _A Survey of Large Language Models_ (arXiv:2303.18223). arXiv. [https://doi.org/10.48550/arXiv.2303.18223](https://doi.org/10.48550/arXiv.2303.18223)

In Figure 2, we describe the evolution process of language models in terms of the **task solving capacity**. At first, statistical language models mainly assisted in some specific tasks (e.g., retrieval or speech tasks), in which the predicted or estimated probabilities can enhance the performance of task-specific approaches. Subsequently, neural language models focused on learning task-agnostic representations (e.g., features), aiming to reduce the efforts for human feature engineering. Furthermore, pre-trained language models learned context-aware representations that can be optimized according to downstream tasks. For the latest generation of language model, LLMs are enhanced by exploring the scaling effect on model capacity, which can be considered as **general-purpose task solvers**. To summarize, in the evolution process, the task scope that can be solved by language models have been greatly extended, and the task performance attained by language models have been significantly enhanced.

![](../../../../Assets/Pics/Screenshot%202025-09-01%20at%2010.56.49.png)
<small>
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2025). A Survey of Large Language Models (arXiv:2303.18223). arXiv. <br><a>https://doi.org/10.48550/arXiv.2303.18223</a></small>



## Ref
[What is the history of artificial intelligence (AI)?  |Tableau]: https://www.tableau.com/data-insights/ai/history
[History of artificial intelligence | wikipedia]: https://en.wikipedia.org/wiki/History_of_artificial_intelligence#
