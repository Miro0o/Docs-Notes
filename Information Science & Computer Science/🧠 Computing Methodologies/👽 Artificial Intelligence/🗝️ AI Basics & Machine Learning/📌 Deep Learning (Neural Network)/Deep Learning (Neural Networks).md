# Deep Learning (Neural Network)

[TOC]



## Res
### Related Topics
â†— [Neural Network Models](2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/Neural%20Network%20Models.md)
â†— [LLM (Large Language Model)](../../Natural%20Language%20Processing%20(NLP)/ğŸ¦‘%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md)

â†— [Information Theory](../../../../ğŸ§®%20Mathematics/ğŸ§%20Information%20Theory/Information%20Theory.md)
â†— [Linear Algebra](../../../../ğŸ§®%20Mathematics/ğŸ§Š%20Algebra/Linear%20Algebra/Linear%20Algebra.md)
â†— [Probabilities & Statistics](../../../../ğŸ§®%20Mathematics/ğŸ“%20Measures%20(Measure%20Theory)/ğŸ“Š%20Probabilities%20&%20Statistics/Probabilities%20&%20Statistics.md)
â†— [Analytical Mathematics](../../../../ğŸ§®%20Mathematics/Analytical%20Mathematics/Analytical%20Mathematics.md)


### Learning Resource
>  More on machine learning go to â†—ï¸ [AI Basics & Machine Learning](../AI%20Basics%20&%20Machine%20Learning.md)

---
åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ , ææ²
ğŸ  https://zh.d2l.ai/index.html (ZH)
ğŸ  https://d2l.ai/index.html (EN)

ğŸš§ https://github.com/d2l-ai/d2l-en (EN)
ğŸš§ https://github.com/d2l-ai/d2l-zh (ZH)

ğŸ‘¥ https://discuss.d2l.ai/c/chinese-version/16
ğŸ¬ã€è·Ÿç€ææ²ã€åŠ¨æ‰‹å­¦æ·±åº¦å­¦ä¹ ã€‘è¯¾ç¨‹ï¼Œå¤§ä½¬äº²æˆå…¨æ–¹é¢è§£è¯»â€œèŠ±ä¹¦â€ï¼Œå¸¦ä½ ä»å…¥é—¨åˆ°ç²¾é€šï¼ˆäººå·¥æ™ºèƒ½/æ·±åº¦å­¦ä¹ /è®¡ç®—æœºè§†è§‰/å›¾åƒå¤„ç†ï¼‰ã€‘ https://www.bilibili.com/video/BV1QP411j7jB/?share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d
- é¢å‘ä¸­æ–‡è¯»è€…çš„èƒ½è¿è¡Œã€å¯è®¨è®ºçš„æ·±åº¦å­¦ä¹ æ•™ç§‘ä¹¦
- å« PyTorchã€NumPy/MXNetã€TensorFlow å’Œ PaddlePaddle å®ç°
- è¢«å…¨çƒ 60 å¤šä¸ªå›½å®¶ 400 å¤šæ‰€å¤§å­¦ç”¨äºæ•™å­¦

---
https://hrl.boyuai.com/chapter/intro
æœ¬ä¹¦ä½œè€…åœ¨ä¸Šæµ·äº¤é€šå¤§å­¦è‡´è¿œå­¦é™¢å’Œç”µå­ä¿¡æ¯ä¸ç”µæ°”å·¥ç¨‹å­¦é™¢ä¸ºå¤§ä¸‰æœ¬ç§‘ç”Ÿå¼€è®¾å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹ã€‚ç›®å‰ä¸¤ä¸ªå­¦é™¢çš„å¼ºåŒ–å­¦ä¹ è¯¾ç¨‹åœ¨å…¶åŸ¹å…»æ–¹æ¡ˆä¸­çš†ä¸º 2 å­¦åˆ†ï¼ŒåŒ…å«äº†æ‰€æœ‰æˆè¯¾å’Œå®éªŒçš„å­¦æ—¶ã€‚åœ¨æˆè¯¾å’Œæ‰¹æ”¹å­¦ç”Ÿçš„è¯¾ç¨‹ä½œä¸šçš„è¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬å‘ç°å¼ºåŒ–å­¦ä¹ å¯¹äºå­¦ç”Ÿå’Œè€å¸ˆæ¥è¯´éƒ½æ˜¯ä¸€ä¸ªè¾ƒéš¾ç§‘ç›®ã€‚å¯¹äºå­¦ç”Ÿï¼Œå¼ºåŒ–å­¦ä¹ çš„ç†è®ºéƒ¨åˆ†å±äºæœºå™¨å­¦ä¹ å¤§ç§‘ç›®ä¸­è¿›é˜¶éƒ¨åˆ†å†…å®¹ï¼Œæ¶‰åŠåˆ°çš„æ•°å­¦å†…å®¹æ¯”ä¸€èˆ¬æœ‰ç›‘ç£å­¦ä¹ æ›´åŠ å¤æ‚ï¼Œè€Œå¯¹è¿™äº›å†…å®¹çš„çœŸæ­£ç†è§£ç¦»ä¸å¼€ç¼–ç¨‹å®éªŒï¼Œæ²¡æœ‰ç¬¬ä¸€æ‰‹çš„ç¼–ç¨‹å®ç°å’Œè°ƒè¯•ç»éªŒï¼Œå¾ˆå¤šå¼ºåŒ–å­¦ä¹ çš„åŸç†å°±æ— æ³•çœŸåˆ‡ä½“ä¼šã€‚æ€»å¾—æ¥è¯´ï¼Œå¯¹å¼ºåŒ–å­¦ä¹ æŠ€æœ¯çš„æ‰å®æŒæ¡ç¦»ä¸å¼€åŠ¨æ‰‹å®è·µï¼Œè€Œå¸‚é¢ä¸Šç›®å‰å°šæœªæœ‰è¾ƒä¸ºæƒå¨çš„é›†å¼ºåŒ–å­¦ä¹ åŸç†å’ŒåŠ¨æ‰‹å®è·µäºä¸€ä½“çš„ä¹¦ç±ã€‚
åŸºäºåœ¨å¼ºåŒ–å­¦ä¹ ç ”ç©¶å’Œæ•™å­¦ä¸­çš„æµ…è–„ç»éªŒï¼Œæˆ‘ä»¬æ¨å‡ºè¿™æœ¬ã€ŠåŠ¨æ‰‹å­¦å¼ºåŒ–å­¦ä¹ ã€‹ï¼Œæ—¨åœ¨æ¢ç´¢ä¸€ç§æ›´å¥½çš„å¼ºåŒ–å­¦ä¹ çš„æ•™å­¦æ–¹å¼ï¼Œä¸ºä¸­å›½å¼ºåŒ–å­¦ä¹ çš„äººæ‰åŸ¹å…»è´¡çŒ®ä¸€ä»½åŠ›é‡ã€‚ä¹¦ä¸­éš¾å…æœ‰é”™è°¬ä¹‹å¤„ï¼Œè¿˜æœ›å¹¿å¤§è¯»è€…ä¸åæŒ‡æ­£ï¼Œæˆ‘ä»¬æ„Ÿæ¿€ä¸å°½ã€‚

---
ğŸ« å®ç”¨æœºå™¨å­¦ä¹  [CS 329P Practical Machine Learning](../../../../ğŸ—º%20CS%20Overview/ğŸ’‹%20Intro%20to%20Computer%20Science/ğŸ‘©ğŸ¼â€ğŸ«%20Courses%20of%20Universities/Stanford/CS%20329P%20Practical%20Machine%20Learning/CS%20329P%20Practical%20Machine%20Learning.md)

---
ğŸ“– èŠ±ä¹¦ 
https://www.deeplearningbook.org èŠ±ä¹¦å®˜ç½‘  
https://github.com/exacity/deeplearningbook-chinese èŠ±ä¹¦ä¸­æ–‡ç‰ˆç¿»è¯‘  
https://github.com/MingchaoZhu/DeepLearning èŠ±ä¹¦åŸç†æ¨å¯¼åŠä»£ç å®ç°  
https://zhuanlan.zhihu.com/p/38431213 çŸ¥ä¹èŠ±ä¹¦å„ç« ç¬”è®°

ğŸ« https://cs230.stanford.edu 

ğŸ”¥ ğŸ“„ https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE
Papers must know to understand the world of deep learning & AIGC

[Practical Deep Learning for Coders](https://course.fast.ai/) course fromÂ [fast.ai](https://www.fast.ai/)
This course is extremely practical and oriented in a top-down manner, meaning that you learn how to implement ideas in code and use all the relevant tools first, then dig deeper into the details afterwards to understand how everything works. If youâ€™re new to the space and want to quickly get a working understanding of AI-related tools, how to use them, and how they work, start with these videos.

ğŸ‘ ğŸ‘ https://www.byhand.ai/
AI by Hand

https://www.fast.ai/
- **Courses**:Â [Practical Deep Learning for Coders](https://course.fast.ai/);Â [From Deep Learning Foundations to Stable Diffusion](https://course.fast.ai/Lessons/part2.html)
- **Software**:Â [fastai for PyTorch](https://docs.fast.ai/);Â [nbdev](https://nbdev.fast.ai/)
- **Book**:Â [Practical Deep Learning for Coders with fastai and PyTorch](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)
- **In the news**:Â [The Economist](https://www.economist.com/business/2018/10/25/new-schemes-teach-the-masses-to-build-ai);Â [The New York Times](https://www.nytimes.com/2018/11/18/technology/artificial-intelligence-language.html);Â [MIT Tech Review](https://www.technologyreview.com/s/611858/small-team-of-ai-coders-beats-googles-code/)

https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=AUDMGwyz7-yL33Xd
Neural networks | 3Blue1Brown
- [But what is a neural network? | Deep learning chapter 1](https://youtu.be/aircAruvnKk?si=RiyEviyfGbC8YwS0)

Michael Nielsen
Neural Networks and Deep Learning

https://distill.pub/

https://colah.github.io/



## Intro: Neural Network
### Neuron, and The Connection of Information
![computing.excalidraw | 800](../../../../../Assets/Illustrations/Computer%20Science%20Philosophy/computing.excalidraw.md)

![|600](../../../../../Assets/Pics/Screenshot%202025-09-04%20at%2020.19.48.png)

Features in data (æ•°æ®ç‰¹å¾): such **connections** between **informations** that leads to some semantic interpretation. ğŸ¤”


### Neural Network / Deep Network?
- **Neural Networks**: Slow learning, potentially very powerful, can learn very complex patterns. Can be prone to overfitting due to high density of the parameters.
- **Deep Networks**: Very large complex (neural) networks, but made up of many often heterogeneous types of networks that try to simplify the inputs, so that we can use a relatively small dense network to perform final decisions.


### The Evolution of Neural Networks
â†— [The Development History of AI / ğŸ‘‰ Big Data, Deep Learning, AGI (2005â€“2017)](../The%20Development%20History%20of%20AI.md#ğŸ‘‰%20Big%20Data,%20Deep%20Learning,%20AGI%20(2005â€“2017))
â†— [The Development History of AI /ğŸ‘‰ From NLP to AGI: Boom of LLM (2017~)](../The%20Development%20History%20of%20AI.md#ğŸ‘‰%20From%20NLP%20to%20AGI:%20Boom%20of%20LLM%20(2017~))
â†— [LLM (Large Language Model) / LLM Milestone Papers](../../Natural%20Language%20Processing%20(NLP)/ğŸ¦‘%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md#LLM%20Milestone%20Papers)



## Understanding Neural Networks and Deep Learning
https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=AUDMGwyz7-yL33Xd
Neural networks | 3Blue1Brown
- [But what is a neural network? | Deep learning chapter 1](https://youtu.be/aircAruvnKk?si=RiyEviyfGbC8YwS0)
- [Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://youtu.be/IHZwWFHWa-w?si=DqZgN_65JZfHX-81)
- [Backpropagation, intuitively | Deep Learning Chapter 3](https://youtu.be/Ilg3gGewQ5U?si=yYl6Vi6Sb-NxWbh5)
- [Backpropagation calculus | Deep Learning Chapter 4](https://youtu.be/tIeHLnjs5U8?si=w84SrOkyDnMwKSk7)
- [Large Language Models explained briefly](https://youtu.be/LPZh9BOjkQs?si=7CRyWTVnx3BIGQGy)
- [Transformers, the tech behind LLMs | Deep Learning Chapter 5](https://youtu.be/wjZofJX0v4M?si=cLC36CWJiJPKQJgT)
- [Attention in transformers, step-by-step | Deep Learning Chapter 6](https://youtu.be/eMlx5fFNoYc?si=UqpVj1vDxOtWAnlc)
- [How might LLMs store facts | Deep Learning Chapter 7](https://youtu.be/9-Jl0dxWQs8?si=jJPuNPfLV6AtWNJa)


### Basics & MLP (Multi Layer Perceptrons)
1. Linear Perceptrons
	1. XOR Problems
2. MLP (Multi-Layer Perceptron)
![](../../../../../../../../Assets/Pics/Screenshot%202023-01-29%20at%2012.54.02%20AM.png)
fully connected, dense layer


### CNN (Convolution Neural Network)
â†— [CNN (Convolutional Neural Network)](2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/CNN%20(Convolutional%20Neural%20Network)/CNN%20(Convolutional%20Neural%20Network).md)
- [VGGNet](2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/CNN%20(Convolutional%20Neural%20Network)/VGGNet/VGGNet.md)
- [YOLO (You Only Look Once)](2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/CNN%20(Convolutional%20Neural%20Network)/YOLO%20(You%20Only%20Look%20Once)/YOLO%20(You%20Only%20Look%20Once).md)
- etc.


### RNN (Recurrent Neural Network)
â†— [RNN (Recurrent Neural Network)](2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/RNN%20(Recurrent%20Neural%20Network)/RNN%20(Recurrent%20Neural%20Network).md)
- â†— [LSTM (Long-Short Term Memories)](2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/RNN%20(Recurrent%20Neural%20Network)/LSTM%20(Long-Short%20Term%20Memories)/LSTM%20(Long-Short%20Term%20Memories).md)
- etc.


### Transformer & LLM
â†— [Transformers (Encoder-Decoder)](2ï¸âƒ£%20Neural%20Network%20Models%20ğŸ—¿/Transformers%20(Encoder-Decoder)/Transformers%20(Encoder-Decoder).md)
â†— [LLM (Large Language Model)](../../Natural%20Language%20Processing%20(NLP)/ğŸ¦‘%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md)



## NN Hyperparameters



## Ref
[Deep Learning vs. Machine Learning]: https://dzone.com/articles/deep-learning-vs-machine-learning-the-hottest-topi

[What Is the Necessity of Bias in Neural Networks?]: https://www.turing.com/kb/necessity-of-bias-in-neural-networks#components-in-artificial-neural-networks
Components in artificial neural networks:
1.  **Inputs:**Â Theyâ€™re usually represented as features of a dataset which are passed on to a neural network to make predictions.
2.  **Weights:**Â These are the real values associated with the features. They are significant as they tell the importance of each feature which is passed as an input to theÂ [artificial neural network](https://www.turing.com/kb/importance-of-artificial-neural-networks-in-artificial-intelligence).
3.  **Bias:**Â Bias in a neural network is required to shift the activation function across the plane either towards the left or the right. We will cover it in more detail later.
4.  **Summation function:**Â It is defined as the function which sums up the product of the weight and the features with bias.
5.  **Activation function:**Â It is required to add non-linearity to the neural network model

[AAAI2024 | åˆ†äº«10ç¯‡ä¼˜ç§€è®ºæ–‡ï¼Œæ¶‰åŠå›¾ç¥ç»ç½‘ç»œã€å¤§æ¨¡å‹ä¼˜åŒ–ã€è¡¨æ ¼åˆ†æç­‰çƒ­é—¨è¯é¢˜]: https://mp.weixin.qq.com/s/F7X8N_wUyZQNhDtIfHm17Q
