# MLLM Evaluation & Benchmarks

[TOC]



## Res
### Related Topics
â†— [LLM Models List & Evaluation & Leaderboard](../ðŸ¦‘%20LLM%20(Large%20Language%20Model)/ðŸªœ%20LLM%20Models%20List%20&%20Evaluation%20&%20Leaderboard/LLM%20Models%20List%20&%20Evaluation%20&%20Leaderboard.md)


### Papers
https://mllm2024.github.io/CVPR2025/
Evaluations and Benchmarks in Context of Multimodal LLM
Part I: Survey
1. Li, et al., 2024,Â [**A Survey on Benchmarks of Multimodal Large Language Models**](https://arxiv.org/abs/2408.08632)  
2. Li, et al., 2024,Â [**A Survey on Multimodal Benchmarks: In the Era of Large AI Models**](https://arxiv.org/abs/2409.18142)  
3. Huang, et al., 2024,Â [**A Survey on Evaluation of Multimodal Large Language Models**](https://arxiv.org/abs/2408.15769)  
4. Fu, et al., 2024,Â [**MME-Survey: A Comprehensive Survey on Evaluation of Multimodal LLMs**](https://arxiv.org/abs/2411.15296)  
Part II: Benchmarks
5. Yue, et al., 2024,Â [**MMMU: A Massive Multi-discipline Multimodal Understanding and Reasoning Benchmark for Expert AGI**](https://mmmu-benchmark.github.io/)  
6. Fei, et al., 2025,Â [**On Path to Multimodal Generalist: General-Level and General-Bench**](https://generalist.top/)  
7. Li, et al., 2023,Â [**MVBench: A Comprehensive Multi-modal Video Understanding Benchmark**](https://arxiv.org/abs/2311.17005)  
8. Fu, et al., 2023,Â [**MME: A comprehensive evaluation benchmark for multimodal large language models**](https://arxiv.org/abs/2306.13394)  
9. Xu, et al., 2023,Â [**LVLM-eHub: A Comprehensive Evaluation Benchmark for Large Vision-Language Models**](https://arxiv.org/abs/2306.09265)  
10. Yu, et al., 2023,Â [**MM-Vet: Evaluating Large Multimodal Models for Integrated Capabilities**](https://arxiv.org/abs/2308.02490)  
11. Xia, et al., 2024,Â [**MMIE: Massive Multimodal Interleaved Comprehension Benchmark for Large Vision-Language Models**](https://arxiv.org/abs/2410.10139)  
12. Wu, et al., 2023,Â [**Q-Bench: A Benchmark for General-Purpose Foundation Models on Low-level Vision**](https://arxiv.org/abs/2309.14181)  
13. Liu, et al., 2023,Â [**MMBench: Is Your Multi-modal Model an All-around Player?**](https://arxiv.org/abs/2307.06281)  
14. Meng, et al., 2024,Â [**MMIU: Multimodal Multi-image Understanding for Evaluating Large Vision-Language Models**](https://arxiv.org/abs/2408.02718)  
15. Ying, et al., 2024,Â [**MMT-Bench: A Comprehensive Multimodal Benchmark for Evaluating Large Vision-Language Models Towards Multitask AGI**](https://arxiv.org/abs/2404.16006)  
16. Chen, et al., 2023,Â [**MEGA-Bench: Scaling Multimodal Evaluation to over 500 Real-World Tasks**](https://arxiv.org/abs/2410.10563)  
17. Li, et al., 2023,Â [**SEED-Bench: Benchmarking Multimodal LLMs with Generative Comprehension**](https://arxiv.org/abs/2307.16125)  
18. Li, et al., 2023,Â [**SEED-Bench-2: Benchmarking Multimodal Large Language Models**](https://arxiv.org/abs/2311.17092)


### Others
https://huggingface.co/collections/btjhjeon/multimodal-benchmarks

https://agi.safe.ai/



## Intro



## Ref
