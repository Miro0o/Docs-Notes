# LLM - Risks, Threats, and Security

[TOC]



## Res
### Related Topics


### Other Resources
https://stanford-cs324.github.io/winter2022/lectures/harms-1/
https://stanford-cs324.github.io/winter2022/lectures/harms-2/
CS324 - Large Language Model | Stanford



## Risks & Harms of Using LLM
> ðŸ”— https://stanford-cs324.github.io/winter2022/lectures/introduction/#risks
> So far, we have seen that by scaling up language models, they become exceptionally capable of tackling many tasks. However, not everything is as rosy, and there areÂ **substantial risks**Â associated with the use of language models. Multiple papers, includingÂ [the stochastic parrots paper](https://dl.acm.org/doi/pdf/10.1145/3442188.3445922),Â [the foundation models report](https://arxiv.org/pdf/2108.07258.pdf), andÂ [DeepMindâ€™s paper on ethical and social harms](https://arxiv.org/pdf/2112.04359.pdf)Â detail the risks. Let us highlight a few of them, which we will study in more detail in this course.


### Reliability


### Disinformation


### Performance Disparties


### Social Bias


### Toxicity


### Privacy, Security & Crime
> ðŸ”— https://stanford-cs324.github.io/winter2022/lectures/security/


### Legality
> ðŸ”— https://stanford-cs324.github.io/winter2022/lectures/legality/

#### Copyright & Legal Protection


### Cost and Environmental Impact
> ðŸ”— https://stanford-cs324.github.io/winter2022/lectures/environment/


### Access



## Threats Against LLM & LLM Security



## Ref
