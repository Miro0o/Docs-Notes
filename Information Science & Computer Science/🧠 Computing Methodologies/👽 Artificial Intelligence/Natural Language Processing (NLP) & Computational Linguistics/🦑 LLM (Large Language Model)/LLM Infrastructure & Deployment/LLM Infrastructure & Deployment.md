# LLM Infrastructure & Deployment

[TOC]



## Res
### Related Topics
â†— [AI (Data) Infrastructure & Techniques Stack](../../../ðŸ—ï¸%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack.md)
- â†— [Foundation Models & Development & SDKs](../../../ðŸ—ï¸%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/ðŸ›«%20Foundation%20Models%20&%20Development%20&%20SDKs/Foundation%20Models%20&%20Development%20&%20SDKs.md)
- â†— [Model Monitoring & Observability](../../../ðŸ—ï¸%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/Model%20Monitoring%20&%20Observability/Model%20Monitoring%20&%20Observability.md)
- â†— [Model Web Demo & Web Deployment](../../../ðŸ—ï¸%20AI%20(Data)%20Infrastructure%20&%20Techniques%20Stack/Model%20Web%20Demo%20&%20Web%20Deployment/Model%20Web%20Demo%20&%20Web%20Deployment.md)

â†— [AI(LLM) x SE](../../../../../Software%20Engineering/ðŸ¤–%20AI(LLM)%20x%20SE/AI(LLM)%20x%20SE.md)
- â†— [LLM Application Dev & Agentic AI Workflow](../../../../../Software%20Engineering/ðŸ¤–%20AI(LLM)%20x%20SE/LLM%20Application%20Dev%20&%20Agentic%20AI%20Workflow/LLM%20Application%20Dev%20&%20Agentic%20AI%20Workflow.md)


### LLM Deployment
> ðŸ”— https://github.com/Hannibal046/Awesome-LLM/tree/main?tab=readme-ov-file#llm-deployment (2025.01)
> Reference:Â [llm-inference-solutions](https://github.com/mani-kantap/llm-inference-solutions)

- [SGLang](https://github.com/sgl-project/sglang)Â - SGLang is a fast serving framework for large language models and vision language models.
- [vLLM](https://github.com/vllm-project/vllm)Â - A high-throughput and memory-efficient inference and serving engine for LLMs.
- [TGI](https://huggingface.co/docs/text-generation-inference/en/index)Â - a toolkit for deploying and serving Large Language Models (LLMs).
- [exllama](https://github.com/turboderp/exllama)Â - A more memory-efficient rewrite of the HF transformers implementation of Llama for use with quantized weights.
- [llama.cpp](https://github.com/ggerganov/llama.cpp)Â - LLM inference in C/C++.
- [ollama](https://github.com/ollama/ollama)Â - Get up and running with Llama 3, Mistral, Gemma, and other large language models.
- [Langfuse](https://github.com/langfuse/langfuse)Â - Open Source LLM Engineering Platform ðŸª¢ Tracing, Evaluations, Prompt Management, Evaluations and Playground.
- [FastChat](https://github.com/lm-sys/FastChat)Â - A distributed multi-model LLM serving system with web UI and OpenAI-compatible RESTful APIs.
- [mistral.rs](https://github.com/EricLBuehler/mistral.rs)Â - Blazingly fast LLM inference.
- [MindSQL](https://github.com/Mindinventory/MindSQL)Â - A python package for Txt-to-SQL with self hosting functionalities and RESTful APIs compatible with proprietary as well as open source LLM.
- [SkyPilot](https://github.com/skypilot-org/skypilot)Â - Run LLMs and batch jobs on any cloud. Get maximum cost savings, highest GPU availability, and managed execution -- all with a simple interface.
- [Haystack](https://haystack.deepset.ai/)Â - an open-source NLP framework that allows you to use LLMs and transformer-based models from Hugging Face, OpenAI and Cohere to interact with your own data.
- [Sidekick](https://github.com/ai-sidekick/sidekick)Â - Data integration platform for LLMs.
- [QA-Pilot](https://github.com/reid41/QA-Pilot)Â - An interactive chat project that leverages Ollama/OpenAI/MistralAI LLMs for rapid understanding and navigation of GitHub code repository or compressed file resources.
- [Shell-Pilot](https://github.com/reid41/shell-pilot)Â - Interact with LLM using Ollama models(or openAI, mistralAI)via pure shell scripts on your Linux(or MacOS) system, enhancing intelligent system management without any dependencies.
- [LangChain](https://github.com/hwchase17/langchain)Â - Building applications with LLMs through composability
- [Floom](https://github.com/FloomAI/Floom)Â AI gateway and marketplace for developers, enables streamlined integration of AI features into products
- [Swiss Army Llama](https://github.com/Dicklesworthstone/swiss_army_llama)Â - Comprehensive set of tools for working with local LLMs for various tasks.
- [LiteChain](https://github.com/rogeriochaves/litechain)Â - Lightweight alternative to LangChain for composing LLMs
- [magentic](https://github.com/jackmpcollins/magentic)Â - Seamlessly integrate LLMs as Python functions
- [wechat-chatgpt](https://github.com/fuergaosi233/wechat-chatgpt)Â - Use ChatGPT On Wechat via wechaty
- [promptfoo](https://github.com/typpo/promptfoo)Â - Test your prompts. Evaluate and compare LLM outputs, catch regressions, and improve prompt quality.
- [Agenta](https://github.com/agenta-ai/agenta)Â - Easily build, version, evaluate and deploy your LLM-powered apps.
- [Serge](https://github.com/serge-chat/serge)Â - a chat interface crafted with llama.cpp for running Alpaca models. No API keys, entirely self-hosted!
- [Langroid](https://github.com/langroid/langroid)Â - Harness LLMs with Multi-Agent Programming
- [Embedchain](https://github.com/embedchain/embedchain)Â - Framework to create ChatGPT like bots over your dataset.
- [Opik](https://github.com/comet-ml/opik)Â - Confidently evaluate, test, and ship LLM applications with a suite of observability tools to calibrate language model outputs across your dev and production lifecycle.
- [IntelliServer](https://github.com/intelligentnode/IntelliServer)Â - simplifies the evaluation of LLMs by providing a unified microservice to access and test multiple AI models.
- [OpenLLM](https://github.com/bentoml/OpenLLM)Â - Fine-tune, serve, deploy, and monitor any open-source LLMs in production. Used in production atÂ [BentoML](https://bentoml.com/)Â for LLMs-based applications.
- [DeepSpeed-Mii](https://github.com/microsoft/DeepSpeed-MII)Â - MII makes low-latency and high-throughput inference, similar to vLLM powered by DeepSpeed.
- [Text-Embeddings-Inference](https://github.com/huggingface/text-embeddings-inference)Â - Inference for text-embeddings in Rust, HFOIL Licence.
- [Infinity](https://github.com/michaelfeil/infinity)Â - Inference for text-embeddings in Python
- [TensorRT-LLM](https://github.com/NVIDIA/TensorRT-LLM)Â - Nvidia Framework for LLM Inference
- [FasterTransformer](https://github.com/NVIDIA/FasterTransformer)Â - NVIDIA Framework for LLM Inference(Transitioned to TensorRT-LLM)
- [Flash-Attention](https://github.com/Dao-AILab/flash-attention)Â - A method designed to enhance the efficiency of Transformer models
- [Langchain-Chatchat](https://github.com/chatchat-space/Langchain-Chatchat)Â - Formerly langchain-ChatGLM, local knowledge based LLM (like ChatGLM) QA app with langchain.
- [Search with Lepton](https://github.com/leptonai/search_with_lepton)Â - Build your own conversational search engine using less than 500 lines of code byÂ [LeptonAI](https://github.com/leptonai).
- [Robocorp](https://github.com/robocorp/robocorp)Â - Create, deploy and operate Actions using Python anywhere to enhance your AI agents and assistants. Batteries included with an extensive set of libraries, helpers and logging.
- [LMDeploy](https://github.com/InternLM/lmdeploy)Â - A high-throughput and low-latency inference and serving framework for LLMs and VLs
- [Tune Studio](https://studio.tune.app/)Â - Playground for devs to finetune & deploy LLMs
- [LLocalSearch](https://github.com/nilsherzig/LLocalSearch)Â - Locally running websearch using LLM chains
- [AI Gateway](https://github.com/Portkey-AI/gateway)Â â€” Gateway streamlines requests to 100+ open & closed source models with a unified API. It is also production-ready with support for caching, fallbacks, retries, timeouts, loadbalancing, and can be edge-deployed for minimum latency.
- [talkd.ai dialog](https://github.com/talkdai/dialog)Â - Simple API for deploying any RAG or LLM that you want adding plugins.
- [Wllama](https://github.com/ngxson/wllama)Â - WebAssembly binding for llama.cpp - Enabling in-browser LLM inference
- [GPUStack](https://github.com/gpustack/gpustack)Â - An open-source GPU cluster manager for running LLMs
- [MNN-LLM](https://github.com/alibaba/MNN)Â -- A Device-Inference framework, including LLM Inference on device(Mobile Phone/PC/IOT)
- [CAMEL](https://www.camel-ai.org/)Â - First LLM Multi-agent framework.



## Intro
### Deploy LLM on Different Levels - Desktop and Production
To explain these two deployments, take the comparison of ollama and vLLM for example (generated by Gemini 2.5 Flash):
#vLLM #ollama #LLM #software_deployment

While both Ollama and vLLM are tools for LLM inference (running a model), their **design goals** and **primary use cases** are fundamentally different:

| **Aspect**             | **Ollama**                                                                                                    | **vLLM (Very Large Language Model)**                                                                         |
| ---------------------- | ------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------ |
| **Primary Goal**       | **Simplicity and Accessibility.** To make it easy to run LLMs locally for prototyping and personal use.       | **High-Throughput and Efficiency.** To maximize LLM serving performance in production.                       |
| **Target Environment** | Local machines, developer laptops, single-user setups.                                                        | Production servers, cloud deployments, multi-GPU clusters.                                                   |
| **Performance**        | Good for single-user, low-concurrency requests. Prioritizes a simple user experience over raw speed at scale. | **Significantly Higher Throughput and Lower Latency** under heavy, concurrent load.                          |
| **Key Optimization**   | **Quantization** and easy packaging for resource-constrained hardware.                                        | **PagedAttention** (efficient memory management) and **Continuous Batching** (efficient request scheduling). |
| **Hardware Focus**     | Consumer-grade hardware (CPU and GPU) and Apple Silicon.                                                      | High-end, dedicated GPUs (like NVIDIA A100s/H100s).                                                          |
| **User Experience**    | Simple CLI and API, minimal setup. **Beginner-friendly.**                                                     | More complex setup, focused on advanced configuration for production needs. **Engineer-focused.**            |
| **Model Scope**        | Curated model library that are pre-packaged.                                                                  | Works with a wide range of models from the Hugging Face ecosystem.                                           |



## Ref
