# Pre-Training

[TOC]



## Res
### Related Topics
↗ [Transformers](../../../../🗝️%20AI%20Basics%20&%20Machine%20Learning%20(ML)/🌊%20Deep%20Learning%20(Neural%20Network)/2️⃣%20Neural%20Network%20Models%20🗿/Transformers/Transformers.md)


### Other Resources
https://stanford-cs324.github.io/winter2022/lectures/training/
CS324 - Large Language Model | Stanford

https://stanford-cs324.github.io/winter2022/lectures/parallelism/
CS324 - Large Language Model | Stanford
- [Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism](https://arxiv.org/pdf/1909.08053.pdf). _M. Shoeybi, M. Patwary, Raul Puri, P. LeGresley, J. Casper, Bryan Catanzaro_. 2019.
- [GPipe: Efficient Training of Giant Neural Networks using Pipeline Parallelism](https://arxiv.org/pdf/1811.06965.pdf). _Yanping Huang, Yonglong Cheng, Dehao Chen, HyoukJoong Lee, Jiquan Ngiam, Quoc V. Le, Z. Chen_. NeurIPS 2018.
- [Efficient large-scale language model training on GPU clusters using Megatron-LM](https://arxiv.org/pdf/2104.04473.pdf). _D. Narayanan, M. Shoeybi, J. Casper, P. LeGresley, M. Patwary, V. Korthikanti, Dmitri Vainbrand, Prethvi Kashinkunti, J. Bernauer, Bryan Catanzaro, Amar Phanishayee, M. Zaharia_. SC 2021.
- [TeraPipe: Token-Level Pipeline Parallelism for Training Large-Scale Language Models](https://arxiv.org/pdf/2102.07988.pdf). _Zhuohan Li, Siyuan Zhuang, Shiyuan Guo, Danyang Zhuo, Hao Zhang, D. Song, I. Stoica_. ICML 2021.



## Intro



## Ref
