# Deep Learning (Neural Network)

[TOC]



## Res
### Related Topics
↗ [Neural Network Models](2️⃣%20Neural%20Network%20Models%20🗿/Neural%20Network%20Models.md)
↗ [LLM (Large Language Model)](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/🦑%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md)

↗ [Information Theory](../../../../🧮%20Mathematics/🧐%20Information%20Theory/Information%20Theory.md)
↗ [Linear Algebra](../../../../🧮%20Mathematics/🧊%20Algebra/🎃%20Algebraic%20Structure%20&%20Abstract%20Algebra%20&%20Modern%20Algebra/Module-Like%20Algebraic%20Structure/Linear%20Algebra/Linear%20Algebra.md)
↗ [Probabilities & Statistics](../../../../🧮%20Mathematics/📐%20Measures%20(Measure%20Theory)/📊%20Probabilities%20&%20Statistics/Probabilities%20&%20Statistics.md)
↗ [Analytical Mathematics](../../../../🧮%20Mathematics/Analytical%20Mathematics/Analytical%20Mathematics.md)


### Learning Resource
>  More on machine learning go to ↗️ [AI Basics & Machine Learning (ML)](../AI%20Basics%20&%20Machine%20Learning%20(ML).md)

动手学深度学习, 李沐
🏠 https://zh.d2l.ai/index.html (ZH)
🏠 https://d2l.ai/index.html (EN)
🚧 https://github.com/d2l-ai/d2l-en (EN)
🚧 https://github.com/d2l-ai/d2l-zh (ZH)
👥 https://discuss.d2l.ai/c/chinese-version/16
🎬【跟着李沐【动手学深度学习】课程，大佬亲授全方面解读“花书”，带你从入门到精通（人工智能/深度学习/计算机视觉/图像处理）】 https://www.bilibili.com/video/BV1QP411j7jB/?share_source=copy_web&vd_source=7740584ebdab35221363fc24d1582d9d
- 面向中文读者的能运行、可讨论的深度学习教科书
- 含 PyTorch、NumPy/MXNet、TensorFlow 和 PaddlePaddle 实现
- 被全球 60 多个国家 400 多所大学用于教学

https://hrl.boyuai.com/chapter/intro
本书作者在上海交通大学致远学院和电子信息与电气工程学院为大三本科生开设强化学习课程。目前两个学院的强化学习课程在其培养方案中皆为 2 学分，包含了所有授课和实验的学时。在授课和批改学生的课程作业的过程中，我们发现强化学习对于学生和老师来说都是一个较难科目。对于学生，强化学习的理论部分属于机器学习大科目中进阶部分内容，涉及到的数学内容比一般有监督学习更加复杂，而对这些内容的真正理解离不开编程实验，没有第一手的编程实现和调试经验，很多强化学习的原理就无法真切体会。总得来说，对强化学习技术的扎实掌握离不开动手实践，而市面上目前尚未有较为权威的集强化学习原理和动手实践于一体的书籍。
基于在强化学习研究和教学中的浅薄经验，我们推出这本《动手学强化学习》，旨在探索一种更好的强化学习的教学方式，为中国强化学习的人才培养贡献一份力量。书中难免有错谬之处，还望广大读者不吝指正，我们感激不尽。

🏫 实用机器学习 [CS 329P Practical Machine Learning](../../../../🗺%20CS%20Overview/💋%20Intro%20to%20Computer%20Science/👩🏼‍🏫%20Courses%20of%20Universities/Stanford/CS%20329P%20Practical%20Machine%20Learning/CS%20329P%20Practical%20Machine%20Learning.md)
🏫 [CS50's Introduction to AI with Python](../../../../🗺%20CS%20Overview/💋%20Intro%20to%20Computer%20Science/👩🏼‍🏫%20Courses%20of%20Universities/Harvard/CS50's%20Introduction%20to%20AI%20with%20Python/CS50's%20Introduction%20to%20AI%20with%20Python.md)
🏫 [CS188 Introduction to Artificial Intelligence](../../../../🗺%20CS%20Overview/💋%20Intro%20to%20Computer%20Science/👩🏼‍🏫%20Courses%20of%20Universities/UC%20Berkeley/CS188%20Introduction%20to%20Artificial%20Intelligence/CS188%20Introduction%20to%20Artificial%20Intelligence.md)
🏫 [CS 231n Deep Learning for Computer Vision](../../../../🗺%20CS%20Overview/💋%20Intro%20to%20Computer%20Science/👩🏼‍🏫%20Courses%20of%20Universities/Stanford/CS%20231n%20Deep%20Learning%20for%20Computer%20Vision/CS%20231n%20Deep%20Learning%20for%20Computer%20Vision.md)
🏫 https://cs230.stanford.edu

📖 花书 
https://www.deeplearningbook.org 花书官网  
https://github.com/exacity/deeplearningbook-chinese 花书中文版翻译  
https://github.com/MingchaoZhu/DeepLearning 花书原理推导及代码实现  
https://zhuanlan.zhihu.com/p/38431213 知乎花书各章笔记

🔥 📄 https://arc.net/folder/D0472A20-9C20-4D3F-B145-D2865C0A9FEE
Papers must know to understand the world of deep learning & AIGC

[Practical Deep Learning for Coders](https://course.fast.ai/) course from [fast.ai](https://www.fast.ai/)
This course is extremely practical and oriented in a top-down manner, meaning that you learn how to implement ideas in code and use all the relevant tools first, then dig deeper into the details afterwards to understand how everything works. If you’re new to the space and want to quickly get a working understanding of AI-related tools, how to use them, and how they work, start with these videos.

👍 👍 https://www.byhand.ai/
AI by Hand

https://www.fast.ai/
- **Courses**: [Practical Deep Learning for Coders](https://course.fast.ai/); [From Deep Learning Foundations to Stable Diffusion](https://course.fast.ai/Lessons/part2.html)
- **Software**: [fastai for PyTorch](https://docs.fast.ai/); [nbdev](https://nbdev.fast.ai/)
- **Book**: [Practical Deep Learning for Coders with fastai and PyTorch](https://www.amazon.com/Deep-Learning-Coders-fastai-PyTorch/dp/1492045527)
- **In the news**: [The Economist](https://www.economist.com/business/2018/10/25/new-schemes-teach-the-masses-to-build-ai); [The New York Times](https://www.nytimes.com/2018/11/18/technology/artificial-intelligence-language.html); [MIT Tech Review](https://www.technologyreview.com/s/611858/small-team-of-ai-coders-beats-googles-code/)

https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=AUDMGwyz7-yL33Xd
Neural networks | 3Blue1Brown
- [But what is a neural network? | Deep learning chapter 1](https://youtu.be/aircAruvnKk?si=RiyEviyfGbC8YwS0)
- [Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://youtu.be/IHZwWFHWa-w?si=DqZgN_65JZfHX-81)
- [Backpropagation, intuitively | Deep Learning Chapter 3](https://youtu.be/Ilg3gGewQ5U?si=yYl6Vi6Sb-NxWbh5)
- [Backpropagation calculus | Deep Learning Chapter 4](https://youtu.be/tIeHLnjs5U8?si=w84SrOkyDnMwKSk7)
- [Large Language Models explained briefly](https://youtu.be/LPZh9BOjkQs?si=7CRyWTVnx3BIGQGy)
- [Transformers, the tech behind LLMs | Deep Learning Chapter 5](https://youtu.be/wjZofJX0v4M?si=cLC36CWJiJPKQJgT)
	- 【【官方双语】GPT是什么？直观解释Transformer | 深度学习第5章-哔哩哔哩】 https://b23.tv/rcO76mO
- [Attention in transformers, step-by-step | Deep Learning Chapter 6](https://youtu.be/eMlx5fFNoYc?si=UqpVj1vDxOtWAnlc)
	- 【【官方双语】直观解释注意力机制，Transformer的核心 | 【深度学习第6章】-哔哩哔哩】 https://b23.tv/f0udg4P
- [How might LLMs store facts | Deep Learning Chapter 7](https://youtu.be/9-Jl0dxWQs8?si=jJPuNPfLV6AtWNJa)

Michael Nielsen
Neural Networks and Deep Learning

https://distill.pub/

https://colah.github.io/

https://www.mit.edu/~amidi/
https://stanford.edu/~shervine/
- [15.003 ― Data Science Tools](https://www.mit.edu/~amidi/teaching/data-science-tools/)
- [15.003 - Modeling](https://www.mit.edu/~amidi/teaching/modeling/)
- [CS 221 ― Artificial Intelligence](https://stanford.edu/~shervine/teaching/cs-221/)
- [CS 229 ― Machine Learning](https://stanford.edu/~shervine/teaching/cs-229/)
- [CS 230 ― Deep Learning](https://stanford.edu/~shervine/teaching/cs-230/)
![](../../../../../Assets/Pics/Screenshot%202025-10-05%20at%2023.42.44.png)



## Intro: Neural Network
### Neuron, and The Connection of Information
![computing.excalidraw | 800](../../../../../Assets/Illustrations/Computer%20Science%20Philosophy/computing.excalidraw.md)

![|600](../../../../../Assets/Pics/Screenshot%202025-09-04%20at%2020.19.48.png)
<small><a>https://youtu.be/aircAruvnKk?si=RiyEviyfGbC8YwS0</a></small>

Features in data (数据特征): such **connections** between **informations** that leads to some semantic interpretation. 🤔


### Neural Network / Deep Network?
- **Neural Networks**: Slow learning, potentially very powerful, can learn very complex patterns. Can be prone to overfitting due to high density of the parameters.
- **Deep Networks**: Very large complex (neural) networks, but made up of many often heterogeneous types of networks that try to simplify the inputs, so that we can use a relatively small dense network to perform final decisions.


### The Technical Evolution of Neural Networks
↗ [The Development History of AI / 👉 Big Data, Deep Learning, AGI (2005–2017)](../The%20Development%20History%20of%20AI.md#👉%20Big%20Data,%20Deep%20Learning,%20AGI%20(2005–2017))
↗ [The Development History of AI /👉 From NLP to AGI: Boom of LLM (2017~)](../The%20Development%20History%20of%20AI.md#👉%20From%20NLP%20to%20AGI:%20Boom%20of%20LLM%20(2017~))
↗ [Natural Language Processing (NLP) /📜 A Brief History of The Technical Evolution Of Language Models](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics.md#📜%20A%20Brief%20History%20of%20The%20Technical%20Evolution%20Of%20Language%20Models)
↗ [LLM (Large Language Model) /⭐ LLM Milestone Papers](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/🦑%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md#⭐%20LLM%20Milestone%20Papers)

![](../../../../../Assets/Pics/Screenshot%202025-09-01%20at%2010.56.49.png)
<small>
Zhao, W. X., Zhou, K., Li, J., Tang, T., Wang, X., Hou, Y., Min, Y., Zhang, B., Zhang, J., Dong, Z., Du, Y., Yang, C., Chen, Y., Chen, Z., Jiang, J., Ren, R., Li, Y., Tang, X., Liu, Z., … Wen, J.-R. (2025). A Survey of Large Language Models (arXiv:2303.18223). arXiv. <br><a>https://doi.org/10.48550/arXiv.2303.18223</a></small>



## Understanding Neural Networks and Deep Learning
https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=AUDMGwyz7-yL33Xd
Neural networks | 3Blue1Brown
- [But what is a neural network? | Deep learning chapter 1](https://youtu.be/aircAruvnKk?si=RiyEviyfGbC8YwS0)
- [Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://youtu.be/IHZwWFHWa-w?si=DqZgN_65JZfHX-81)
- [Backpropagation, intuitively | Deep Learning Chapter 3](https://youtu.be/Ilg3gGewQ5U?si=yYl6Vi6Sb-NxWbh5)
- [Backpropagation calculus | Deep Learning Chapter 4](https://youtu.be/tIeHLnjs5U8?si=w84SrOkyDnMwKSk7)
- [Large Language Models explained briefly](https://youtu.be/LPZh9BOjkQs?si=7CRyWTVnx3BIGQGy)
- [Transformers, the tech behind LLMs | Deep Learning Chapter 5](https://youtu.be/wjZofJX0v4M?si=cLC36CWJiJPKQJgT)
- [Attention in transformers, step-by-step | Deep Learning Chapter 6](https://youtu.be/eMlx5fFNoYc?si=UqpVj1vDxOtWAnlc)
- [How might LLMs store facts | Deep Learning Chapter 7](https://youtu.be/9-Jl0dxWQs8?si=jJPuNPfLV6AtWNJa)


### Neural Network Basics
1. Linear Perceptrons
	1. XOR Problems
2. MLP (Multi-Layer Perceptron)
#### MLP (Multi Layer Perceptrons)
![](../../../../../../../../Assets/Pics/Screenshot%202023-01-29%20at%2012.54.02%20AM.png)
<small>fully connected, dense layer</small>
#### Gradient Descent & Backpropagation
https://youtube.com/playlist?list=PLZHQObOWTQDNU6R1_67000Dx_ZCJB-3pi&si=AUDMGwyz7-yL33Xd
Neural networks | 3Blue1Brown
- [But what is a neural network? | Deep learning chapter 1](https://youtu.be/aircAruvnKk?si=RiyEviyfGbC8YwS0)
- [Gradient descent, how neural networks learn | Deep Learning Chapter 2](https://youtu.be/IHZwWFHWa-w?si=DqZgN_65JZfHX-81)
- [Backpropagation, intuitively | Deep Learning Chapter 3](https://youtu.be/Ilg3gGewQ5U?si=yYl6Vi6Sb-NxWbh5)
- [Backpropagation calculus | Deep Learning Chapter 4](https://youtu.be/tIeHLnjs5U8?si=w84SrOkyDnMwKSk7)

🎬 https://youtu.be/VMj-3S1tku0?si=GljktCky9TZOrXMF
This is the most step-by-step spelled-out explanation of backpropagation and training of neural networks. It only assumes basic knowledge of Python and a vague recollection of calculus from high school. Links:
- micrograd on github: [https://github.com/karpathy/micrograd](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqblpTMk9PQTViUGt2RElMcEJNWmRTdkhUNnFRZ3xBQ3Jtc0ttV01BaEUtdW04bHF3UTlXNFQycWFiUUFjWGZ1TDZBWnNCWlA1WHpQckxmWE5rVHRvejNMWnFtd0k1M3JQRTN0RUhJRE5XSkRWeEVxeHNIc1VaTExVaXdxVVVUSUkwZEVJaXB3X3h4b0ZQNUJIR3dTUQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fmicrograd&v=VMj-3S1tku0)
- jupyter notebooks I built in this video: [https://github.com/karpathy/nn-zero-t...](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbHFxeGdRcGVWdXhjLV9RbHQyZm94djdLYm4tUXxBQ3Jtc0tuS1JKOFBIcTRadWtVY1BBZFUtY3d6U09iZ29FcjR4R2c2MzgtSlRjZWlnOEkxUFUtVUlZaTNXSkFRUXJSaXBxNkVER3NSbTMzbG9iQnBuckl5WWNWU1hOUTdwSGtuNmNLbUhUNWg1c1dWanpCYkZNUQ&q=https%3A%2F%2Fgithub.com%2Fkarpathy%2Fnn-zero-to-hero%2Ftree%2Fmaster%2Flectures%2Fmicrograd&v=VMj-3S1tku0)
- my website: [https://karpathy.ai](https://www.youtube.com/redirect?event=video_description&redir_token=QUFFLUhqbU9pTktUTXpQLU45U3AzbkZZdUlXUTdZZzdwQXxBQ3Jtc0ttQlU0QmJ3S05XNmJJYWFoa0ZNQmhQMnJUdGhlWG9RcDgtYzR4MUE2amhLLVBRQ2lzTTMyZUxtWG90bTU4a1pPWW9CaGY2dldoRXNweS1Qb3FFMzRsVDZYSVEyV0JoZVJfcE02N2pWVGJIVWVSdDlkNA&q=https%3A%2F%2Fkarpathy.ai%2F&v=VMj-3S1tku0)



### CNN (Convolution Neural Network)
↗ [CNN (Convolutional Neural Network)](2️⃣%20Neural%20Network%20Models%20🗿/CNN%20(Convolutional%20Neural%20Network)/CNN%20(Convolutional%20Neural%20Network).md)
- ↗ [VGGNet](2️⃣%20Neural%20Network%20Models%20🗿/CNN%20(Convolutional%20Neural%20Network)/VGGNet/VGGNet.md)
- ↗ [YOLO (You Only Look Once)](2️⃣%20Neural%20Network%20Models%20🗿/CNN%20(Convolutional%20Neural%20Network)/YOLO%20(You%20Only%20Look%20Once)/YOLO%20(You%20Only%20Look%20Once).md)
- etc.


### RNN (Recurrent Neural Network)
↗ [RNN (Recurrent Neural Network)](2️⃣%20Neural%20Network%20Models%20🗿/RNN%20(Recurrent%20Neural%20Network)/RNN%20(Recurrent%20Neural%20Network).md)
#### LSTM (Long-Short Term Memories)
↗ [LSTM (Long-Short Term Memories)](2️⃣%20Neural%20Network%20Models%20🗿/RNN%20(Recurrent%20Neural%20Network)/LSTM%20(Long-Short%20Term%20Memories)/LSTM%20(Long-Short%20Term%20Memories).md)


### Transformer & LLM
↗ [Transformers](2️⃣%20Neural%20Network%20Models%20🗿/Transformers/Transformers.md)
↗ [LLM (Large Language Model)](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/🦑%20LLM%20(Large%20Language%20Model)/LLM%20(Large%20Language%20Model).md)
- ↗ [OpenAI ChatGPT](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/🦑%20LLM%20(Large%20Language%20Model)/🪜%20LLM%20Models%20Guide%20&%20Leaderboard/OpenAI%20ChatGPT.md)
- ↗ [Google Gemini](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/🦑%20LLM%20(Large%20Language%20Model)/🪜%20LLM%20Models%20Guide%20&%20Leaderboard/Google%20Gemini.md)
- ↗ [Anthropic Claude](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/🦑%20LLM%20(Large%20Language%20Model)/🪜%20LLM%20Models%20Guide%20&%20Leaderboard/Anthropic%20Claude.md)
- ↗ [Meta LLama](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/🦑%20LLM%20(Large%20Language%20Model)/🪜%20LLM%20Models%20Guide%20&%20Leaderboard/Meta%20LLama.md)
- ↗ [DeepSeek](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/🦑%20LLM%20(Large%20Language%20Model)/🪜%20LLM%20Models%20Guide%20&%20Leaderboard/DeepSeek.md)
- ↗ [xAI Grok](../../Natural%20Language%20Processing%20(NLP)%20&%20Computational%20Linguistics/🦑%20LLM%20(Large%20Language%20Model)/🪜%20LLM%20Models%20Guide%20&%20Leaderboard/xAI%20Grok.md)



## NN Hyperparameters



## Ref
[Deep Learning vs. Machine Learning]: https://dzone.com/articles/deep-learning-vs-machine-learning-the-hottest-topi

[What Is the Necessity of Bias in Neural Networks?]: https://www.turing.com/kb/necessity-of-bias-in-neural-networks#components-in-artificial-neural-networks
Components in artificial neural networks:
1.  **Inputs:** They’re usually represented as features of a dataset which are passed on to a neural network to make predictions.
2.  **Weights:** These are the real values associated with the features. They are significant as they tell the importance of each feature which is passed as an input to the [artificial neural network](https://www.turing.com/kb/importance-of-artificial-neural-networks-in-artificial-intelligence).
3.  **Bias:** Bias in a neural network is required to shift the activation function across the plane either towards the left or the right. We will cover it in more detail later.
4.  **Summation function:** It is defined as the function which sums up the product of the weight and the features with bias.
5.  **Activation function:** It is required to add non-linearity to the neural network model

[AAAI2024 | 分享10篇优秀论文，涉及图神经网络、大模型优化、表格分析等热门话题]: https://mp.weixin.qq.com/s/F7X8N_wUyZQNhDtIfHm17Q
